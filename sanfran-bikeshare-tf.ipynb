{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Have you ever tried to use a cycle share program? Often, it's a race to the docking station which the app is showing as having available bikes. Being data-savy, I want to utilise publicly available data to predict if a station in San Francisco is going to have a few bikes in some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in /usr/local/envs/py2env/lib/python2.7/site-packages\n",
      "Requirement already satisfied: branca in /usr/local/envs/py2env/lib/python2.7/site-packages (from folium)\n",
      "Requirement already satisfied: six in /usr/local/envs/py2env/lib/python2.7/site-packages (from folium)\n",
      "Requirement already satisfied: jinja2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from folium)\n",
      "Requirement already satisfied: requests in /usr/local/envs/py2env/lib/python2.7/site-packages (from folium)\n",
      "Requirement already satisfied: MarkupSafe in /usr/local/envs/py2env/lib/python2.7/site-packages (from jinja2->folium)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "!pip install folium \n",
    "\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from folium.plugins import HeatMap\n",
    "from google.cloud import bigquery\n",
    "import google.datalab.bigquery as bq\n",
    "from random import randrange\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqsv\" id=\"1_152332694023\"></div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "      });\n",
       "\n",
       "      require(['datalab/bigquery', 'datalab/element!1_152332694023',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/bigquery.css'],\n",
       "        function(bq, dom) {\n",
       "          bq.renderSchema(dom, [{\"description\": \"Station ID number\", \"type\": \"INTEGER\", \"name\": \"station_id\", \"mode\": \"REQUIRED\"}, {\"description\": \"Name of station\", \"type\": \"STRING\", \"name\": \"name\", \"mode\": \"NULLABLE\"}, {\"description\": \"Latitude\", \"type\": \"FLOAT\", \"name\": \"latitude\", \"mode\": \"NULLABLE\"}, {\"description\": \"Longitude\", \"type\": \"FLOAT\", \"name\": \"longitude\", \"mode\": \"NULLABLE\"}, {\"description\": \"Number of total docks at station\", \"type\": \"INTEGER\", \"name\": \"dockcount\", \"mode\": \"NULLABLE\"}, {\"description\": \"City (San Francisco, Redwood City, Palo Alto, Mountain View, San Jose)\", \"type\": \"STRING\", \"name\": \"landmark\", \"mode\": \"NULLABLE\"}, {\"description\": \"Original date that station was installed. If station was moved, it is noted below.\", \"type\": \"DATE\", \"name\": \"installation_date\", \"mode\": \"NULLABLE\"}]);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq tables describe --name bigquery-public-data.san_francisco.bikeshare_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name stations_by_landmark\n",
    "\n",
    "select stations.landmark as area, count(distinct stations.station_id) as cd from `bigquery-public-data.san_francisco.bikeshare_stations` stations\n",
    "group by stations.landmark\n",
    "order by cd desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%chart columns --data stations_by_landmark --fields area,cd\n",
    "{\n",
    "  \"legend\":{\"position\":\"none\"},\n",
    "  \"vAxis\": {\n",
    "    \"title\":\"number of docking stations\"\n",
    "  },\n",
    "  \"hAxis\": {\n",
    "    \"title\": \"location of docking station\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that most of the stations are in San Francisco, and there are a few more around other areas. Let's take a look at data from one station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqsv\" id=\"3_152332012910\"></div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "      });\n",
       "\n",
       "      require(['datalab/bigquery', 'datalab/element!3_152332012910',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/bigquery.css'],\n",
       "        function(bq, dom) {\n",
       "          bq.renderSchema(dom, [{\"description\": \"Station ID number\", \"type\": \"INTEGER\", \"name\": \"station_id\", \"mode\": \"REQUIRED\"}, {\"description\": \"Number of available bikes\", \"type\": \"INTEGER\", \"name\": \"bikes_available\", \"mode\": \"NULLABLE\"}, {\"description\": \"Number of available docks\", \"type\": \"INTEGER\", \"name\": \"docks_available\", \"mode\": \"NULLABLE\"}, {\"description\": \"Date and time, PST\", \"type\": \"TIMESTAMP\", \"name\": \"time\", \"mode\": \"NULLABLE\"}]);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq tables describe --name bigquery-public-data.san_francisco.bikeshare_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name station-91\n",
    "\n",
    "SELECT \n",
    "    status.time\n",
    "  , status.bikes_available\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_stations` stations \n",
    "JOIN `bigquery-public-data.san_francisco.bikeshare_status` status on stations.station_id = status.station_id\n",
    "where stations.station_id = 91\n",
    "order by status.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%chart line --data station-91 --fields time,bikes_available\n",
    "{\n",
    "  \"legend\":{\"position\":\"none\"},\n",
    "  \"vAxis\": {\n",
    "    \"title\":\"number of bikes available\"\n",
    "  },\n",
    "  \"hAxis\": {\n",
    "    \"title\": \"time\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Uh-oh - that time series is looking pretty short! Maybe it's just this station?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqtv\" id=\"4_152332093531\"><table><tr><th>station_id</th><th>cs</th></tr><tr><td>6</td><td>1559598</td></tr><tr><td>5</td><td>1559598</td></tr><tr><td>7</td><td>1559598</td></tr><tr><td>16</td><td>1559597</td></tr><tr><td>11</td><td>1559597</td></tr><tr><td>12</td><td>1559597</td></tr><tr><td>13</td><td>1559597</td></tr><tr><td>27</td><td>1559597</td></tr><tr><td>9</td><td>1559597</td></tr><tr><td>10</td><td>1559597</td></tr></table></div>\n",
       "    <br />(rows: 10, time: 3.1s,   860MB processed, job: job_OJV7QGMUFoIWQVzK4af3BSJg9bvz)<br />\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting', 'datalab/element!4_152332093531', 'base/js/events',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/charting.css'],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render('gcharts', dom, events, 'table', [], {\"rows\": [{\"c\": [{\"v\": 6}, {\"v\": 1559598}]}, {\"c\": [{\"v\": 5}, {\"v\": 1559598}]}, {\"c\": [{\"v\": 7}, {\"v\": 1559598}]}, {\"c\": [{\"v\": 16}, {\"v\": 1559597}]}, {\"c\": [{\"v\": 11}, {\"v\": 1559597}]}, {\"c\": [{\"v\": 12}, {\"v\": 1559597}]}, {\"c\": [{\"v\": 13}, {\"v\": 1559597}]}, {\"c\": [{\"v\": 27}, {\"v\": 1559597}]}, {\"c\": [{\"v\": 9}, {\"v\": 1559597}]}, {\"c\": [{\"v\": 10}, {\"v\": 1559597}]}], \"cols\": [{\"type\": \"number\", \"id\": \"station_id\", \"label\": \"station_id\"}, {\"type\": \"number\", \"id\": \"cs\", \"label\": \"cs\"}]},\n",
       "            {\n",
       "              pageSize: 25,\n",
       "              cssClassNames:  {\n",
       "                tableRow: 'gchart-table-row',\n",
       "                headerRow: 'gchart-table-headerrow',\n",
       "                oddTableRow: 'gchart-table-oddrow',\n",
       "                selectedTableRow: 'gchart-table-selectedrow',\n",
       "                hoverTableRow: 'gchart-table-hoverrow',\n",
       "                tableCell: 'gchart-table-cell',\n",
       "                headerCell: 'gchart-table-headercell',\n",
       "                rowNumberCell: 'gchart-table-rownumcell'\n",
       "              }\n",
       "            },\n",
       "            {source_index: 0, fields: 'station_id,cs'},\n",
       "            0,\n",
       "            10);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "QueryResultsTable job_OJV7QGMUFoIWQVzK4af3BSJg9bvz"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq query\n",
    "\n",
    "SELECT station_id, count(*) as cs from `bigquery-public-data.san_francisco.bikeshare_status` \n",
    "group by station_id\n",
    "order by cs desc\n",
    "limit 10\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqtv\" id=\"1_152349333674\"><table><tr><th>f0_</th><th>f1_</th></tr><tr><td>2013-08-29 12:06:01</td><td>2016-08-31 23:58:59</td></tr></table></div>\n",
       "    <br />(rows: 1, time: 5.5s,     1GB processed, job: job_4X8kaIc-et0A9Pj1RyswrUQ5xy3W)<br />\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting', 'datalab/element!1_152349333674', 'base/js/events',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/charting.css'],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render('gcharts', dom, events, 'table', [], {\"rows\": [{\"c\": [{\"v\": \"2013-08-29T12:06:01\"}, {\"v\": \"2016-08-31T23:58:59\"}]}], \"cols\": [{\"type\": \"timestamp\", \"id\": \"f0_\", \"label\": \"f0_\"}, {\"type\": \"timestamp\", \"id\": \"f1_\", \"label\": \"f1_\"}]},\n",
       "            {\n",
       "              pageSize: 25,\n",
       "              cssClassNames:  {\n",
       "                tableRow: 'gchart-table-row',\n",
       "                headerRow: 'gchart-table-headerrow',\n",
       "                oddTableRow: 'gchart-table-oddrow',\n",
       "                selectedTableRow: 'gchart-table-selectedrow',\n",
       "                hoverTableRow: 'gchart-table-hoverrow',\n",
       "                tableCell: 'gchart-table-cell',\n",
       "                headerCell: 'gchart-table-headercell',\n",
       "                rowNumberCell: 'gchart-table-rownumcell'\n",
       "              }\n",
       "            },\n",
       "            {source_index: 0, fields: 'f0_,f1_'},\n",
       "            0,\n",
       "            1);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "QueryResultsTable job_4X8kaIc-et0A9Pj1RyswrUQ5xy3W"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq query\n",
    "\n",
    "SELECT min(time) as min_time, max(time) as max_time FROM `bigquery-public-data.san_francisco.bikeshare_status` status\n",
    "join (\n",
    "SELECT station_id, count(*) as cs from `bigquery-public-data.san_francisco.bikeshare_status` \n",
    "group by station_id\n",
    "order by cs desc\n",
    "limit 10) top_10 on top_10.station_id = status.station_id\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name count_by_day\n",
    "select extract(date from time) as date, count(*) as cs\n",
    "from  `bigquery-public-data.san_francisco.bikeshare_status`\n",
    "group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqgc-container\">\n",
       "      \n",
       "      <div class=\"bqgc \" id=\"2_152349354355\">\n",
       "      </div>\n",
       "    </div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting',\n",
       "               'datalab/element!2_152349354355',\n",
       "               'base/js/events',\n",
       "               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n",
       "              ],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render(\n",
       "              'gcharts',\n",
       "              dom,\n",
       "              events,\n",
       "              'columns',\n",
       "              [],\n",
       "              {\"rows\": [{\"c\": [{\"v\": \"2013-09-06\"}, {\"v\": 81408}]}, {\"c\": [{\"v\": \"2013-09-02\"}, {\"v\": 81408}]}, {\"c\": [{\"v\": \"2013-08-30\"}, {\"v\": 82432}]}, {\"c\": [{\"v\": \"2013-10-15\"}, {\"v\": 87040}]}, {\"c\": [{\"v\": \"2013-09-12\"}, {\"v\": 87296}]}, {\"c\": [{\"v\": \"2013-09-11\"}, {\"v\": 87552}]}, {\"c\": [{\"v\": \"2013-09-25\"}, {\"v\": 88064}]}, {\"c\": [{\"v\": \"2013-11-02\"}, {\"v\": 88576}]}, {\"c\": [{\"v\": \"2013-09-23\"}, {\"v\": 89600}]}, {\"c\": [{\"v\": \"2013-09-20\"}, {\"v\": 90112}]}, {\"c\": [{\"v\": \"2013-10-14\"}, {\"v\": 90368}]}, {\"c\": [{\"v\": \"2013-10-01\"}, {\"v\": 90368}]}, {\"c\": [{\"v\": \"2013-10-02\"}, {\"v\": 90624}]}, {\"c\": [{\"v\": \"2013-09-30\"}, {\"v\": 90624}]}, {\"c\": [{\"v\": \"2013-10-07\"}, {\"v\": 90880}]}, {\"c\": [{\"v\": \"2013-09-16\"}, {\"v\": 91136}]}, {\"c\": [{\"v\": \"2013-10-11\"}, {\"v\": 91136}]}, {\"c\": [{\"v\": \"2013-09-17\"}, {\"v\": 91392}]}, {\"c\": [{\"v\": \"2013-12-01\"}, {\"v\": 91648}]}, {\"c\": [{\"v\": \"2013-10-06\"}, {\"v\": 91904}]}, {\"c\": [{\"v\": \"2013-12-20\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-14\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-07\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-06\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-12\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-18\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-04\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-16\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-19\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-21\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-05\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-26\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-19\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-10-28\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-10-29\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-10\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-10-31\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-10-25\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-11\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-06\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-03\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-02\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-09\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-04\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-22\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-10\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-15\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-23\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-07\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-08\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-05\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-28\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-16\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-29\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-27\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-11\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-25\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-08\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-09\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-17\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-13\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-30\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-28\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-27\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-17\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-24\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-20\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-14\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-29\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-15\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-26\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-13\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-23\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-11-21\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-22\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-25\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2013-12-24\"}, {\"v\": 92160}]}, {\"c\": [{\"v\": \"2016-05-03\"}, {\"v\": 100100}]}, {\"c\": [{\"v\": \"2016-04-08\"}, {\"v\": 100100}]}, {\"c\": [{\"v\": \"2016-06-23\"}, {\"v\": 91910}]}, {\"c\": [{\"v\": \"2013-09-01\"}, {\"v\": 82695}]}, {\"c\": [{\"v\": \"2016-08-23\"}, {\"v\": 84487}]}, {\"c\": [{\"v\": \"2016-07-18\"}, {\"v\": 89607}]}, {\"c\": [{\"v\": \"2014-01-04\"}, {\"v\": 91656}]}, {\"c\": [{\"v\": \"2016-06-07\"}, {\"v\": 93450}]}, {\"c\": [{\"v\": \"2014-03-01\"}, {\"v\": 99084}]}, {\"c\": [{\"v\": \"2015-09-26\"}, {\"v\": 100625}]}, {\"c\": [{\"v\": \"2016-05-18\"}, {\"v\": 96530}]}, {\"c\": [{\"v\": \"2016-05-20\"}, {\"v\": 96530}]}, {\"c\": [{\"v\": \"2016-03-13\"}, {\"v\": 96530}]}, {\"c\": [{\"v\": \"2015-06-12\"}, {\"v\": 100627}]}, {\"c\": [{\"v\": \"2016-04-21\"}, {\"v\": 97300}]}, {\"c\": [{\"v\": \"2014-08-20\"}, {\"v\": 100629}]}, {\"c\": [{\"v\": \"2016-08-27\"}, {\"v\": 79127}]}, {\"c\": [{\"v\": \"2016-08-28\"}, {\"v\": 79127}]}, {\"c\": [{\"v\": \"2016-07-06\"}, {\"v\": 88855}]}, {\"c\": [{\"v\": \"2014-01-01\"}, {\"v\": 96279}]}, {\"c\": [{\"v\": \"2016-05-15\"}, {\"v\": 98840}]}, {\"c\": [{\"v\": \"2016-07-16\"}, {\"v\": 89115}]}, {\"c\": [{\"v\": \"2016-08-07\"}, {\"v\": 90651}]}, {\"c\": [{\"v\": \"2015-06-06\"}, {\"v\": 82460}]}, {\"c\": [{\"v\": \"2015-06-09\"}, {\"v\": 100380}]}, {\"c\": [{\"v\": \"2016-06-22\"}, {\"v\": 92960}]}, {\"c\": [{\"v\": \"2014-02-25\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-17\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-10\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-02-26\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-05\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-12\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-14\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-31\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-08-21\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-26\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-24\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2015-09-27\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-21\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-25\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-04-03\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-06-14\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-19\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-04-05\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-04-07\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-18\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-04-06\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-27\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-04-04\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-04-01\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-03\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-23\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-15\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-22\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-20\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-13\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-07\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-04-08\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-02-21\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-04-02\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-16\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-06-29\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-02-28\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-02-27\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-02-22\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-11\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-02-23\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-05-04\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-02-24\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-03-08\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2015-06-14\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2015-06-13\"}, {\"v\": 99360}]}, {\"c\": [{\"v\": \"2014-01-24\"}, {\"v\": 95268}]}, {\"c\": [{\"v\": \"2016-06-11\"}, {\"v\": 95270}]}, {\"c\": [{\"v\": \"2016-05-06\"}, {\"v\": 95270}]}, {\"c\": [{\"v\": \"2014-05-05\"}, {\"v\": 100394}]}, {\"c\": [{\"v\": \"2016-08-15\"}, {\"v\": 84267}]}, {\"c\": [{\"v\": \"2016-05-01\"}, {\"v\": 91691}]}, {\"c\": [{\"v\": \"2016-04-19\"}, {\"v\": 97580}]}, {\"c\": [{\"v\": \"2016-02-17\"}, {\"v\": 99120}]}, {\"c\": [{\"v\": \"2015-04-10\"}, {\"v\": 99120}]}, {\"c\": [{\"v\": \"2016-04-03\"}, {\"v\": 99120}]}, {\"c\": [{\"v\": \"2014-12-16\"}, {\"v\": 99890}]}, {\"c\": [{\"v\": \"2015-06-01\"}, {\"v\": 99890}]}, {\"c\": [{\"v\": \"2016-07-20\"}, {\"v\": 90675}]}, {\"c\": [{\"v\": \"2016-07-11\"}, {\"v\": 90675}]}, {\"c\": [{\"v\": \"2016-08-31\"}, {\"v\": 81204}]}, {\"c\": [{\"v\": \"2016-08-18\"}, {\"v\": 81204}]}, {\"c\": [{\"v\": \"2016-07-28\"}, {\"v\": 86580}]}, {\"c\": [{\"v\": \"2015-05-05\"}, {\"v\": 100660}]}, {\"c\": [{\"v\": \"2015-01-07\"}, {\"v\": 100660}]}, {\"c\": [{\"v\": \"2015-11-09\"}, {\"v\": 100660}]}, {\"c\": [{\"v\": \"2014-10-27\"}, {\"v\": 100660}]}, {\"c\": [{\"v\": \"2015-02-12\"}, {\"v\": 100660}]}, {\"c\": [{\"v\": \"2015-07-03\"}, {\"v\": 100660}]}, {\"c\": [{\"v\": \"2014-05-17\"}, {\"v\": 100150}]}, {\"c\": [{\"v\": \"2016-07-12\"}, {\"v\": 86840}]}, {\"c\": [{\"v\": \"2016-06-13\"}, {\"v\": 93240}]}, {\"c\": [{\"v\": \"2015-07-08\"}, {\"v\": 93240}]}, {\"c\": [{\"v\": \"2014-02-20\"}, {\"v\": 98616}]}, {\"c\": [{\"v\": \"2016-05-21\"}, {\"v\": 94010}]}, {\"c\": [{\"v\": \"2016-05-10\"}, {\"v\": 94010}]}, {\"c\": [{\"v\": \"2016-08-26\"}, {\"v\": 79931}]}, {\"c\": [{\"v\": \"2016-08-29\"}, {\"v\": 79931}]}, {\"c\": [{\"v\": \"2016-06-12\"}, {\"v\": 94780}]}, {\"c\": [{\"v\": \"2016-05-23\"}, {\"v\": 94780}]}, {\"c\": [{\"v\": \"2016-06-03\"}, {\"v\": 95550}]}, {\"c\": [{\"v\": \"2014-01-22\"}, {\"v\": 97599}]}, {\"c\": [{\"v\": \"2013-09-08\"}, {\"v\": 80192}]}, {\"c\": [{\"v\": \"2013-09-03\"}, {\"v\": 81472}]}, {\"c\": [{\"v\": \"2013-09-07\"}, {\"v\": 81728}]}, {\"c\": [{\"v\": \"2013-10-16\"}, {\"v\": 84288}]}, {\"c\": [{\"v\": \"2013-09-05\"}, {\"v\": 84288}]}, {\"c\": [{\"v\": \"2013-09-26\"}, {\"v\": 87616}]}, {\"c\": [{\"v\": \"2013-09-29\"}, {\"v\": 90432}]}, {\"c\": [{\"v\": \"2013-09-15\"}, {\"v\": 90944}]}, {\"c\": [{\"v\": \"2013-10-19\"}, {\"v\": 91456}]}, {\"c\": [{\"v\": \"2013-10-05\"}, {\"v\": 91456}]}, {\"c\": [{\"v\": \"2013-10-03\"}, {\"v\": 91712}]}, {\"c\": [{\"v\": \"2013-10-08\"}, {\"v\": 91712}]}, {\"c\": [{\"v\": \"2013-10-24\"}, {\"v\": 91968}]}, {\"c\": [{\"v\": \"2013-10-26\"}, {\"v\": 84289}]}, {\"c\": [{\"v\": \"2016-04-25\"}, {\"v\": 97090}]}, {\"c\": [{\"v\": \"2016-08-11\"}, {\"v\": 85827}]}, {\"c\": [{\"v\": \"2015-06-15\"}, {\"v\": 100419}]}, {\"c\": [{\"v\": \"2015-11-03\"}, {\"v\": 97860}]}, {\"c\": [{\"v\": \"2014-05-03\"}, {\"v\": 100420}]}, {\"c\": [{\"v\": \"2014-06-16\"}, {\"v\": 100676}]}, {\"c\": [{\"v\": \"2016-04-18\"}, {\"v\": 98630}]}, {\"c\": [{\"v\": \"2014-07-01\"}, {\"v\": 100679}]}, {\"c\": [{\"v\": \"2016-08-17\"}, {\"v\": 81740}]}, {\"c\": [{\"v\": \"2016-06-28\"}, {\"v\": 92750}]}, {\"c\": [{\"v\": \"2016-05-02\"}, {\"v\": 93520}]}, {\"c\": [{\"v\": \"2016-05-17\"}, {\"v\": 93520}]}, {\"c\": [{\"v\": \"2016-06-26\"}, {\"v\": 93520}]}, {\"c\": [{\"v\": \"2016-07-10\"}, {\"v\": 84305}]}, {\"c\": [{\"v\": \"2014-06-30\"}, {\"v\": 100433}]}, {\"c\": [{\"v\": \"2016-06-27\"}, {\"v\": 94290}]}, {\"c\": [{\"v\": \"2016-06-08\"}, {\"v\": 95830}]}, {\"c\": [{\"v\": \"2015-03-08\"}, {\"v\": 96600}]}, {\"c\": [{\"v\": \"2016-05-27\"}, {\"v\": 97370}]}, {\"c\": [{\"v\": \"2016-05-16\"}, {\"v\": 97370}]}, {\"c\": [{\"v\": \"2016-04-01\"}, {\"v\": 97370}]}, {\"c\": [{\"v\": \"2016-07-25\"}, {\"v\": 89180}]}, {\"c\": [{\"v\": \"2014-02-19\"}, {\"v\": 94044}]}, {\"c\": [{\"v\": \"2016-04-11\"}, {\"v\": 98910}]}, {\"c\": [{\"v\": \"2016-04-06\"}, {\"v\": 98910}]}, {\"c\": [{\"v\": \"2015-09-28\"}, {\"v\": 100446}]}, {\"c\": [{\"v\": \"2014-03-04\"}, {\"v\": 99168}]}, {\"c\": [{\"v\": \"2016-05-14\"}, {\"v\": 99680}]}, {\"c\": [{\"v\": \"2014-03-28\"}, {\"v\": 87906}]}, {\"c\": [{\"v\": \"2014-05-18\"}, {\"v\": 99426}]}, {\"c\": [{\"v\": \"2014-04-11\"}, {\"v\": 100450}]}, {\"c\": [{\"v\": \"2014-11-15\"}, {\"v\": 100450}]}, {\"c\": [{\"v\": \"2016-08-05\"}, {\"v\": 89445}]}, {\"c\": [{\"v\": \"2014-04-22\"}, {\"v\": 99941}]}, {\"c\": [{\"v\": \"2016-06-04\"}, {\"v\": 93030}]}, {\"c\": [{\"v\": \"2016-07-19\"}, {\"v\": 89960}]}, {\"c\": [{\"v\": \"2016-06-25\"}, {\"v\": 93800}]}, {\"c\": [{\"v\": \"2016-06-18\"}, {\"v\": 93800}]}, {\"c\": [{\"v\": \"2016-08-02\"}, {\"v\": 85865}]}, {\"c\": [{\"v\": \"2016-06-09\"}, {\"v\": 94570}]}, {\"c\": [{\"v\": \"2016-08-08\"}, {\"v\": 88172}]}, {\"c\": [{\"v\": \"2016-08-24\"}, {\"v\": 81005}]}, {\"c\": [{\"v\": \"2016-05-05\"}, {\"v\": 96110}]}, {\"c\": [{\"v\": \"2016-07-21\"}, {\"v\": 90480}]}, {\"c\": [{\"v\": \"2016-04-28\"}, {\"v\": 96880}]}, {\"c\": [{\"v\": \"2016-04-24\"}, {\"v\": 96880}]}, {\"c\": [{\"v\": \"2015-08-02\"}, {\"v\": 96880}]}, {\"c\": [{\"v\": \"2016-06-06\"}, {\"v\": 97650}]}, {\"c\": [{\"v\": \"2016-04-10\"}, {\"v\": 97650}]}, {\"c\": [{\"v\": \"2016-05-30\"}, {\"v\": 97650}]}, {\"c\": [{\"v\": \"2016-07-01\"}, {\"v\": 83316}]}, {\"c\": [{\"v\": \"2016-04-23\"}, {\"v\": 98420}]}, {\"c\": [{\"v\": \"2013-11-01\"}, {\"v\": 90229}]}, {\"c\": [{\"v\": \"2016-07-02\"}, {\"v\": 81270}]}, {\"c\": [{\"v\": \"2016-04-13\"}, {\"v\": 99190}]}, {\"c\": [{\"v\": \"2014-09-17\"}, {\"v\": 63350}]}, {\"c\": [{\"v\": \"2014-03-02\"}, {\"v\": 99191}]}, {\"c\": [{\"v\": \"2016-07-29\"}, {\"v\": 86905}]}, {\"c\": [{\"v\": \"2016-07-08\"}, {\"v\": 86905}]}, {\"c\": [{\"v\": \"2016-08-14\"}, {\"v\": 86394}]}, {\"c\": [{\"v\": \"2015-06-07\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-07\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-18\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-19\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2014-11-03\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-26\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-24\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-27\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-21\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2014-10-01\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2015-03-17\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2015-04-02\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-29\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2014-05-02\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2015-07-27\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-27\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-23\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2015-07-28\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-25\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2015-03-30\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-28\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-22\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-06\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-10\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-01\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-24\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2015-01-31\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-18\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-09\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2014-04-28\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-11\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-19\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-20\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2014-10-25\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-02-16\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-03\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-30\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-16\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2015-05-30\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2015-05-31\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-04\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-29\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-12\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2015-11-08\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-28\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-05\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-17\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-20\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2016-03-06\"}, {\"v\": 100730}]}, {\"c\": [{\"v\": \"2014-03-30\"}, {\"v\": 30843}]}, {\"c\": [{\"v\": \"2016-07-24\"}, {\"v\": 91260}]}, {\"c\": [{\"v\": \"2016-06-21\"}, {\"v\": 92540}]}, {\"c\": [{\"v\": \"2016-07-07\"}, {\"v\": 87165}]}, {\"c\": [{\"v\": \"2016-08-25\"}, {\"v\": 79998}]}, {\"c\": [{\"v\": \"2016-06-15\"}, {\"v\": 93310}]}, {\"c\": [{\"v\": \"2013-09-04\"}, {\"v\": 82560}]}, {\"c\": [{\"v\": \"2013-09-10\"}, {\"v\": 83840}]}, {\"c\": [{\"v\": \"2013-09-13\"}, {\"v\": 87680}]}, {\"c\": [{\"v\": \"2013-09-27\"}, {\"v\": 88448}]}, {\"c\": [{\"v\": \"2013-09-19\"}, {\"v\": 90752}]}, {\"c\": [{\"v\": \"2013-10-09\"}, {\"v\": 91264}]}, {\"c\": [{\"v\": \"2013-10-10\"}, {\"v\": 91264}]}, {\"c\": [{\"v\": \"2013-09-18\"}, {\"v\": 91520}]}, {\"c\": [{\"v\": \"2013-10-04\"}, {\"v\": 91776}]}, {\"c\": [{\"v\": \"2013-10-27\"}, {\"v\": 92032}]}, {\"c\": [{\"v\": \"2014-02-12\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-01-27\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-05\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-01-30\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-18\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-01-28\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-01-29\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-01-23\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-13\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-06\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-14\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-03\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-10\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-17\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-15\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-04\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-07\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-02\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-16\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-09\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-08\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-02-11\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-01-26\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2014-01-25\"}, {\"v\": 97920}]}, {\"c\": [{\"v\": \"2013-08-29\"}, {\"v\": 41088}]}, {\"c\": [{\"v\": \"2016-07-30\"}, {\"v\": 87425}]}, {\"c\": [{\"v\": \"2014-10-13\"}, {\"v\": 100737}]}, {\"c\": [{\"v\": \"2016-08-13\"}, {\"v\": 86658}]}, {\"c\": [{\"v\": \"2016-06-20\"}, {\"v\": 94850}]}, {\"c\": [{\"v\": \"2016-05-11\"}, {\"v\": 95620}]}, {\"c\": [{\"v\": \"2016-07-31\"}, {\"v\": 87685}]}, {\"c\": [{\"v\": \"2016-06-02\"}, {\"v\": 97160}]}, {\"c\": [{\"v\": \"2016-07-13\"}, {\"v\": 87945}]}, {\"c\": [{\"v\": \"2016-04-26\"}, {\"v\": 97930}]}, {\"c\": [{\"v\": \"2016-04-02\"}, {\"v\": 97930}]}, {\"c\": [{\"v\": \"2016-08-04\"}, {\"v\": 88461}]}, {\"c\": [{\"v\": \"2016-04-16\"}, {\"v\": 99470}]}, {\"c\": [{\"v\": \"2016-05-04\"}, {\"v\": 99470}]}, {\"c\": [{\"v\": \"2014-06-13\"}, {\"v\": 100494}]}, {\"c\": [{\"v\": \"2014-01-05\"}, {\"v\": 88976}]}, {\"c\": [{\"v\": \"2015-12-20\"}, {\"v\": 100240}]}, {\"c\": [{\"v\": \"2016-07-15\"}, {\"v\": 88465}]}, {\"c\": [{\"v\": \"2016-07-09\"}, {\"v\": 84370}]}, {\"c\": [{\"v\": \"2015-04-21\"}, {\"v\": 92050}]}, {\"c\": [{\"v\": \"2016-06-30\"}, {\"v\": 92050}]}, {\"c\": [{\"v\": \"2016-08-19\"}, {\"v\": 83348}]}, {\"c\": [{\"v\": \"2016-08-01\"}, {\"v\": 88725}]}, {\"c\": [{\"v\": \"2016-05-24\"}, {\"v\": 94360}]}, {\"c\": [{\"v\": \"2016-07-03\"}, {\"v\": 83097}]}, {\"c\": [{\"v\": \"2016-08-22\"}, {\"v\": 84889}]}, {\"c\": [{\"v\": \"2016-08-03\"}, {\"v\": 88985}]}, {\"c\": [{\"v\": \"2016-07-17\"}, {\"v\": 88985}]}, {\"c\": [{\"v\": \"2016-06-05\"}, {\"v\": 95130}]}, {\"c\": [{\"v\": \"2016-05-09\"}, {\"v\": 95130}]}, {\"c\": [{\"v\": \"2016-06-01\"}, {\"v\": 95900}]}, {\"c\": [{\"v\": \"2016-07-22\"}, {\"v\": 89245}]}, {\"c\": [{\"v\": \"2014-01-16\"}, {\"v\": 96413}]}, {\"c\": [{\"v\": \"2016-05-13\"}, {\"v\": 96670}]}, {\"c\": [{\"v\": \"2016-05-07\"}, {\"v\": 96670}]}, {\"c\": [{\"v\": \"2016-04-20\"}, {\"v\": 97440}]}, {\"c\": [{\"v\": \"2016-04-07\"}, {\"v\": 98980}]}, {\"c\": [{\"v\": \"2015-09-14\"}, {\"v\": 100773}]}, {\"c\": [{\"v\": \"2016-06-14\"}, {\"v\": 90790}]}, {\"c\": [{\"v\": \"2015-04-07\"}, {\"v\": 100520}]}, {\"c\": [{\"v\": \"2015-07-06\"}, {\"v\": 100520}]}, {\"c\": [{\"v\": \"2015-08-21\"}, {\"v\": 100520}]}, {\"c\": [{\"v\": \"2016-06-17\"}, {\"v\": 92330}]}, {\"c\": [{\"v\": \"2016-04-30\"}, {\"v\": 92330}]}, {\"c\": [{\"v\": \"2016-06-16\"}, {\"v\": 93100}]}, {\"c\": [{\"v\": \"2016-08-20\"}, {\"v\": 81070}]}, {\"c\": [{\"v\": \"2014-03-29\"}, {\"v\": 51888}]}, {\"c\": [{\"v\": \"2016-08-09\"}, {\"v\": 85425}]}, {\"c\": [{\"v\": \"2016-06-19\"}, {\"v\": 95410}]}, {\"c\": [{\"v\": \"2016-08-06\"}, {\"v\": 89780}]}, {\"c\": [{\"v\": \"2016-05-25\"}, {\"v\": 96180}]}, {\"c\": [{\"v\": \"2014-02-01\"}, {\"v\": 97716}]}, {\"c\": [{\"v\": \"2014-06-28\"}, {\"v\": 100533}]}, {\"c\": [{\"v\": \"2016-05-29\"}, {\"v\": 97720}]}, {\"c\": [{\"v\": \"2015-04-29\"}, {\"v\": 100794}]}, {\"c\": [{\"v\": \"2015-12-02\"}, {\"v\": 99260}]}, {\"c\": [{\"v\": \"2015-04-25\"}, {\"v\": 100284}]}, {\"c\": [{\"v\": \"2014-07-24\"}, {\"v\": 100796}]}, {\"c\": [{\"v\": \"2014-07-18\"}, {\"v\": 100030}]}, {\"c\": [{\"v\": \"2014-05-15\"}, {\"v\": 100798}]}, {\"c\": [{\"v\": \"2014-12-17\"}, {\"v\": 100799}]}, {\"c\": [{\"v\": \"2013-09-09\"}, {\"v\": 80320}]}, {\"c\": [{\"v\": \"2013-08-31\"}, {\"v\": 84160}]}, {\"c\": [{\"v\": \"2013-10-17\"}, {\"v\": 88000}]}, {\"c\": [{\"v\": \"2013-09-24\"}, {\"v\": 88768}]}, {\"c\": [{\"v\": \"2013-09-22\"}, {\"v\": 90304}]}, {\"c\": [{\"v\": \"2013-10-13\"}, {\"v\": 90304}]}, {\"c\": [{\"v\": \"2013-09-21\"}, {\"v\": 90304}]}, {\"c\": [{\"v\": \"2013-09-28\"}, {\"v\": 90560}]}, {\"c\": [{\"v\": \"2013-10-12\"}, {\"v\": 90816}]}, {\"c\": [{\"v\": \"2013-09-14\"}, {\"v\": 91072}]}, {\"c\": [{\"v\": \"2013-11-03\"}, {\"v\": 91328}]}, {\"c\": [{\"v\": \"2013-10-21\"}, {\"v\": 92096}]}, {\"c\": [{\"v\": \"2013-10-22\"}, {\"v\": 92096}]}, {\"c\": [{\"v\": \"2013-10-18\"}, {\"v\": 92096}]}, {\"c\": [{\"v\": \"2013-10-20\"}, {\"v\": 92096}]}, {\"c\": [{\"v\": \"2013-12-12\"}, {\"v\": 92096}]}, {\"c\": [{\"v\": \"2013-10-23\"}, {\"v\": 92096}]}, {\"c\": [{\"v\": \"2013-10-30\"}, {\"v\": 92096}]}, {\"c\": [{\"v\": \"2013-11-18\"}, {\"v\": 92096}]}, {\"c\": [{\"v\": \"2013-11-30\"}, {\"v\": 92096}]}, {\"c\": [{\"v\": \"2015-06-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-03-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-03-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-03-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-03-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-03-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-03-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-02-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-05-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-12-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-31\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-03-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-14\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-01-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-25\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-04-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-07-02\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-16\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-30\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-20\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-08-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-03-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-03\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-15\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-12-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-03-22\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-07\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-18\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-26\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-08-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-09-06\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-17\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-02-08\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-04-05\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-06-01\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-27\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-11\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-11-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-01-19\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-10-10\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-28\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-09\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-12\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-10-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-11-29\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-09-13\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-24\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-05-23\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2015-06-21\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2014-07-04\"}, {\"v\": 100800}]}, {\"c\": [{\"v\": \"2016-06-29\"}, {\"v\": 93380}]}, {\"c\": [{\"v\": \"2016-08-16\"}, {\"v\": 81606}]}, {\"c\": [{\"v\": \"2016-08-10\"}, {\"v\": 88775}]}, {\"c\": [{\"v\": \"2016-05-22\"}, {\"v\": 94920}]}, {\"c\": [{\"v\": \"2015-04-26\"}, {\"v\": 99528}]}, {\"c\": [{\"v\": \"2015-06-02\"}, {\"v\": 100555}]}, {\"c\": [{\"v\": \"2016-04-22\"}, {\"v\": 97230}]}, {\"c\": [{\"v\": \"2014-04-09\"}, {\"v\": 100047}]}, {\"c\": [{\"v\": \"2016-04-09\"}, {\"v\": 98000}]}, {\"c\": [{\"v\": \"2016-04-17\"}, {\"v\": 98770}]}, {\"c\": [{\"v\": \"2016-03-31\"}, {\"v\": 98770}]}, {\"c\": [{\"v\": \"2016-04-14\"}, {\"v\": 98770}]}, {\"c\": [{\"v\": \"2015-08-13\"}, {\"v\": 98770}]}, {\"c\": [{\"v\": \"2016-04-04\"}, {\"v\": 99540}]}, {\"c\": [{\"v\": \"2014-08-22\"}, {\"v\": 99542}]}, {\"c\": [{\"v\": \"2015-05-06\"}, {\"v\": 100310}]}, {\"c\": [{\"v\": \"2015-10-21\"}, {\"v\": 100310}]}, {\"c\": [{\"v\": \"2016-07-14\"}, {\"v\": 89050}]}, {\"c\": [{\"v\": \"2016-07-26\"}, {\"v\": 84955}]}, {\"c\": [{\"v\": \"2014-03-06\"}, {\"v\": 99291}]}, {\"c\": [{\"v\": \"2016-06-10\"}, {\"v\": 94430}]}, {\"c\": [{\"v\": \"2013-12-31\"}, {\"v\": 94942}]}, {\"c\": [{\"v\": \"2016-08-12\"}, {\"v\": 86496}]}, {\"c\": [{\"v\": \"2016-05-28\"}, {\"v\": 95200}]}, {\"c\": [{\"v\": \"2016-06-24\"}, {\"v\": 95200}]}, {\"c\": [{\"v\": \"2014-01-08\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-03\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-07\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-14\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-06\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-12\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-13\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-02\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-15\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-09\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-21\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-17\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-11\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-10\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-18\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-20\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2014-01-19\"}, {\"v\": 96480}]}, {\"c\": [{\"v\": \"2016-07-23\"}, {\"v\": 89570}]}, {\"c\": [{\"v\": \"2016-05-19\"}, {\"v\": 95970}]}, {\"c\": [{\"v\": \"2014-04-23\"}, {\"v\": 99554}]}, {\"c\": [{\"v\": \"2016-04-27\"}, {\"v\": 97510}]}, {\"c\": [{\"v\": \"2016-04-15\"}, {\"v\": 98280}]}, {\"c\": [{\"v\": \"2015-07-07\"}, {\"v\": 99820}]}, {\"c\": [{\"v\": \"2014-10-02\"}, {\"v\": 100590}]}, {\"c\": [{\"v\": \"2016-07-05\"}, {\"v\": 87535}]}, {\"c\": [{\"v\": \"2014-06-15\"}, {\"v\": 99567}]}, {\"c\": [{\"v\": \"2016-07-04\"}, {\"v\": 85491}]}, {\"c\": [{\"v\": \"2016-07-27\"}, {\"v\": 82420}]}, {\"c\": [{\"v\": \"2016-08-21\"}, {\"v\": 85492}]}, {\"c\": [{\"v\": \"2014-03-09\"}, {\"v\": 95220}]}, {\"c\": [{\"v\": \"2016-04-29\"}, {\"v\": 94710}]}, {\"c\": [{\"v\": \"2016-08-30\"}, {\"v\": 79864}]}, {\"c\": [{\"v\": \"2016-05-26\"}, {\"v\": 95480}]}, {\"c\": [{\"v\": \"2016-05-31\"}, {\"v\": 95480}]}, {\"c\": [{\"v\": \"2014-01-31\"}, {\"v\": 97784}]}, {\"c\": [{\"v\": \"2016-05-12\"}, {\"v\": 96250}]}, {\"c\": [{\"v\": \"2016-05-08\"}, {\"v\": 97020}]}, {\"c\": [{\"v\": \"2014-11-14\"}, {\"v\": 100349}]}, {\"c\": [{\"v\": \"2016-04-05\"}, {\"v\": 97790}]}, {\"c\": [{\"v\": \"2016-04-12\"}, {\"v\": 97790}]}, {\"c\": [{\"v\": \"2014-09-16\"}, {\"v\": 52990}]}], \"cols\": [{\"type\": \"date\", \"id\": \"date\", \"label\": \"date\"}, {\"type\": \"number\", \"id\": \"cs\", \"label\": \"cs\"}]},\n",
       "              {},\n",
       "              {\"fields\": \"date,cs\", \"source_index\": 0, \"name\": 0},\n",
       "              0,\n",
       "              1099);\n",
       "          }\n",
       "        );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABmAAAADICAYAAAD/Yi74AAAgAElEQVR4Xu29C/SVVZ3//+GiiJEwhGArKZxInUnDxHDE+UVKUngPERDFCxaIt0HtS1og4AUviMaIGFiC5F0Cx8Vk42RqFIOlLh3H8kpTJDkGGIKigPJfn83/+Xq+h3PO9+xz9j7f/ez9etZqJee7n/3sz+vz2df3s/fTbvv27duFCwIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAwBmBdggwzliSEQQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhAwBBBgCAQIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAoxjoGQHAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABBBgiAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQg4JgAAkwdQLdt2yajRo2SIUOGyLhx45pzevvtt2XmzJmycuVK81uXLl3k0ksvlUMPPbQ5zQsvvCBXXHGFrF+/Xtq1aydnn322jBgxwvx3qUufdeutt8pDDz1k/rzvvvvKlClTZK+99mpO/vDDD8vs2bPlgw8+MM+cPHmy9O/fv/nv+qwZM2bIc889Z34bNGiQNDU1SadOneqgwK0QgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQgUE0CAqTEmVBD59re/LSqknHjiiXLeeeeZnD788EM55ZRTjLAyePBg6dOnj/z4xz+WLVu2yI033igHHnig+dvo0aNNehVuli1bJqtXr5aLLrpIjj766JIlmjt3rixdulQOP/xw2WeffeTOO+80wsmSJUtk1113lSeffNIILr169TLlWbBggWzdulXmz59vyrB9+3ZTrnXr1hnR6PXXX5fly5fLwIEDZdq0aWWFnxrxcBsEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAIGkCCDA1uP+1114z4sumTZvM3YUCjAoyEydOlDFjxsjpp59u/q7iytixY+WEE06Q888/XzIx5aabbpIDDjjACCUqjuy+++5GOOnQoUOLUqlgo/ntt99+MmvWLCOWqGiju12mT58uhx12mJx55pmydu1aeeCBB0w+q1atkvHjx8vxxx8vF1xwgdmNoztmtBz6LBVkrrzySlmxYoUsWrRIevbsWQMJboEABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQKAUAQQYy7hQ4eLUU081YofuJLn//vvluOOOa94Bo2LLpEmTZOrUqbL//vub3FWo+cY3vmGEmnPPPVcuvPBCeeONN+Tee+9tFlt0R4v+7+6775bu3bu3KNVLL71khBvdqaI7YLI8R44cKUOHDjXHl+l/H3HEEWYXjV5aTn3Ohg0bZOHChbJ48WK5/fbb5Y477jC7ZPTKxKKrr75aBgwYYEmC5BCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCJQjgABTQ2w8++yz5hgw/c7KsGHD5KijjjICSbnrl7/8pdltoseO6U4V3YGigsfFF1/cfEsmhtx8883Nwk32R929ouJLoXiSCSx6DNlll11mRCHd4ZIJNHqvCjr33HOPEV90Z81jjz1m/t2xY0eTtQpDKtycddZZMnz48BpIcAsEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAKlCCDA1BEXKmCoqDJkyJCyAkx29JcKJXo8WPv27c3OGf0+TKFok+1yKbUbRY8bmzNnTovdMSrANDU1yZtvvimXX365TJgwQYrv1d05uuvlrrvukltuuUVeffXVFkecbd682QgwlcpfDZ6nn366mmSkgQAEIAABCEAAAhCAAAQgAAEIQAACEIAABCCQGwL9+/evuayHjG+bNdOn5tVe5pqN5cayBBBg6giO1gSMhx56SHRHy6677mqOAdtzzz1F71EB5phjjpFx48Y1P/2VV14xx5PNnDlTDjrooBalWrJkicyfP1/uu+8+6dq1q/lbJsDoEWOXXHKJ+c7LddddJwcffHDzvXrfbbfdZo460x04KtZoOVQE0uu9996TESNGyEknnSRnnHFGHSS4FQJ+CajIV0+H57d05A6B+AhQ5+LzKRblhwD1Lz++oqRxEKDOxeFHrMgvAepgfn1HyfNLgHrXON8hwDSOdchPQoCpwzvlBBgVR6655hpz5NfnPvc5I6p87GMfaxZO9Nssu+yyi8yaNUvatWtnfn/kkUfkxhtvLPkNmFLHk23bts0IOXr8mR4/pjtxxowZIyeffHKzRVdddZW8/PLLRnTRb8tkx5F17tzZpFmzZo0RXvgGTB1BwK0NIcDgoCGYeQgEmglQ5wgGCLQdAepf27HnyWkSoM6l6XesDocAdTAcX1CSdAhQ7xrnawSYxrEO+UkIMHV4p5QAo+LL5MmT5Te/+Y0RRE4//fSdnnDttdfKE088YXa07LHHHmY3i97zzDPPmN0q2S6X7MZMKDnttNOad6qsWrVKxo8fbwSXsWPHGjGmV69e5qgyFXV0d4t+a6ZHjx4yb948efzxx40oVLjDZunSpTJ37tydds7UgYRbIeCFAIMDL1jJFAJlCVDnCA4ItB0B6l/bsefJaRKgzqXpd6wOhwB1MBxfUJJ0CFDvGudrBJjGsQ75SQgwdXinlACzfPlyueKKK0yuerzXli1b5MMPP5StW7eao8WOPPJIyb730rt3b5k0aZIRYxYvXmzEFD2W7P333zffh9l7773N91300mPGnn/+eTnnnHOkb9++5nfNW7/v0r17d9HvvehxY0OHDpVjjz3W/Pezzz7bLK5k36vRckydOlU2btwo119/vfTp08cINNmxZHXg4FYIeCPA4MAbWjKGQEkC1DkCAwJtR4D613bseXKaBKhzafodq8MhQB0MxxeUJB0C1LvG+TpEAebdd981L/DrerBegwYNku9+97tmvfntt9+Wm266SRYtWmT+9qUvfcn87Qtf+ELjoEX4JASYOpyafc9l2LBhzTtTdFfLD3/4w5K5nnDCCUZY0evRRx8V3QmTXYMHD5ampibp0KGD2b0ycuRIsxNmwYIF5rd33nlHLr30UnnxxRfNLfqbfvOlX79+5t+6i0Z3szz44IPNeU6YMEG0bNmlu2YmTpxovkOjlwpAKsLoLhkuCIRMgMFByN6hbDESoM7F6FVsygsB6l9ePEU5YyFAnYvFk9iRVwLUwbx6jnLnmQD1rnHeC02A0U9a6JqzvrSvL/t/9rOfle985zvmJf9f//rX5qX9n//85+ZzFfo5DT2x6W9/+5v86le/kp49ezYOXGRPQoBpQ4dqcKuyuNtuu0mXLl1alGTZsmXy1FNPybRp01r8vmHDBtHKortesu/HFCZQoUYFlm7duknHjh13sk6FmrfeesvseNE0XBDIAwEGB3nwEmWMiQB1LiZvYkveCFD/8uYxypt3AtS5vHuQ8uedAHUw7x6k/HkkQL1rnNdCE2B+//vfm5OTdJfL8ccfb0DoiUsnnXSSPPDAA2ZzgK45/+hHPzICzNq1a80nNE488UT51Kc+1ThwkT0JASZAh6qAMnz4cKM6DhgwIMASUiQINJYAg4PG8uZpEKDOEQMQaDsC1L+2Y8+T0yRAnUvT71gdDgHqYDi+oCTpEKDeNc7XoQkwKrLoevOKFStKvpivpyvNmjXLANprr73km9/8pnz961+XT37yk42DFuGTEGAidComQSA2AgwOYvMo9oROgDoXuocoX8wEqH8xexfbQiRAnQvRK5QpJQLUwZS8ja2hEKDeNc4ToQkw+i1xPV5s5cqVssceezSD0O/CdOrUyXzy4pVXXpFHHnlEfvKTn8gf//hHk0Z3wRxyyCGNAxfZkxBgInMo5kAgRgIMDmL0KjaFTIA6F7J3KFvsBKh/sXsY+0IjQJ0LzSOUJzUC1MHUPI69IRCg3jXOC6EJME8++aSMHj3afHP8y1/+sgGxevVq+cpXvmKEGf277nj52te+Zv72hz/8QY455hizE+biiy9uHLjInoQAE5lDMQcCMRJgcBCjV7EpZALUuZC9Q9liJ0D9i93D2BcaAepcaB6hPKkRoA6m5nHsDYEA9a5xXghNgHnvvffkqKOOknXr1smcOXNkzz33lO9+97vy2muvyS9+8Qs5/fTT5c9//rN8//vfl7//+7+Xn/70pzJ79myZN2+efPWrX20cuMiehAATmUMxBwIxEmBwEKNXsSlkAtS5kL1D2WInQP2L3cPYFxoB6lxoHqE8qRGgDqbmcewNgQD1rnFeCE2AUcv/8pe/yEUXXSS//e1vDQg9emzhwoXmO+S6G2b8+PHy0ksvNUO65JJL5JxzzpH27ds3DlxkT0KAicyhmAOBGAkwOIjRq9gUMgHqXMjeoWyxE6D+xe5h7AuNAHUuNI9QntQIUAdT8zj2hkCAetc4L4QowGTWb9iwQbZv3y5du3aVdu3atYCS/a1Lly7SsWPHxgGL9EkIMJE6FrMgEBMBBgcxeRNb8kCAOpcHL1HGWAlQ/2L1LHaFSoA6F6pnKFcqBKiDqXgaO0MiQL1rnDdCFmAaR4EnIcAQAxCAQPAEGBwE7yIKGBkB6lxkDsWcXBGg/uXKXRQ2AgLUuQiciAm5JkAdzLX7KHxOCVDvGuc4BJjGsQ75SQgwIXuHskEAAoYAgwMCAQKNJUCdayxvngaBQgLUP+IBAo0lQJ1rLG+eBoFiAtRBYgICjSdAvWs8c56YNgEEmLT9j/UQyAUBBge5cBOFjIgAdS4iZ2JK7ghQ/3LnMgqccwLUuZw7kOLnngB1MPcuxIAcEqDe5dBpFDnXBBBgcu0+Cg+BNAgwOEjDz1gZDgHqXDi+oCTpEaD+pedzLG5bAtS5tuXP0yFAHSQGINB4AtS7xjPniWkTQIBJ2/9YD4FcEGBwkAs3UciICFDnInImpuSOAPUvdy6jwDknQJ3LuQMpfu4JUAdz70IMyCEB6l0OnUaRc00AASbX7qPwEEiDAIODNPyMleEQoM6F4wtKkh4B6l96PsfitiVAnWtb/jwdAtRBYgACjSdAvWs8c56YNgEEmLT9j/UQyAUBBge5cBOFjIgAdS4iZ2JK7ghQ/3LnMgqccwLUuZw7kOLnngB1MPcuxIAcEqDe5dBpFDnXBBBgcu0+Cg+BNAgwOIjbz4eMf7qFgU/N6y/Fv2mCUr/rb9lVeE+5tKXSVJM2e0Zx2uz5NuWtNm0lm4vtKMWg3P2tsazGH7Y+cm1zOX/UYrMvltXaXMiyMJ58MfaVb7Xx6st3bZFvoc3FfqzF/4UNYT3tUr31wBXLtoyJ1vzR1vWg1r6jVB+ksVbu99Ziqq18VI0d9fb5LutBcR/rK34q2VxN31yuTWorPxePXl21a9XEe2g2V1PmRsVV9hxX/nDt59B856oddcW9Fj56T2vzpFrzLfZ/8b+zPqq1dD7+3pbP9mGP6zxZY3FNlPwgUJkAAgwRAgEIBE8g1cFBqUWVehbGfE7+CoWQ4kG+y0UQ2wUIV5OmerjXWoZaJkJ555OizQgwH9WQRi0+uarPqcdrrW1bI/wcugBTa79YTT+e57hszW8ubKs1j0aUDQGmcn9g03bU6mdf7Vo1ddfGvpjThuY7XzFRa7618Cmem7mKn0IbCuethfPCSiJI4d98iCXZvKh4nlpp8cNHOUJdbEl1jSVUf1Cu+AkgwMTvYyyEQO4JpDg4KBwwunqT2ufkDwGm5S6eWiZHrvxc64TO1WSsHhHIBbcQ7Ki1DFk9ci2+NiImUvRdijbbxHZb8mltsdzGDpu0Njbb5Fvuzela8igeFIb0FnxrfrPhW6vIVY5PI8rmUoAJ2c8p9kk+x+Ct8cxbO+GinufNZpvy1sLHlwBTSkAptyszi9Pieyq1ucV/sxVH6hFgbJ+VxwWXFNdY8ugnyhwPAQSYeHyJJRCIlkCjBgfFoofNYNhlWl+TfJ+TPwQYBJhaJoSITi2P1kOAsTt+sK3jx0XMu+w7WluEa3R5G/28cn1nIwUGG5tr8X01/bhNGUJL62v840KMaETZEGA+8lS99Ta02HYRg6G18bW0YdVwCNl3vmy2ybcWPj4FmEbHZam2uHgeWij2ZLyqXSgpFpDK5V0uv1ICU2EexcJOWwo9jVpjqZY96SAQOwEEGA8efuedd+Taa6+VlStXmtz79esnTU1N0qtXr+anvfDCC3LFFVfI+vXrpV27dnL22WfLiBEjzH+XurZt2ya33nqrPPTQQ+bP++67r0yZMkX22muv5uQPP/ywzJ49Wz744APp0qWLTJ48Wfr3/+j7CPqsGTNmyHPPPWfuGTRokClXp06dPFAgSwi4I1BKGCkcSLqepGX52QyGXab1NcmvZuGmVjsQYBBgapkQtvUCeq3x3tpks9Z8EWAQYLLYqrdfqzUGXca2izYhBDtsymBjs02+5cYlteRRPDoLKdZ8jX9c2NyIsvnyp01cxpI2ZDtCLpuvGLTJFz4ftVg23FJPW0ooKf7uTfH6QWHfUJy20vpDuWcV9tXlnoUA426NiJwgkDcCCDCOPbZ9+3Y588wzZc2aNfKlL31JDjjgAFmwYIHsuuuusnjxYuncubMRXUaPHm2ePG7cOFm2bJmsXr1aLrroIjn66KNLlmju3LmydOlSOfzww2WfffaRO++80wgnS5YsMXk/+eSTRnBRkefEE080z9y6davMnz9f+vTpI1quU045RdatWyejRo2S119/XZYvXy4DBw6UadOmlRV+HOMhOwjURKDRAkylBbBGTAoaMcl3bQcCDAKMi5hykUeeJ6AIMAgwCDAt29K81WebNqwW26p5kcKmDKGlDXn804iy1RIT1YhLofm5sMwp2pyiP2z8DB8EmFqOM65GgCmVb6FQUmoMVqrtb+1Z1cRwcb5ZOQrzrmZ3TLk01dzLDpialqa4CQI1E0CAqRld6RtfeuklOf/88+WEE04w/6+X7naZOHGinHbaaXLGGWdIJqbcdNNNRqBRoUTFkd13390IJx06dGiRuQo2Y8aMkf32209mzZplxBIVbXS3y/Tp0+Wwww4zos/atWvlgQceMPmsWrVKxo8fL8cff7xccMEFZjeO7pgZO3aseZYKMldeeaWsWLFCFi1aJD179nRMguwg4IZAqbdRChcpqxngNGKiZzOxaC1tIyb5LriVGjhmrGsZOFdaeHRd3tZiojUftXa/i/K6yCMEO2zKkKLNlSZgedohlKLvUrSZ+uxuYcyGZeFbteXe0m2tX6rlecUjuVJ5uMw35PFPI8rmkmWjfdda/DW6vWz082x8F3LZbOzwlRY+7vqZ1FgWCyOt9ZfFawql5qLlBJjiF0Rt29xy4/9yZSi3slJ4ZFol8aaUIIMA42a9ilwgUC0BBJhqSVWZTnek6FFhc+bMMYKJXu+9956MHDnSHPmlu1wuvPBCeeONN+Tee+9tFlt0R4v+7+6775bu3bu3eFom6uhOFd0Bo9emTZtMnkOHDjXHl+l/H3HEESZ/vVRg0eds2LBBFi5caHbf3H777XLHHXc0H4WWCUNXX321DBgwoEoLSQaBxhJIUYBpxCTW9YC80oC33LNcl6E1br4mir7yTZFPijYjwOR3oSH1eG2tzU2Rj43NtfQdxffUkoftIlFrfnZZhkaIHDY+iiVtLHbYxFrINodcNhvGvtLCJ7/jIl8xUW2+xQKEy/7LdVy21t+V6+9L2ZilLSXAlDuGTdMiwDR2XYunQQABxnEM6DFhusNFRZi+ffua3HWHy/Dhw6Vr165GBNGdMCp4XHzxxc1Pz8SQm2++Wfbff/8WpdLdKyq+FIonmcCix5Bddtllcuqpp5odLplAoxmooHPPPfcY8UV31jz22GPm3x07djT5ZyLOWWedZcrHBYEQCSDA+DmSx8cgsjB+2AHT8uPqGZtqJxCV0tvkkbe0ruPS5cSrESwL38RrxPNc8knRdynabBOXKfKxsdmGZeHiSmtv9NqUIbS0rS1IuSivizxq8V3xGLtUHr7yxeaP6PtibJNviv6AT+UYhI87PoVj6ZjGueXGAaXak2rSIsCEuPJFmWIngADj2MN/+tOfzI6UAw88UKZOnSp77LGH2emiwstnPvMZUYFFv/8yePDg5iPKtAjZLpdSu1H0uDHdUVO4O0YFmKamJnnzzTfl8ssvlwkTJkjxvffff7957l133SW33HKLvPrqqy2OONu8ebPZOTNkyJAWZXGMpO7sCrd31ptZ4RbN4gXO4klfa8+q5miYcgOArKPMnlHvFtbWBhcxDvRbm6QzkG3sZLPwjRt9MgIMAkxxG1vN4lOMbVVr7XOpvqdUX1WJp0175yttir5L0Wab+EmRj43NNixt0tqUIbS0rY3tXJTXRR42/gghLTY3dkzcWr+foj9s6gF8wopXG9/FnLat47IaUaWauVahj+aNE+nfv39ry17O/l7Nd2mcPYyMIBAgAQQYD0754Q9/KPfdd99OOX/yk580u2N0t8oxxxwj48aNa07zyiuvyLnnniszZ86Ugw46qMW9eqzZ/PnzTZ66i0avTIDRI8YuueQS852X6667Tg4++ODme/W+2267zQhA+r0XFWv0OLL27dubNHo02ogRI+Skk04y36ap9dKtiy6v8fN35KYdQvbfWf76m16FaUo9u7U8ivMt97zC37NnZ8+rJo/i8haWtTC/SuWtxKKaMhSyK8Wz2jxK+cNF2WrNt/C+crFSrW1taUdxTDTKR65tbq1+1OrntuITQnld+ygPLFO0ubgty/q4EGLQpgwp+i5Fm4mJj1pSGxb1jlNsnpXnuGxtbOfCNhd52PgjhLTY7K7eumDpIo8Q4spXGeATVrz68nPe8o0xLovn76XW1Qp/03WKau8ptV5nc3+5srjIozU7y/29kWJVrWXkvrAJIMB48s+qVatEjxXbfffd5ZBDDjHfZvn85z9vjh3Tb7PssssuMmvWLGnXrp0pwSOPPCI33nhjyW/AlDqebNu2bTJq1Cg56qijjKCju2rGjBkjJ598crNFV111lbz88stGdNHdM9lxZJ07dzZp1qxZY4SX0L4BY7sbpPjNe7WtUh42b2ZoXoVvGxSGi4sdMMVvOduUzSZtoR2FNtjkEVra1t6StClvjHwa7Wd2wOwsRJeKQeLyo8i0YZFK2sJdk3mzmXaU2K5mXFQuTmKJHxs7fNVxmzKElra1sZ2L8rrIw5fvfOWLzWG1zyn6wya24RNWvNr4Lua0Mcal7Q6YanawlDoFo3h9rpYl2FLfsqklH+6BQFsSQIBxTF9FjW9961ui4scXv/hFk7uKIOedd5759osKHtdee6088cQTZkeLHlGmu1kmT54szzzzjNmtku1yyYqWCSXZ/fq7Cjzjx483gsvYsWONGNOrVy9zVJmKOrq75ZRTTpEePXrIvHnz5PHHH5drrrmmxQ6b7Hs1xTtnHCOxzs5WgMk6w+xBrZ2NbTMwKOxoW1tgLtUpFy6mFYOod3G2VjsKy2GTR8xpYxxQNdrPrdWPmOPHl23EZXoTUAQYP9+8oo5WrkvwccfHpt2G+87cEWD8tIE2cRlL2pDtCLlsvtolm3zhk974t0+zZaMAACAASURBVLV5q038+EobY1yWE2DKCS3Fv1dz5Hi5Na9Ki4StvaRcuO5QjShkvSDJDRDwRAABxjFYFT70uyp6zNeUKVPk3XffNcKHXosXLxbdfZJ976V3794yadIkI8bo31RM0WPJ3n//ffNNlr333tt830UvPWbs+eefl3POOUf69u1rft+yZYv5vkv37t1Fv/eix40NHTpUjj32WPPfzz77bPOxZJs2bTK7ZLZu3Wq+TbNx40a5/vrrpU+fPkagyY4lc4yjquwKG83WxJNGd3yFz6tlYQwBpmUINFJ0shl8NTqubMrmK60Pm4sHQ60Npn2UIXtmqLFm488U+aRoc/GCozLgGzD0HbRhO2LAps0MIa1NG+arvDZlCC0tAgwCjKtxXGixXTwpjqGNpw37yKuN9GfIse0rJkLIN0bupQSYSt9fLrduZ9tuF64ZZPdWEl0qiTgIMFUtuZIoEAIIMB4cocKHCiT6kXu99txzTyOEqOCSXY8++qjZCZNdgwcPlqamJunQoYPZvaIiju6EWbBggfntnXfekUsvvVRefPFFc4v+pnn269fP/Ft30ej3ZR588MHmPCdMmCDDhg1r/rfumpk4cWJzubQ8KsLoLpm2vMotNIXY0WZlqrZsCDAtI6uRg9NqfWQ7YIhl8OXDDgQYt/Huw0ehx3uKNiPAVF7ACDkmQi6bTR/oK22KfGxshvvOdR8BBgHG1TjFpi42Om2jn+errfGVL3zyOy7yFRMh5BtjXFYSYArb4uy/az2pptyxZJpvuRewWxsPFK6zlUpbSuRpyzVPng0BJYAA4ykOVBB56623jFBSfKRY9kjdwfL222/LbrvtJl26dGlRkmXLlslTTz0l06ZNa/H7hg0bRL//ortesu/HFCZQoUaFn27duknHjh13si4rl+540TQhXAgw/gdZMQ4YCmO3XmEnRT4+bEaAadmiEpc7eNhMmnzEpavFHBs7ak1bbjLhsr2rtWytlSFF36Vos038pMjHxmYbljZpbcoQWtrWFlxclNdFHjb+CCEtNvufa9n4OUV/wKdyDMLHHR8bljZpY6y3xQJMuW/5Zd5ppADT2ryjNd9VEmDYNRPCKnCaZUCACdDvKqAMHz7cHBU2YMCAAEvotkgIMP4nBTEOGOrtlIujuN7F8tYGAa2Vt9E+8vE8BJiWUVVvTPnwUVbCesvmK95TtLl4wVEZcASZ27pEvLbNwkaK9dnGZuJy57hEgGEHjKtxik1dbHTaRj/PV1vjK1/4+F8b8OW7mPONMS7bSoDJWGZznkrtfq3cWxNgCvOtlNbtyie5pU4AASb1CHBsf7Uf7MoeW0lFD6EDr7XBT2Gx3dUECT/HszCGAON20TiW9semjqdoMwJMfhcaUo/X0MY6Nm2Nr7Q2MRFCGWzK24i0CDAIMK7mF42I11rbwJDL5qtdsskXPvkdF9n4OW9pY4zLagWYUkeIldstY5PWpwBTKPIUL3kWr0EiwDheFCa7sgQQYAgOpwRKfbSrtQ95hdyZhVw2m0FLLHbY2GyTNkU+PmxGgEGAqXdnjY+4dLWYY9Om1Jo2q0PsgHFbl2r1R62La76el7d8U6zPNjb78qdNGVJMi81pLO6G7OeQy+arXbLJFz5p1FGbmAghbaxxmbFtC8aNFmCK51eFtjtdFCUzCJQhgABDaDgjUPwBrVINavZb4SJTyJ1ZyGWz6SRjscPGZpu0KfLxYTMCjNtFYx8+Cl2MSNHm4je+C/tJF2+X2byJVk/aFH2Xos30re4WxmxY2qQlLt35KBaWsdgRSz1I0R+x+M7GDl9piZ/KbTzc7fi0pQBTz7zD1s+l5lcIMM6WgsmoSgIIMFWCIlnrBKppQIsbOY4ge7oFWNuOpNrFOQZqTMarjZV6YhABBgGmVPzYxFTqbVXhywk23EJIm6LvUrTZJtZS5GNjsw1Lm7Q2ZUgxLTanMSYO2c8hl82mrfGVFj5p1FFf8eMr3xTjMiWbC9cxilc+y31mofUVUlJAoCUBBBgiwhkBGwGm+KH1LtrR0TJQqyb+yg0iUhpcFNY91/UOAQYBpt6YSrEuFjJDgPHz/QPGCJXHCPBxx8emDYO7O+42LG18FEvaWOyIxc8p+iMW39nY4Sst8UPfUc+6h01cphprWYSVOuHH2eIpGSVJAAEmSbf7MbqajsCmwQ8hbSydTix2+IqJFPn4sBkBBgEGAWZHDNTaViHAIMBkrUi9danWGHQp0vvoZ0LnY2OzLx/ZlCHFtNhcefEyFj4h2xFy2Xy1Szb5wieNOmoTEyGkTTEuU7S5Uqz5WUUl15QIIMCk5G3PtiLAlF50Y8BQeRAJn7bh42NAhQCDAFPvorGPuAx9wZYdMPldaEg9XlsTa1LkY2Ozr/GPTRlSTIvN+W1zbepMyH4OuWw2jH2lhU8addRX/PjKN8W4TNFmBBjPi8aJZ48Ak3gAuDQfAQYBptJCZ8gdeMhly9MgEgEGAQYBZkcM1Fpv2QHDDpg8CYYIMPXFa63tBNx35m7DkjFfGou7Ifs55LLZ1CVfaeGTRh31FT++8k0xLlO0GQHG5QoxeRUTQIAhJpwRQIBBgEGAeXqn+lTvgnSeBpEIMAgw9cZ76gN9BJj6FrQbHT+Nfp6v/sBXvinysbEZ7pUXGeHjjo9NXMaSNmQ7Qi6br3pnky98EGCqWVdqdJw0+nk2dcZX2hRtRoBxtjxMRiUIIMAQFs4IVNNR+uocfOUbS6cTix34OezJOAIMAgwCzI4YqLWtQoBBgKn0IkOtcVXYMtVbR23KkOLYw8ZmG5Y2aW3KkGJabE5jcTdkP4dcNpu2xlda+KRRR33Fj698U4zLFG1GgHG2PExGCDDEgE8CCDDsgKm0cBRyBx5y2fI0iESAQYCpd3E3xbpYyAwBBgEGAablTlJffaCvfG3asBDKYFPeWNLGYodN/GBzWAvaKfqDeK0cg/Bxx8eGpU3aFOttijYjwPhcMSZvdsAQA84IIMAgwCDAcARZFgPVtAcM6sJaELCZhPhKm3pMIMAgwCDAIMAgZO+oBfQz7hYkU+xbQ7Y55LL5qnc2+cKH+UGI88gU4zJFmxFgnC0Pk1EJAggwhIUzAtV0lDaDrxDSxtLpxGKHr5hIkY8Pm9kB07I5ZRHNfhHNR1zmaUEbAQYBJk/xWtji+eqf85avTRvmyzabMqSYFpvTWNwN2c8hl81Xu2STL3zSqKM2MRFC2hTjMkWbEWCcLQ+TEQIMMeCTAAIMO2AqLRyF3IGHXDZfA04fNiPAIMAgOtmLToXMEGAQYBBg2AFDO1pfO4owmK92NE/jXFfts48xuKuy+fKHTb7wQYCpZl2p0XHS6OfZ1BlfaVO0GQHG54oxebMDhhhwRqCajtJX5+Ar31g6nVjswM+VB+Q2fHzEBAIMAgwLh/UtHCLA5Gvh0Ec7yiIaAgztaH3tKAJMvtpRm7GrTdqQ2+eQy2bD2Fda+CDAVLOu1Og4afTzfNUvm3xTtBkBxtnyMBmVIIAAQ1g4I1BNR2nT4IeQNpZOJxY7fMVEinx82IwAgwDDwmF9C4cIMPlaOPTRjiLAIMDQjtbXjiLA5KsdTXFsT9+BwFDNukm5OCF+KsdPim0KNjcmJpwtnJJRsgQQYJJ1vXvDqxlI+OocfOUbywAnFjvws7vBhY+YQIBBgGHhsL6FQwSYfC0c+mhHEWAQYGhH62tHEWDy1Y6mOLan70CAqWbdBAEmrDhJsd6maDM7YNyvE5PjRwQQYIgGZwSqGUj4GmT7yjeWTicWO/AzAkw17Uws8R6LHTb1NkWbC/kgwORr4TD1eGWhu754tWkbbdISl2Et2tn4zlfaFGMiZJtDLpuvGLTJFz60YSHO91KMyxRtRoBxtjxMRiUIIMB4CIvt27fL0qVLZf78+fLBBx/Ixz72MbnwwgvlyCOPbH7aCy+8IFdccYWsX79e2rVrJ2effbaMGDHC/Hepa9u2bXLrrbfKQw89ZP687777ypQpU2SvvfZqTv7www/L7NmzzTO7dOkikydPlv79+zf/XZ81Y8YMee6558xvgwYNkqamJunUqZMTCtV0lDaDrxDSxtLpxGKHr5hIkY8Pm9kB07Ip5S3mHTxs6q2PuMy8Uq8/bOyoNS0CTH0L2o2On0Y/r9a4KmyZGlkPUuRjY7Mvf9qUIcW02PxRi+ArBkPIN2Q/h1w2fFe5fsAnfD6+fJRivU3RZgQYJ0vDZFKGAAKMh9D48Y9/LIsWLZLevXvL0UcfLQsWLJAtW7YYAaVv375GdBk9erR58rhx42TZsmWyevVqueiii0z6UtfcuXONqHP44YfLPvvsI3feeacRTpYsWSK77rqrPPnkk0Zw6dWrl5x44onmmVu3bjUiUJ8+fURFoVNOOUXWrVsno0aNktdff12WL18uAwcOlGnTppUVfmzwIMCUXmj0NQiwyZfOM43JZlvHBAIMAky9i7upt1UIMAgweRIMWxN2UqzPNjbb9Nk2aW3KkGJabE5jTByyn0Mum01b4ystfNKoo77ix1e+KcZlijYjwNisAJPWlgACjC2xVtJv2rTJiCvdunWT22+/XTp27CirVq2S8ePHy/Dhw83/Z2LKTTfdJAcccIARSlQc2X333Y1w0qFDhxZPUcFmzJgxst9++8msWbOMWKKije52mT59uhx22GFy5plnytq1a+WBBx4w+WTPPP744+WCCy6QlStXmh0zY8eONc9SQebKK6+UFStWGLGoZ8+edZNAgEGAqbRwFHIHHnLZ8jSIRIBBgEGA2REDtdZbBBgEGAQYvgFDO1pfO4owmK92tNb+Ms9+TnHeYeNn+CDAVLOu1Og4afTzbOqMr7Qp2owAU/eyMBlUIIAA4zg89GixiRMnyo033igHHnig6NFhKqi89NJL8olPfEJ69OhhjiN744035N57720WW3RHi/7v7rvvlu7du7cold57/vnnm50qugNGLxV6Ro4cKUOHDjXHl+l/H3HEEWYXjV4qsOhzNmzYIAsXLpTFixcbQeiOO+4wu2T0ysp69dVXy4ABA+omUU1H6atz8JVvLJ1OLHbg58oDchs+PmICAQYBhoXD+hYOC/nZ1OcQ0vpoU0IXI1K02SbWUuRjY7MNS5u0NmVIMS02p7G4G7KfQy6bTVvjKy180qijvuLHV74pxmWKNiPA1L0sTAYVCCDAOA6P+++/3wgdZ511lhFT3n33XfMNmMsuu0wOPfTQ5qPAVPC4+OKLm5+eiSE333yz7L///i1KpbtXVHwpFE8ygUWPIdO8Tz31VLPDJRNoNAMVdO655x4jvujOmscee8z8W3fl6JWJOFpW3Z1T74UAww6YSotlIXfgIZctT4NIBBgEGAQYBJhqxgKxtLmx2JGnfiYmUQ7ulRcZ4eOOT4ptVcg2h1w2X/XOJl/4IMCEOJZMMS5TtBkBpt5VYe6vRAABxnF8qABz2223mVyPOeYYc2zYD37wAyPE6E6Tfv36mW+wDB482Oxqya5sl0up3Sh63NicOXNa7I5RAaapqUnefPNNufzyy2XChAkm/8KdLJkYdNddd8ktt9wir776aosjzjZv3mx2zgwZMqRFWWpFUk1HaTP4CiFtLJ1OLHb4iokU+fiwGQEGAQYBBgGmmrGAj/anLRbmY7GDvrVtFrrh7o67DcsU6y02h7WgnaI/qKPu2jvixx1L4jKsttHGH22RttZ1Uu6DQEYAAcZxLGQCTPbNF81ejwHT78KoGHPVVVeZb7CoODNu3Ljmp7/yyity7rnnysyZM+Wggw5qUaolS5bI/Pnz5b777pOuXbuav2UCjOZ9ySWXmO+8XHfddXLwwQc336v3qRikR53p915UrNHjyNq3b2/SvPfeezJixAg56aST5IwzzqiZxNNP7zive/z8llnMG1fdb3pXqGlDLlspZuXKG4sdNjbbpE2Rjw+blXl2VdMe+ChD9vxQ2xTi8qMYsWGRStpCO/Nmc4r1OUWbbeIyRT42NtuwtElrU4YU02JzGv1wyH4OuWw2bY2vtPBJo476ih9f+aYYlynaXCl++vfvX/OaKTdCQAkgwDiOAxU9br31VnMs2JFHHmlyV7FERZLVq1ebI8D0Oy277LKLzJo1S9q1a2fSPPLII+a7MaW+AVPqeDL9tozupDnqqKPM8WMq8IwZM0ZOPvnkZotU7Hn55ZeN6KL5ZseRde7c2aRZs2aNEV74BswOZG2hojfqTeFy9sVss41tKfLxYTM7YFp2KOwGsW9bfcRl5pV6/WHTptSatvC+WvMojMJG2pyi71K02SYuU+RjY7MNS5u0NmVIMS02f9RL2MRV3tKG7OeQyxaCn+GTRh0NIdZsypBiXKZoM0eQtVzP4F9uCSDAuOUpq1atEt39UrgDJjvqSxXTqVOnyrXXXitPPPGE2dGyxx57GIFm8uTJ8swzz5jdKtkul6xomVBy2mmnNe9UyZ6jgsvYsWONGNOrVy9zVJmKOrq7RXfa9OjRQ+bNmyePP/64XHPNNS122CxdulTmzp27086ZWpFUIybYdHIhpI2l04nFDl8xkSIfHzYjwCDA1Lvg7yMuEWB27FKtxMEFdxd5+GrjfeWbos02LFPkY2OzDUubtDZlSDEtNvvvD2zi1VfakP0cctl8+cMmX/ikUUdtYiKEtCnGZYo2I8DUuhrMfdUQQICphpJFGhVTzjvvPNEjxb75zW/KAQccYL4B8+KLL8r06dNl4MCBkn3vpXfv3jJp0iQjxixevNjsXtFjyd5//33zTZa9997bfN9FL91B8/zzz8s555wjffv2Nb9v2bJF9Psu3bt3l+zos6FDh8qxxx5rjh579tlnm8WVTZs2mV0yW7duNSLQxo0b5frrr5c+ffoYgSY7lszC1J2SIsCU3kXDgKHyIBI+bcPHx4AKAaZls4gYsYOHTR33EZeZV+r1h40dtaYtvK/WPAqjsJE2p+i7FG22icsU+djYbMPSJq1NGVJMi81pLO6G7OeQy2bT1vhKC5806qiv+PGVb4pxmaLNCDD1rAhzb2sEEGBaI1TD31VA0eO/Vq5cae7WHSn6jZbjjjuuObdHH33U7ITJrsGDB0tTU5N06NDB7F4ZOXKk2QmzYMEC89s777wjl156qRFy9NLf9Jsv/fr1M/9W4Ud3szz44IPNeU6YMEGGDRvW/G/dNTNx4kTRHTl6qQCkIozuknFxIcAgwGRx5Gvg4yvfFAcXPmxGgGnZkta7+O3DR5XqqK/6ZZNvijaXE11suIWQNkXfpWizTaylyMfGZhuWNmltypBiWmz+aKxiE1d5Sxuyn0MuWwh+hk8adTSEWLMpQ4pxmaLNCDAuVobJoxwBBBiPsfH222+bHScqpHTs2HGnJ+kOFk2z2267SZcuXVr8fdmyZfLUU0/JtGnTWvy+YcMG0e+/6K6X7PsxhQlUqFGBpVu3biWfqULNW2+9ZXa8aBqXFwIMAgwCTMujfkIetPgoGwIMAgyi044YsJnQIcDkd6HBRztaqR+tNa4KW6Z666hNGVLkY2OzDUubtDZlSDEtNue3zY2lHqQYg7H4zsYOX2mJn8ptGNzd8SHWWrJ0uXZKXmkSQIAJ0O8qoAwfPtwcFTZgwIAAS1i6SAgwCDAIMAgwWQxU0x4wqEtjEcRmIpR6TJQTY9pqAR3fUUfractTrM82NtvUL5u0NmVIMS02p9GuheznkMtm09b4SgufNOqor/jxlW+KcZmizeyAyc3ycy4LigCTS7eFWehqJum+OkRf+cbS6cRiB36uPCC34eMjJtgB07JtrvdNcx8+qiSS2sSPr7Qp2swOmPwuNKQer60JgynysbGZdtTdmMaGpY2PYkkbix2x+DlFf8TiOxs7fKUlfug7qll3cxEnLvLwVQ/aIt8wV2EpVZ4IIMDkyVuBl7WajqAtGspqylWuc4ml04nFDl/xkyIfHzYjwCDAIDrtiIEU2yofbQqCYctdlb7iyle+xASLRPWMwYlLd/FDXazMstF8Gv08X3XJV77wCSteffk5b/mmGJcp2swOmMAXnXNePASYnDswpOJXM8mio3U3mbJhSefJQLa4fvqKiUyEqaY98FWGWPKNxQ7aKnftfsgxEXLZbGLQJm2KNsPH3XjChqVNWuLSnY9iYRmLHbHUgxT9EYvvbOzwlZb4cTeutvFRitxTtBkBJqQV5vjKggATn0/bzKJqFlxtOrkQ0sbS6cRih6+YSJGPL5sRYNxNCnz5KOR8Qy6br/bHJt+Q+YRcNhvGNmlTtBk+7hb3bVjapCUu3fkoFpax2BFLPUjRH7H4zsYOX2mJH3dzLRsfpcg9RZsRYNpsOTmJByPAJOHmxhiJAFP62Bmbjt1XWjpPJuPsgPF3LBT11t1EiLYqv21Vir5L0Wab9i5FPjY227C0SWtThhTTYnN++5lY6kGKMRiL72zs8JWW+HE377DxUYrcU7QZAaYxa8epPgUBJlXPe7AbAQYBJgsrm8FMCGlTHFz4spkdMO4mBb58FHK+IZeNtiqNRTsbPxOvxEQ9LzfYxJpNWuKSuKwnLmOJn5DtCLlsNm2Nr7TwoQ2rZl2p0XHS6Of5ql82+aZoMwKMh4VismwmgABDMDgjUE1HadPgh5A2lk4nFjt8xUSKfHzZjACDAFNNX1Au/nzFZSz5hmxHyGWj73DXLtmwJCbgXk9/YBNrNmmJyzQWd0P2c8hls6lLvtLCJ4066it+fOWbYlymaDMCjLPlYTIqQQABhrBwRqCaSZavDtFXvrF0OrHYgZ/dLeb4igkEmPB95Mv3LvJ1kYevdiKEfEPmE3LZfPkuRZttWKbIx8ZmG5Y2aW3KkGJabE5jcTdkP4dcNpu2xlda+KRRR33Fj698U4zLFG1GgHG2PExGCDDEgE8CCDAcQZbFl6+Bj698Uxxc+LIZAQYBppq+oFz8+YrLWPIN2Y6Qy0bf4a5dsmFJTMC9nv7AJtZs0hKXaSzuhuznkMtmU5d8pYVPGnXUV/z4yjfFuEzRZgQYnyvG5M0OGGLAGYFqJlm+OkRf+cbS6cRiB352t5jjKyYQYML3kS/fu8jXRR6+2okQ8g2ZT8hl8+W7FG22YZkiHxubbVjapLUpQ4ppsTmNxd2Q/Rxy2WzaGl9p4ZNGHfUVP77yTTEuU7QZAcbZ8jAZlSCAAENYOCOAAMMOmCyYfA18fOWb4uDCl80IMAgw1fQF5eLPV1zGkm/IdoRcNvoOd+2SDUtiAu719Ac2sWaTlrhMY3E3ZD+HXDabuuQrLXzSqKO+4sdXvinGZYo2I8A4Wx4mIwQYYsAngWomWb46RF/5xtLpxGIHfna3mOMrJhBgwveRL9+7yNdFHr7aiRDyDZlPyGXz5bsUbbZhmSIfG5ttWNqktSlDimmxOY3F3ZD9HHLZbNoaX2nhk0Yd9RU/vvJNMS5TtBkBxueKMXmzA4YYcEYAAYYdMFkw+Rr4+Mo3xcGFL5sRYBBgqukLysWfr7iMJd+Q7Qi5bPQd7tolG5bEBNzr6Q9sYs0mLXGZxuJuyH4OuWw2dclXWvikUUd9xY+vfFOMyxRtRoBxtjxMRiUIIMAQFk4IFE+wYmmssSONAWAsfrYZcPqyGQHG3YKbLx+FnG/IZbOpX77Shswn5LKl6A9fNtvkS0y46w/g7o4lccnYvq3nrSnGIG0YbViIgjxxmUZ/YONnBBgny8NkUoYAAgyh4YRAWw9kXTWq2LEjHErx9MU4hHyx2d3gCwGGCVY9E6wU66JNGxgyn5DLZsPYJm2KNsPHbX/pY9xJXLrzUSwsY7EjlvYnRX/E4jsbO3ylJX7czbVsfJQi9xRtRoBxsjxMJggwxIBPAj4mkDYdoq+0sXQ6sdiBn90NOH3FBAJM+D7y5XsX+brIw1c7EUK+IfMJuWy+fJeizTYsU+RjY7MNS5u0NmVIMS02pyFQheznkMtm09b4SgufNOqor/jxlW+KcZmizQgwPleNyZsdMMSAEwIIMDswhrpzJOSy+Rok2eSbIh9fNiPAIMCwA6Z0X+CizrnIw6ZttEkbctls7LBJm6LN8HG3MGbD0iYtcenOR7GwjMWOWOpBiv6IxXc2dvhKS/y4m2vZ+ChF7inajADjZHmYTMoQQIAhNJwQQIBBgCkMJJvBTAhpUxxc+LIZAcbdpMCXj0LON+Sy0VaxqBnrWMdXbKdYn21shru7/tKGpY2PYkkbix2x+DlFf8TiOxs7fKUlfug76nnZzSYuibWWseZk4ZRMkiaAAOPB/Rs2bJAbbrhBVq5caXLv16+ffOc735E999yz+WkvvPCCXHHFFbJ+/Xpp166dnH322TJixAjz36Wubdu2ya233ioPPfSQ+fO+++4rU6ZMkb322qs5+cMPPyyzZ8+WDz74QLp06SKTJ0+W/v37N/9dnzVjxgx57rnnzG+DBg2SpqYm6dSpU90UYl2UiKXTicUOmwGDTdoU+fiyGQHG3aTAl49Czjfkstm0Kb7Shswn5LKl6A9fNtvkS0y46w/g7o4lcVmZZSx8QrYj5LLZtDW+0sInjTrqK3585ZtiXKZoMztg6l4aJoMKBBBgHIfH9u3bjZiyevVqOfroo+XjH/+43HfffUbkWLJkiey6665GdBk9erR58rhx42TZsmUm/UUXXWTuKXXNnTtXli5dKocffrjss88+cuedd7bI88knnzSCS69eveTEE0+UBQsWyNatW2X+/PnSp08f0XKdcsopsm7dOhk1apS8/vrrsnz5chk4cKBMmzatrPBTLR4EmB2kSjXYvgYBNvmGXDYbO3ylTZGPL5sRYFgkquetLF9xGUu+IdsRctnoO9y1SzYsiQm419Mf2MSaTVriMo3F3ZD9HHLZbOqSr7TwSaOO+oofX/mmGJcp2owAU+0KMOlqIYAAUwu1CvesWrVKxo8fb0SQ8847z6S8//775bbbbpObb75Z9t9/f8nElJtuukkOOOAAI5SoOLL77rsb4aRDhw4tnqCCzZgxY2S/takehwAAIABJREFU/faTWbNmGbFERRvd7TJ9+nQ57LDD5Mwzz5S1a9fKAw88YPLJynH88cfLBRdcYHbj6I6ZsWPHmmepIHPllVfKihUrZNGiRdKzZ8+6SCDAIMAUBpCvgY+vfFMcXPiyGQHG3YKbLx+FnG/IZfPV/tjkGzKfkMtmw9gmbYo2w8fdwpgNS5u0xKU7H8XCMhY7YqkHKfojFt/Z2OErLfHjbq5l46MUuadoMwJMXcvC3NwKAQQYxyGix4+pOKI7S3Snil7Z7pTrrrtOvvjFL8qFF14ob7zxhtx7773NYovuaNH/3X333dK9e/cWpXrppZfk/PPPNztVdAeMXps2bZKRI0fK0KFDzY4b/e8jjjjC7KLRSwUWfY6WZ+HChbJ48WK5/fbb5Y477jC7ZPTSY9AmTpwoV199tQwYMKAuEggwO/CxA6Y8B/iEtSDgyx8IMO4mBb58FHK+IZfNZpLmK23IfEIuW4r+8GWzTb7EhLv+AO7uWBKXYY1HbWLbJm3Ifg65bDaMfaWFTxp11Ff8+Mo3xbhM0WYEmLqWhbm5FQIIMB5D5LXXXjPfW9FjwHr06GEEkF122cXsQFHB4+KLL25+eiaGZLtkCoulu1dUfCkUTzKBRY82u+yyy+TUU081O1wygUbvV0HnnnvuMeKL7qx57LHHzL87duxoss9EnLPOOkuGDx9eFwkEmPLCg69BgE2+dJ4MZBtVRxFgWCSq58gZ2qr8tlUp+i5Fmxl7uKujNixt0hKX7nwUC8tY7IilHqToj1h8Z2OHr7TEj7u5lo2PUuSeos0IMHUtC3MzAkzbxIAKJN/61rfkj3/8oynAvvvua44M++CDD8w3WAYPHmx2tWRXtsul1G4U3VEzZ86cFrtjNP+mpiZ588035fLLL5cJEybstJNFjz5T0eeuu+6SW265RV599dUWR5xt3rzZ7JwZMmRIi7LYEnv66adl/Pyd75o3Tnb6vdRvemeoaUMumw3LWOywsdkmbYp8UrTZJiZCSJuij1K02SbWQuYTctlsGNukTdFm+Hw03rVh0chxLnHpzkexsIzFDps6F7LNIZfNhrGvtPChDSteW/IVazb5phiXKdpcKSb69+9vu1RKegi0IMAOGI8BsW3bNpP7Qw89JLfeequccMIJ5rgwFWCOOeYYGTduXPPTX3nlFTn33HNl5syZctBBB7Uo1ZIlS8wumvvuu0+6du1q/pYJMHrE2CWXXGK+86JHnB188MHN9+p9+u0ZPepMv/eiYo0eR9a+fXuT5r333pMRI0bISSedJGeccUZdJBr1dr0WspHHfDX6eTZvYdikjcUOG5tt0qbIJ0WbbWIihLQp+ihFm21iLWQ+IZfNhrFN2hRths9Hw2UbFoxdd3Crl5mLOucijxDssCkDNrurty5YusjDxv95SwufsOI1b/Hjq7wpxmWKNrMDpq5lYW5uhQACjOMQUdHlz3/+s3zmM5+Rdu3amdw//PBDc+yYHhemO1L0Oy16FNmsWbOa0zzyyCNy4403lvwGTKnjyfQ5KuQcddRR5vix0aNHy5gxY+Tkk09utuiqq66Sl19+2Ygu+m2Z7Diyzp07mzRr1qwxwgvfgAlnUsiAofKAEz7u+DCgcseSuHTHkrjM76Q7Rd+laLNNe5ciHxubbVjapLUpQ4ppsTm//Uws9SDFGIzFdzZ2+EpL/Libd9j4KEXuKdqMAON4gZzsWhBAgHEcEPq9Fv0Wi36zJfsei35rRQUSPYZMd7joTpUnnnjC7GjZY489zG6WyZMnyzPPPGN2q2S7XLKiZULJaaed1rxTZdWqVTJ+/HgjuIwdO9aIMb169TJHlanwo7tbVPTRb8/MmzdPHn/8cbnmmmta7LBZunSpzJ07d6edM7UgYQdMeRHHpmP3lZbOM43Jpk38EBNtM3jHR9TFWL+Rk2KbkqLNtGHu2jAbljZpiUt3PoqFZSx2xFIPUvRHLL6zscNXWuKnbeZwKXJP0WYEmFpWg7mnWgIIMNWSqjJd9mF7/dbLpEmTjADy/e9/X1avXt0symTfe+ndu7dJo2LM4sWLjZiix5K9//775psse++9t/m+i156zNjzzz8v55xzjvTt29f8vmXLFvN9l+7du4t+70WPGxs6dKgce+yx5r+fffbZZnElE4G2bt0qU6dOlY0bN8r1118vffr0MQJNdixZlWbulAwBBgGmMCh8DTh95Zvi4CJFm33Fj698U/RRijbbxE/IfEIumw1jm7Qp2gwfd4v7Nixt0hKX7nwUC8tY7IilHqToj1h8Z2OHr7TEDwJMPS9z2cQlsdYy1mpdK+U+CGQEEGA8xIIe+6W7YNavX29y79ChgxFajjzyyOanPfroo3Lttdc2/3vw4MHS1NRk0urulZEjR5qdMAsWLDC/vfPOO3LppZfKiy++2Jyn7qTp16+f+bfuotHdLA8++GBznhMmTJBhw4Y1/1t3zUycOFE2b95sflMBSEUYFYnqvRBgdhAs1aHZdHK+0oZcNl822+SbIp8UbbaJiRDSpuijFG22ibWQ+YRcNhvGNmlTtBk+lRd+bGLChqVNWpsypJgWm93FcMgsKVt+/Ryy72zaYl9p4VM5tuHujg+x1pJlvWum3A8BBBiPMaACjAojukMl+x5M4eN0B8vbb78tu+22m3Tp0qVFSZYtWyZPPfWU2TVTeG3YsEH0+y/l8lShRgWWbt26SceOHXeyTsvz1ltvmR0vmsbVhQCzgyQCTHkO8AlrIhSyP3wNnPOWb4o+StFmm7gMmU/IZbNhbJM2RZvh464vt2Fpk5a4dOejWFjGYkcs9SBFf8TiOxs7fKUlfiq38XB3x4dYa8nS1dop+aRLAAEmQN+rgDJ8+HBzVNiAAQMCLOHORUKAQYApjApfAx9f+aY4uEjRZl/x4yvfFH2Uos028RMyn5DLZsPYJm2KNsPH3eK+DUubtMSlOx/FwjIWO2KpByn6Ixbf2djhKy3x405gsPFRitxTtLlSTORiYZZCBk0AASZo9+SncAgwCDAIMP0lT/WAAVXbDN4Z6LMwVs+5zSHX25DLZlPvbNKmaDN83LVhNixt0hKX7nwUC8tY7IilHqToj1h8Z2OHr7TET9vM4VLknqLNCDD5WYPOY0kRYPLotQDLnKeFZ5vBUCydTix22PjOJm2KfFK02SYmQkiboo9StNkm1kLmE3LZbBjbpE3RZvi4W9y3YWmTlrh056NYWMZiRyz1IEV/xOI7Gzt8pSV+EGDqeZnLJi6JtZaxFuAyLEXKGQEEmJw5LNTiIsDs8EypDs2mk/OVNuSy+bLZJt8U+aRos01MhJA2RR+laLNNrIXMJ+Sy2TC2SZuizfBxt7hvw9ImLXHpzkexsIzFjljqQYr+iMV3Nnb4Skv8IMAgwJRed/NdN0Jdi6Vc+SGAAJMfXwVdUgQYBJjCAPU14PSVr+/OulGDJBs+KdpswyeEtCn6KEWbbWItZD4hl82GsU3aFG2Gj7vFfRuWNmmJS3c+ioVlLHbEUg9S9EcsvrOxw1da4gcBplFrC8Ray1gLekGWwuWCAAJMLtwUfiERYBBgEGD4BkwWA6HuBPM1EfKVb4qD3hRttomfkPmEXDYbxjZpU7QZPu4W921Y2qQlLt35KBaWsdgRSz1I0R+x+M7GDl9piR8EGAQYdsCEv0JMCUsRQIAhLpwQQIBBgEGAQYBBgGnZnNYrRKU4wUrRZpsJesh8Qi6bDWObtCnaDB93i/s2LG3SEpfufBQLy1jsiKUepOiPWHxnY4evtMQPAgwCDAKMk0VcMmk4AQSYhiOP84EIMAgwCDAIMAgwCDCITuX7AhcTZhd5sCDgbuIesj98+dkm3xT52Nhsw9ImrU0ZUkyLzWkIVCH7OeSy2bQ1vtLCJ4066it+fOWbYlymaHOl+IlzJRerGkkAAaaRtCN+FgIMAgwCDAIMAgwCDAIMAkyj3srzNcG2yZeJKYtE9Yx/bWLNJi1xSVzWE5exxE/IdoRcNpu2xlda+NCGhTiWTDEuU7QZASbiResATEOACcAJMRQh1oF+LJ1OLHYw0K88ILfhQ0y4Y2nD3SZtij5K0eZYYiJF36VocyzxamOHTVqbmLDJ1yatTRlSTIvNaSzuhuznkMtm09b4SgufNOqor/jxlW+KcZmizQgwMaxOh2sDAky4vslVyRBgdrir3re/GTC0zaJ4yL4jJtomJnxxt8mXuGQCmqe+lXglXvMUrzZtsU1am3pgk69NWpsypJgWm9Noq0L2c8hls2lrfKWFTxp11Ff8+Mo3xbhM0WYEmFwtQ+eusAgwuXNZmAWOddIdS6cTix0MqNyJEcSEO5bEpTuWxGV+J90p+i5Fm23auxT52Nhsw9ImrU0ZUkyLzfntZ2KpBynGYCy+s7HDV1rix928w8ZHKXJP0WYEmDDXm2MpFQJMLJ5sYzsQYHY4gB0w5TnAJ6wJb8j+sBkMx5w2RR+laLNNDIfMJ+Sy2TC2SZuizfBx15fbsLRJS1y681EsLGOxI5Z6kKI/YvGdjR2+0hI/CDCN+kYOsdYy1tp4yZXHR0AAASYCJ4ZgAgIMAkxhHPoacPrKN8XBRYo2+4ofX/mm6KMUbbaJn5D5hFw2G8Y2aVO0GT7uFvdtWNqkJS7d+SgWlrHYEUs9SNEfsfjOxg5faYkfBBgEmNIvPvuuGyGsu1KGfBNAgMm3/4IpPQIMAgwCTH/JUz3wPUBp1MDQ1+QmhHxT9FGKNtvEWsh8Qi6bDWObtCnaDB93i/s2LG3SEpfufBQLy1jsiKUepOiPWHxnY4evtMQPAkyj5tnEWstYC2bxlYLklgACTG5dF1bB87TwbDMYiqXTicUOG9/ZpE2RT4o228RECGlT9FGKNtvEWsh8Qi6bDWObtCnaDB93i/s2LG3SEpfufBQLy1jsiKUepOiPWHxnY4evtMQPAgwCDDtgwloNpjTVEkCAqZYU6SoSQIDZgafUQMvX4Msm35DLZmOHr7Qp8knRZl/x4yvfFH2Uos028RMyn5DLZsPYJm2KNsPH3eK+DUubtMSlOx/FwjIWO2KpByn6Ixbf2djhKy3xgwCDAIMAw/J0PgkgwOTTb8GVGgEGAaYwKH0NOH3lm+JANkWbfcWPr3xT9FGKNtvET8h8Qi6bDWObtCnaDB93i/s2LG3SEpfufBQLy1jsiKUepOiPWHxnY4evtMQPAgwCDAJMcAvCFKgqAggwVWEiUWsEEGAQYBBg+AZMFgOh7gTzNRHylW+KE6wUbbaJn5D5hFw2G8Y2aVO0GT7uFvdtWNqkJS7d+SgWlrHYEUs9SNEfsfjOxg5faYkfBBgEGASY1tZn+XuYBBBgwvSLl1KtX79eZsyYIc8995zJf9CgQdLU1CSdOnWq+3kIMAgwCDAIMAgwLZvSeoWoFCdYKdpsM0EPmU/IZbNhbJM2RZvh425x34alTVri0p2PYmEZix2x1IMU/RGL72zs8JWW+EGAQYBBgKl7AZcM2oQAAkybYG/8Q7dv3y6nnHKKrFu3TkaNGiWvv/66LF++XAYOHCjTpk2Tdu3a1VUoBBgEGAQYBBgEGAQYRKfyfYGLCbOLPFgQcDdxD9kfvvxsk2+KfGxstmFpk9amDCmmxeY0BKqQ/Rxy2WzaGl9p4ZNGHfUVP77yTTEuU7S5UvzUtWDKzRAQEQSYRMJg5cqVMmXKFBk7dqwRYlSQufLKK2XFihWyaNEi6dmzZ10kEGAQYBBgEGAQYBBgEGAQYBr1Vp6vCbZNvkxMWSSqZ/xrE2s2aYlL4rKeuIwlfkK2I+Sy2bQ1vtLChzYsxLFkinGZos0IMHUtC3NzKwQQYBIJkfvvv19uv/12ueOOO6RXr17G6hdeeEEmTpwoV199tQwYMKAuErEO9GPpdGKxg4F+5QG5DR9iwh1LG+42aVP0UYo2xxITKfouRZtjiVcbO2zS2sSETb42aW3KkGJabE5jcTdkP4dcNpu2xlda+KRRR33Fj698U4zLFG1GgKlrWZibEWCIASUwd+5ceeyxx+See+6Rjh07GiibNm2SkSNHyllnnSXDhw+vCxQCzA589b79zYChbRbFQ/YdMdE2MeGLu02+xCUT0Dz1rcQr8ZqneLVpi23S2tQDm3xt0tqUIcW02JxGWxWyn0Mum01b4ystfNKoo77ix1e+KcZlijYjwNS1LMzNCDDEQHbc2KuvvioLFiyQDh06GCibN282AsyQIUPk/PPPrxnUIYccUvO93AgBCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAIEQCTz31VIjFokw5IsARZDlyVq1FVQGmqalJ3nzzTVm4cKG0b9/eZPXee+/JiBEj5KSTTpIzzjij1uy5DwIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAgSICCDCJhMSdd95pjh9bvHixdO7c2Vi9Zs0aI7y4+AZMIhgxEwIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCBQFQEEmKow5T/RL37xC7nmmmtk5syZctBBBxmDli5dar4Nc91118nBBx+cfyOxAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAQCAEEmEAc4bsYmzZtktGjR8vWrVtl6tSpsnHjRrn++uulT58+Mm/evOZjyXyXg/whAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAikQQIBJwcv/v42rVq2SiRMnyubNm80vvXv3NiJMjx49EqKAqRCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAAB/wQQYPwzDuoJ27dvl7feesvseOnWrVtQZaMwEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAIFYCCDAxOJJ7IAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQCAYAggwwbiCgkAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIBALAQSYWDyJHRCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCARDAAEmGFdQEAhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABGIhgAATiyexAwJtQGDZsmUye/ZsmTlzphx00EFeSvD222+b/FeuXGny79Kli1x66aVy6KGHNj/vhRdekCuuuELWr18v7dq1k7PPPltGjBhh/rvw2rZtm4waNUqGDBki48aNK1nep59+2uR/3XXXycEHH+zFJjKFQL0E3nvvPTnllFNk7733NnWwffv2FbPcvHmzjBw5UiZPniwDBgwomfbRRx81eWlavY466ij5l3/5F+nUqZP5t9avGTNmyHPPPWf+PWjQIGlqamr+e2Gm//Ef/yE33XST3H///bLHHnvs9Lzt27fLRRddJO+8847Mmzev1fLXy4v7IeCCwKpVq2T8+PFy2mmnyRlnnOEiyzapi1r/tH7+z//8T4u698lPflKmT59OffTmWTKulYDGrMbmb3/7W7nnnntK9iuFeWv6Cy+8UPbbbz85//zzSz725Zdfluuvv17++Mc/mr/vv//+8p3vfMf0q9n18MMPm37xgw8+MONP7UP79++/U36rV6+WsWPH7jR2/NOf/mT6yY9//OPN92zYsEG+973veRs318qY+yBQTMC23tVKsBF1UeeAWpd/9rOfmWJ+5jOfMfX9c5/7XK3F5j4IeCdgO+4MZb735ptvyrRp0+SVV14xjHTuOWnSJOnatat3ZjwAAiETQIAJ2TuUDQKBE9DF1dtuu02uvvrqsou69Zjw4YcfmkVmXfgdPHiw9OnTR3784x/Lli1b5MYbb5QDDzzQ/G306NHmMSqqqCikE2Fd3D366KObH68D729/+9uiYs2JJ54o55133k5F27Rpk8lLBy++bKqHB/dCICOQxWq3bt1kwYIF0qFDh4pwNKaHDx8uU6dOLVlXly9fbkRMXWAaM2aMqBD5m9/8xixI6YRVxUyti+vWrTMi5uuvvy56z8CBA80Au1Ds/K//+i+5/PLLTZnuvvtu6d69+05lu+++++SHP/yh6IJvNeXH8xAIgcBLL71kFnPL9SEuytiIurh161ZTn3Uh+GMf+1hzsT/1qU/JzTffjADjwpHk4ZSALgRfcskl8rvf/a5sv1L4wNYEGF0cOv3000XHmdrnbdy4UZYuXSq77rprs8Dz5JNPGsGlV69eps5rX6V1Z/78+WY8ml2a1ze/+c2SY0d9GeGGG24wfeTuu+9ubtEXD3jJx2l4kJknArb1rpZiNKIuqh0679PF4COOOEJ69uwpOg7V+r548WLp3LlzLUXnHgh4J2A77gxhvqdjS+1XtSyFc8Z+/fqZl2qLX5D1DpEHQCAgAggwATmDokAgbwR8CzAqlkycONF04jpR1it7y/CEE04wC2Fz5841k2Z9m/eAAw4wk2NdWNKJbraw+9prrxnxRRet9Sq1eKaDc51o66KzXggweYvGtMqbDWr33HNPsxhUzQ6YcgJMtlClb1kVvlmsC0SPPfaYWezStxOnTJli3vDV+qX3XHnllbJixQpZtGiRmcyqyHnnnXfKXXfdZZxRToDRvDIBVN9ArKb8aXkXa0MlYDsRtrWjUXUxe3FBd4uefPLJtsUkPQQaTkDrhu4k0V1b5YT9wkK1JsBoP7Vw4cIWO7gzsUTHf1/60pfkzDPPlLVr18oDDzxgxpTZm8jHH3+8XHDBBeZxTzzxhFxzzTVmh0ypseOtt95qXgxikbfhIcMDHRCwrXe1PLIRdTHru4855hgzr9RL5446h0QMrcVr3NMoArbjzkoCTKPGmNn6kK6r6GkJ+lzd/fL8888b4ZNdMI2KHp4TIgEEmBC9QpkgkBMCxQLM+++/b44T+ulPf2omo/qGgw52zznnHHNMkW771rd79a16XbTVS9+G0CO/evTosZPVKrZoh61v7es9eqmI8o1vfMOIKOeee645YuKNN96Qe++9t3kXgC4C6/90kv53f/d3cuqpp5pJtL6FoWU+7rjjdtoBk0289TkvvvgiAkxOYjDVYhYLMP/7v/8rl112mdmRlh35pYtF2W+77LJL2R0wOjDWRSLd3aKD5ezNJN35osevaD36+c9/Lrfffrvccccd5m1gvTKBNBMr//u//9u8oaw70/RtQt1FU7xQlgmkWh79b13U0kWw1gSkVP2M3WERKJ4Ia9156KGHzM5MfeNPLz2OU+vBXnvtJdon6kLt1772NdP/aT3VmNeFXe3Hiq9G1cWs7uqbiPvuu6/ZVaq76bggECqB4oVg3bmlu55VlNGXb/TS+pb99vnPf77iEWS/+MUvTH82Z86c5uPBsiNotU/TfkyP7dS35XVHtV7Z4pXWde233n33XVOPdefokUceadqCwpd3svR676xZs8z4VcudHesZKmvKBYGMQCkBZs2aNXLLLbc0v7CmfZqeHqBHP+ulx/rpkXu6q0zHjtlcUF+86dix405wG1EXs5eDVAjVOqgvDOk8VV8I6tu3LztgCPlgCRSPO3Us+etf/7rFcbGFv2k/WOmFO9/zvazf06Oy9QU73WWqc7y33npL/vKXv8g//MM/MOcLNtooWCMIIMA0gjLPgECkBAoFGH1bUMUQFS90QqoTYhU1dEeJvmGrk+Isvb4Zr2fo//WvfzVijYoe//qv/1rVltRf/vKX5s17HezrIpa+ja/nil588cXNlLPFJT1KRfN+9tlnZZ999jGT5GHDhplvWxSeCa6TCX2z/7DDDjPn+n/rW99CgIk0ZmMxq1iA0WMV9HsthYKHDtqz31QQqXQEWTEXHTBPmDBB9Px6fVtJJ6+6G0Z3yGQTaF1M0gWqs846y+Sti1J6lv4XvvAF82ahirHFAozuVFNRRwflP/rRj0x6BJhYojJ+O4onwlovdBFX+z8VWbTv0djX44k0/nWBNjvWUhd09U1ArRO6AyU7RrM1aj7qYvbmb+GzdTedvgncu3fv1orE3yHQcALFC8Gl+rTCN3+zMWmlb8AUG5HtqFZh8tOf/rSpu7rz8/DDD29OqnVe+0FdyN1tt93MN2kOOeQQI67q96EKBRhdgNI+UtuBwkvHrdpvcgxLw8OIB1oSKK53emSX1gsV7XXepEKLjhH1WFrtP774xS8aUVS/FahzLj29QBeL9d82305zWRczG/RlPT2aWk9H0Et3YOsRuoXffLLEQ3IIeCdQPO7U+qb9j/5/9vJa4W+VBJhShXU9xsy+QaPf0dUXe/793//dPPb//b//Z765xAsI3kOGBwROAAEmcAdRPAiETKBQgNGdLHrMl05UdaeJXvqGuy7MZoKHptfvPqgwopNivfS7E6+++mpV34FYuXKlmQxr561HQujAQ5+l34cpFFSywUrxMWLZdzOGDBnSnF4H5noMy//93/+ZhTNdENadNRxBFnLkUTafAkzh95L0jUU97k9Fz+J6mg2yC+tT5hmt67pjplCAyc7T10Wqk046yUzS9exvBBjiOS8ECifC2k/oIpEK+FdddZVZTM0WejSudZFHF6l0AVbFlyxNdoxmNX2Mj7pYeNymLkYNHTrULI5p36yLa7qbtPCD4XnxDeWMm4BvAUZfCNDYV+FG6+Yf/vCHnQQVJVyqb9PfS4079QUGHV/qQrTuotHFqB/84AfmOxTZcZ5xew3r8k6guN7p94t0rqdzsWznWXY0X3Z0n+4A1Z0lOk9ToVTngro7pniuVo6N67qoNnzve98zYqn201on9dI+r9K3CvPuO8ofB4FSAsySJUvMiwCFAkz2m40A42OMmc1P9cUD7fu0vulR8HoUJ9/9jCMmsaI+Aggw9fHjbggkTaD4CDId5OoHuHWhVYUMfRtYr+ybK5pe3xYuXODR337yk5+0GEiUgqpHO6hwowtEumCrb+tmnbwec6Y7bLJLJ7e6OKZvMepxMNlVasFYj47R49CytNkxSvpWVOFbj0k7GuODI1CNAKMTYN2VpiJItTtg9M18vUcFycLvvZQSS/TtXp1Uq5iiO8d6V2JxAAAL3klEQVQKr+JFquyDjPp2ve5200mDPkd/V6Gm1LEUwUGnQMkTKHUWt/Zz+oavLkLpbks91iT7tpFOhFWA0f/pUZh6ZfVGj/vT3ZvlLl91UZ+n9U6Pgyj8kHi2K2b69OkycODA5H0NgLAIVCPAFNatanfAaL76DRfd4akvEulb/Loom32rrPj7ELrIpUd96hvH2XGfSqqUAKOLW/rWvabL0maL0SpyZt8pDIs0pYHARwRKHUGm/dp//ud/is6XVKjU3V96FQowGufZyQaah3535bOf/awZ95W7fNVFzVdFIf1+lJYpO9I6m+/p/JFvoRH1oRKoRoApXEupVoDxNcbM1lr0qGldY9EjCvXSuZ6KRtnpJKHyplwQ8E0AAcY3YfKHQCQEdACrbw9pR5q99ZQJMDNmzDDHDulWc+3Q9XxdnfzqGdza4WZvyOuEVbeiFr7xrr8Vv8lRiKxwQP65z33OCCWav17ZOaPayev52tlxDo888og53qX4+KNiAUbv1+PGVCwqdembGrydH0kA59wMfetQF3n1CCPdAVZOgCn8uKEeAahHflUrwGRHlukCsn6XSd9WzK7CY1dUzNFL3/xX4aXUm/zFAkx2LGA5N1SzGyDnLqT4OSSgdULFSD06QfuXbCKsizXad+hZ99kZ94ceeqg5fkUXcrNvRGQTYd3p2b9/f0Og0gdSM0Q+66I+Q9sT7f/07cTsyl5coC7mMFAjLLLGp34zUHdL63fHygkwhXWrsE+qRoDReqq7PLWOZ98VzMaR2Y7pMWPGtFic1Z1sKs4Ujw1LCTDal+oz9FuEWb7Fu+RU7OGCQCgEWqt3Wi9UsNDY1hdq9CU3fSFO53qZAKMiS+HRf9lcrdJxgD7rYuEOGB3LZt8xzOp44XeeQvED5UiXQKVxp9a94rUUjW+te1m/VI0A43OMmR29qW3Egw8+2PyC3e9+9ztzLDYvuKYb21i+gwACDJEAAQhURUA7eD23Whd/deKpk8nCj93rh9X0DafCYxWyb0ToMSc6yS0ltlQSYAqPStFJsAo8xde1114rTzzxRPPbiNk9zzzzjNlp07Vr1+ZbSgkwOsHXN4HVHhVydACj22T/+Z//2Rydpt+F4ZzuqkKERB4JZAJG9t2IrG7p8UF6/F62+KMfV9QPiuqlR53o4LcaASZ721ff0tWPq+oHxAsv/UiqviVcuKsse2O++A1hva9YgFFh9vHHH2+x00WPmcjOEdcPGH/iE5/wSJCsIWBPYPbs2eabRbpLU4X/rB7qZPcf//Efzc4WXVTKXgDIjrTUN9+1nyw1EW5NgPFdF7WMenSnlkPrcLYAnH3PplR9tifHHRCoj4D2cXqErcaqfu9P41bHkfq77hzR+NVvUei4UNPp9fTTT5uXB8otBBeWSEVI/SaF5qMLQv/0T//UosBah/XZulg7Z84cMw7UhSUdB/fo0cN84yk7/kVvLCXA6PhSBSI9/ugrX/mKyT9b9NV8i/Oojxh3Q6B+Aq3VO32DXU8y0Jd7spfxnn/+efMdzloFmEbUxWy8Wvj2fdbX6g5vfVGQCwIhEKg07tRd09lpIhrTeipINePOQrsaMcbUHd66I1yPIcx2wGTH0OsOUt0lzgWBVAkgwKTqeeyGgCUB7eD1OxA6odQ3BXUCmh2foANyfXtJxRc9614nyfpvTa+D+eyDxNoRVzq3tHAyq8XLJq/633rUkS7W6sfidGu7vnWli7bZpFffxJo0aZIRY/TjdPqGcuGxZJpHpW9WZDiys4z1zWZ9m5kLAiEQyL4boW8a6lFG+pZ94UdNs7jVeqCCjP5NhZfsfOtKR5BpndJFJRVJ9H7dZfP2228bs3URSt/010sXu7TuTZ06VTZu3Gje/s/qdnHdLXdOfsYyewv4r3/9a7OgGwJnygCBQgK6i+yGG24wRxPpjjAVVbSeqBCqx5hov6SXnoevRwrpcZb6rTI9Uk9fANDJsS4Oa53JjhurJMA0qi5mR0Hozh5tT7TMalu5+kxUQKDRBLTv0XGc1hf9YL0e5aUvx2i902OEsuP99P/1w746PtRFYX3rthoBRtP+9Kc/NX2kvuCj9Vov7eOOP/548yJDtstbX3Q49thjzdFjuqhUSqQsJcDoOFj7TS2Tjk913Pz9739ftD9H6Gx0RPG8agi0Vu/0hAHtE7/xjW/IcccdJ7///e9Nf6gxri8k6PcebHfANLouqiCqfbO2E9puaL/ds2fPavCQBgLeCVQad+oaSyYm6pj061//uvzbv/2b/OpXv2px9G3xuDMrdKPGmNnxfjqn1O+f6Tdg9OU+FV70xQN2fnoPIx4QMAEEmICdQ9EgEBoBnUzqxxez8361A9VJpC5O6YKqTk5VZMkuHRzoQq1OWFWk+dnPfmYm0IVHN+gEt/i37H7dHaMfSSx16YfBdaFZr0cffVR0J0zhc/WNpuIOPju2adiwYTt9syK7NzuGhclxaNFHebSe6JtR2fXlL39Zvvvd75o41/qnk0h9i10v/U3rn9aNbAeMTo5LfXcieyNX60fxVfiBUhV5dJdblk4H1irC6KJS8aX1WgXa4nPys3TZkRAqwPAWMLEdKgGNU11s0kWn7DrnnHPMd4/0+uUvfyl6BKcuPumli7b6VrDuPNNFKf13cb3LXgRoy7qoi2y6W06/rZZdKhBdfvnlZpcrFwRCIKA7zlS40EVSvXTXiL7Brkd66aXfHNRvFmX176tf/ap5OUF3nVQ6giz7JoS+uV/qyo7h03Rz58419Tm7JkyYIDqGLL4yAab424Nqg5YnE3i0Ty214yYE3pQBAkqgUr1TwTN7uz2jpTvF9ChOHQvqGFWPGar2CLJG1kUVPrU9Wbt2rSm6HsGpRwrqcdlcEAiFQGvjTh2/qYioJ33opS/mffrTnzbHQmc7r0OY761YscL0fVn/vO+++xrRs1u3bqGgphwQaBMCCDBtgp2HQiDfBP72t7+ZBV/tRIuP53r33XfNZHm33XYz/2vUpc/Ut/b1mYXn2jfq+TwHAo0g0Fqc6+RYj3PQo8R8fNhe670e2ac7XhhEN8LjPCMEAtq36JvxeqRlcb3SybD+Xd+obWTf46IuanuhL0no8ZuFx3WGwJwyQEAJZHGuwkWpGNXFHY1hrZe+6p/2qSqcap9Xa7+qLzBpW9G9e3eOtSW0gyfQWr3TeM6+I1ZrnagFgou6qGKojmG1PeGI6Vq8wD2NIFBp3KnP17qgx2L66lNcjDGz76A1enzcCP/wDAjUSgABplZy3AcBCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEyhBAgCE0IAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIBjAggwjoGSHQQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhBAgCEGIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIBjAggwjoGSHQQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhBAgCEGIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIBjAggwjoGSHQQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhBAgCEGIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIBjAggwjoGSHQQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhBAgCEGIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIBjAggwjoGSHQQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhBAgCEGIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIBjAggwjoGSHQQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhBAgCEGIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIBjAggwjoGSHQQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhBAgCEGIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIBjAv8fFa6wHvOC73AAAAAASUVORK5CYII="
     },
     "metadata": {
      "source_id": "2_152349354355"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%chart columns --data count_by_day --fields date,cs\n",
    "{\"legend\":{\"position\":\"none\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name count_91_by_day\n",
    "select extract(date from time) as date, count(*) as cs\n",
    "from  `bigquery-public-data.san_francisco.bikeshare_status`\n",
    "where station_id = 91\n",
    "group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqgc-container\">\n",
       "      \n",
       "      <div class=\"bqgc \" id=\"16_152349566085\">\n",
       "      </div>\n",
       "    </div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting',\n",
       "               'datalab/element!16_152349566085',\n",
       "               'base/js/events',\n",
       "               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n",
       "              ],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render(\n",
       "              'gcharts',\n",
       "              dom,\n",
       "              events,\n",
       "              'columns',\n",
       "              [],\n",
       "              {\"rows\": [{\"c\": [{\"v\": \"2016-08-24\"}, {\"v\": 559}]}, {\"c\": [{\"v\": \"2016-08-28\"}, {\"v\": 1181}]}, {\"c\": [{\"v\": \"2016-08-27\"}, {\"v\": 1181}]}, {\"c\": [{\"v\": \"2016-08-30\"}, {\"v\": 1192}]}, {\"c\": [{\"v\": \"2016-08-26\"}, {\"v\": 1193}]}, {\"c\": [{\"v\": \"2016-08-29\"}, {\"v\": 1193}]}, {\"c\": [{\"v\": \"2016-08-25\"}, {\"v\": 1194}]}, {\"c\": [{\"v\": \"2016-08-31\"}, {\"v\": 1212}]}], \"cols\": [{\"type\": \"date\", \"id\": \"date\", \"label\": \"date\"}, {\"type\": \"number\", \"id\": \"cs\", \"label\": \"cs\"}]},\n",
       "              {\"legend\": {\"position\": \"none\"}},\n",
       "              {\"fields\": \"date,cs\", \"source_index\": 1, \"name\": 1},\n",
       "              0,\n",
       "              8);\n",
       "          }\n",
       "        );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "source_id": "16_152349566085"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%chart columns --data count_91_by_day --fields date,cs\n",
    "{\"legend\":{\"position\":\"none\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query  --name count_6_by_day\n",
    "select extract(date from time) as date, count(*) as cs\n",
    "from  `bigquery-public-data.san_francisco.bikeshare_status`\n",
    "where station_id = 6\n",
    "group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqgc-container\">\n",
       "      \n",
       "      <div class=\"bqgc \" id=\"4_152349376853\">\n",
       "      </div>\n",
       "    </div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting',\n",
       "               'datalab/element!4_152349376853',\n",
       "               'base/js/events',\n",
       "               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n",
       "              ],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render(\n",
       "              'gcharts',\n",
       "              dom,\n",
       "              events,\n",
       "              'columns',\n",
       "              [],\n",
       "              {\"rows\": [{\"c\": [{\"v\": \"2016-08-11\"}, {\"v\": 1281}]}, {\"c\": [{\"v\": \"2013-08-30\"}, {\"v\": 1288}]}, {\"c\": [{\"v\": \"2016-07-02\"}, {\"v\": 1290}]}, {\"c\": [{\"v\": \"2013-09-04\"}, {\"v\": 1290}]}, {\"c\": [{\"v\": \"2013-09-01\"}, {\"v\": 1293}]}, {\"c\": [{\"v\": \"2016-07-10\"}, {\"v\": 1297}]}, {\"c\": [{\"v\": \"2016-06-14\"}, {\"v\": 1297}]}, {\"c\": [{\"v\": \"2016-07-09\"}, {\"v\": 1298}]}, {\"c\": [{\"v\": \"2016-08-12\"}, {\"v\": 1302}]}, {\"c\": [{\"v\": \"2016-07-26\"}, {\"v\": 1307}]}, {\"c\": [{\"v\": \"2016-08-14\"}, {\"v\": 1309}]}, {\"c\": [{\"v\": \"2013-09-10\"}, {\"v\": 1310}]}, {\"c\": [{\"v\": \"2016-08-13\"}, {\"v\": 1313}]}, {\"c\": [{\"v\": \"2016-06-23\"}, {\"v\": 1313}]}, {\"c\": [{\"v\": \"2016-05-01\"}, {\"v\": 1314}]}, {\"c\": [{\"v\": \"2015-04-21\"}, {\"v\": 1315}]}, {\"c\": [{\"v\": \"2016-06-30\"}, {\"v\": 1315}]}, {\"c\": [{\"v\": \"2013-08-31\"}, {\"v\": 1315}]}, {\"c\": [{\"v\": \"2016-08-08\"}, {\"v\": 1316}]}, {\"c\": [{\"v\": \"2013-10-26\"}, {\"v\": 1317}]}, {\"c\": [{\"v\": \"2013-09-05\"}, {\"v\": 1317}]}, {\"c\": [{\"v\": \"2013-10-16\"}, {\"v\": 1317}]}, {\"c\": [{\"v\": \"2016-07-03\"}, {\"v\": 1319}]}, {\"c\": [{\"v\": \"2016-06-17\"}, {\"v\": 1319}]}, {\"c\": [{\"v\": \"2016-04-30\"}, {\"v\": 1319}]}, {\"c\": [{\"v\": \"2016-08-02\"}, {\"v\": 1321}]}, {\"c\": [{\"v\": \"2016-06-21\"}, {\"v\": 1322}]}, {\"c\": [{\"v\": \"2016-07-01\"}, {\"v\": 1322}]}, {\"c\": [{\"v\": \"2016-08-10\"}, {\"v\": 1325}]}, {\"c\": [{\"v\": \"2016-06-28\"}, {\"v\": 1325}]}, {\"c\": [{\"v\": \"2016-06-22\"}, {\"v\": 1328}]}, {\"c\": [{\"v\": \"2014-01-05\"}, {\"v\": 1328}]}, {\"c\": [{\"v\": \"2016-06-04\"}, {\"v\": 1329}]}, {\"c\": [{\"v\": \"2016-06-16\"}, {\"v\": 1330}]}, {\"c\": [{\"v\": \"2015-07-08\"}, {\"v\": 1332}]}, {\"c\": [{\"v\": \"2016-06-13\"}, {\"v\": 1332}]}, {\"c\": [{\"v\": \"2016-07-28\"}, {\"v\": 1332}]}, {\"c\": [{\"v\": \"2016-06-15\"}, {\"v\": 1333}]}, {\"c\": [{\"v\": \"2016-06-29\"}, {\"v\": 1334}]}, {\"c\": [{\"v\": \"2016-08-05\"}, {\"v\": 1335}]}, {\"c\": [{\"v\": \"2016-06-07\"}, {\"v\": 1335}]}, {\"c\": [{\"v\": \"2016-05-02\"}, {\"v\": 1336}]}, {\"c\": [{\"v\": \"2016-05-17\"}, {\"v\": 1336}]}, {\"c\": [{\"v\": \"2016-07-12\"}, {\"v\": 1336}]}, {\"c\": [{\"v\": \"2016-06-26\"}, {\"v\": 1336}]}, {\"c\": [{\"v\": \"2016-07-08\"}, {\"v\": 1337}]}, {\"c\": [{\"v\": \"2016-07-29\"}, {\"v\": 1337}]}, {\"c\": [{\"v\": \"2016-08-06\"}, {\"v\": 1340}]}, {\"c\": [{\"v\": \"2016-06-18\"}, {\"v\": 1340}]}, {\"c\": [{\"v\": \"2016-06-25\"}, {\"v\": 1340}]}, {\"c\": [{\"v\": \"2016-07-07\"}, {\"v\": 1341}]}, {\"c\": [{\"v\": \"2016-08-04\"}, {\"v\": 1342}]}, {\"c\": [{\"v\": \"2016-05-21\"}, {\"v\": 1343}]}, {\"c\": [{\"v\": \"2016-05-10\"}, {\"v\": 1343}]}, {\"c\": [{\"v\": \"2016-07-30\"}, {\"v\": 1345}]}, {\"c\": [{\"v\": \"2016-06-27\"}, {\"v\": 1347}]}, {\"c\": [{\"v\": \"2016-05-24\"}, {\"v\": 1348}]}, {\"c\": [{\"v\": \"2016-06-10\"}, {\"v\": 1349}]}, {\"c\": [{\"v\": \"2016-07-31\"}, {\"v\": 1349}]}, {\"c\": [{\"v\": \"2016-06-09\"}, {\"v\": 1351}]}, {\"c\": [{\"v\": \"2016-08-07\"}, {\"v\": 1353}]}, {\"c\": [{\"v\": \"2016-07-13\"}, {\"v\": 1353}]}, {\"c\": [{\"v\": \"2016-04-29\"}, {\"v\": 1353}]}, {\"c\": [{\"v\": \"2016-06-12\"}, {\"v\": 1354}]}, {\"c\": [{\"v\": \"2016-05-23\"}, {\"v\": 1354}]}, {\"c\": [{\"v\": \"2016-06-20\"}, {\"v\": 1355}]}, {\"c\": [{\"v\": \"2016-05-22\"}, {\"v\": 1356}]}, {\"c\": [{\"v\": \"2016-07-04\"}, {\"v\": 1357}]}, {\"c\": [{\"v\": \"2016-06-05\"}, {\"v\": 1359}]}, {\"c\": [{\"v\": \"2016-05-09\"}, {\"v\": 1359}]}, {\"c\": [{\"v\": \"2013-10-15\"}, {\"v\": 1360}]}, {\"c\": [{\"v\": \"2016-05-28\"}, {\"v\": 1360}]}, {\"c\": [{\"v\": \"2016-06-24\"}, {\"v\": 1360}]}, {\"c\": [{\"v\": \"2016-06-11\"}, {\"v\": 1361}]}, {\"c\": [{\"v\": \"2016-07-15\"}, {\"v\": 1361}]}, {\"c\": [{\"v\": \"2016-05-06\"}, {\"v\": 1361}]}, {\"c\": [{\"v\": \"2016-06-19\"}, {\"v\": 1363}]}, {\"c\": [{\"v\": \"2016-05-26\"}, {\"v\": 1364}]}, {\"c\": [{\"v\": \"2013-09-12\"}, {\"v\": 1364}]}, {\"c\": [{\"v\": \"2016-05-31\"}, {\"v\": 1364}]}, {\"c\": [{\"v\": \"2016-06-03\"}, {\"v\": 1365}]}, {\"c\": [{\"v\": \"2016-08-01\"}, {\"v\": 1365}]}, {\"c\": [{\"v\": \"2016-05-11\"}, {\"v\": 1366}]}, {\"c\": [{\"v\": \"2016-07-06\"}, {\"v\": 1367}]}, {\"c\": [{\"v\": \"2014-01-04\"}, {\"v\": 1368}]}, {\"c\": [{\"v\": \"2013-09-11\"}, {\"v\": 1368}]}, {\"c\": [{\"v\": \"2016-08-03\"}, {\"v\": 1369}]}, {\"c\": [{\"v\": \"2016-07-17\"}, {\"v\": 1369}]}, {\"c\": [{\"v\": \"2013-09-26\"}, {\"v\": 1369}]}, {\"c\": [{\"v\": \"2016-06-08\"}, {\"v\": 1369}]}, {\"c\": [{\"v\": \"2013-09-13\"}, {\"v\": 1370}]}, {\"c\": [{\"v\": \"2016-07-05\"}, {\"v\": 1370}]}, {\"c\": [{\"v\": \"2016-06-01\"}, {\"v\": 1370}]}, {\"c\": [{\"v\": \"2016-07-14\"}, {\"v\": 1370}]}, {\"c\": [{\"v\": \"2016-05-19\"}, {\"v\": 1371}]}, {\"c\": [{\"v\": \"2016-07-16\"}, {\"v\": 1371}]}, {\"c\": [{\"v\": \"2016-07-25\"}, {\"v\": 1372}]}, {\"c\": [{\"v\": \"2016-07-22\"}, {\"v\": 1373}]}, {\"c\": [{\"v\": \"2016-05-05\"}, {\"v\": 1373}]}, {\"c\": [{\"v\": \"2016-05-25\"}, {\"v\": 1374}]}, {\"c\": [{\"v\": \"2013-10-17\"}, {\"v\": 1375}]}, {\"c\": [{\"v\": \"2016-05-12\"}, {\"v\": 1375}]}, {\"c\": [{\"v\": \"2013-09-25\"}, {\"v\": 1376}]}, {\"c\": [{\"v\": \"2016-07-18\"}, {\"v\": 1376}]}, {\"c\": [{\"v\": \"2016-07-23\"}, {\"v\": 1378}]}, {\"c\": [{\"v\": \"2016-05-20\"}, {\"v\": 1379}]}, {\"c\": [{\"v\": \"2016-05-18\"}, {\"v\": 1379}]}, {\"c\": [{\"v\": \"2016-03-13\"}, {\"v\": 1379}]}, {\"c\": [{\"v\": \"2014-03-09\"}, {\"v\": 1380}]}, {\"c\": [{\"v\": \"2015-03-08\"}, {\"v\": 1380}]}, {\"c\": [{\"v\": \"2016-05-13\"}, {\"v\": 1381}]}, {\"c\": [{\"v\": \"2016-05-07\"}, {\"v\": 1381}]}, {\"c\": [{\"v\": \"2013-09-27\"}, {\"v\": 1382}]}, {\"c\": [{\"v\": \"2014-02-19\"}, {\"v\": 1383}]}, {\"c\": [{\"v\": \"2015-08-02\"}, {\"v\": 1384}]}, {\"c\": [{\"v\": \"2013-11-02\"}, {\"v\": 1384}]}, {\"c\": [{\"v\": \"2016-04-24\"}, {\"v\": 1384}]}, {\"c\": [{\"v\": \"2016-07-19\"}, {\"v\": 1384}]}, {\"c\": [{\"v\": \"2016-04-28\"}, {\"v\": 1384}]}, {\"c\": [{\"v\": \"2016-05-08\"}, {\"v\": 1386}]}, {\"c\": [{\"v\": \"2013-09-24\"}, {\"v\": 1387}]}, {\"c\": [{\"v\": \"2016-04-25\"}, {\"v\": 1387}]}, {\"c\": [{\"v\": \"2016-06-02\"}, {\"v\": 1388}]}, {\"c\": [{\"v\": \"2016-04-22\"}, {\"v\": 1389}]}, {\"c\": [{\"v\": \"2016-04-21\"}, {\"v\": 1390}]}, {\"c\": [{\"v\": \"2016-05-27\"}, {\"v\": 1391}]}, {\"c\": [{\"v\": \"2016-04-01\"}, {\"v\": 1391}]}, {\"c\": [{\"v\": \"2016-05-16\"}, {\"v\": 1391}]}, {\"c\": [{\"v\": \"2016-04-20\"}, {\"v\": 1392}]}, {\"c\": [{\"v\": \"2016-07-21\"}, {\"v\": 1392}]}, {\"c\": [{\"v\": \"2016-04-27\"}, {\"v\": 1393}]}, {\"c\": [{\"v\": \"2016-04-19\"}, {\"v\": 1394}]}, {\"c\": [{\"v\": \"2016-07-11\"}, {\"v\": 1395}]}, {\"c\": [{\"v\": \"2016-07-20\"}, {\"v\": 1395}]}, {\"c\": [{\"v\": \"2016-05-30\"}, {\"v\": 1395}]}, {\"c\": [{\"v\": \"2016-06-06\"}, {\"v\": 1395}]}, {\"c\": [{\"v\": \"2016-04-10\"}, {\"v\": 1395}]}, {\"c\": [{\"v\": \"2016-05-29\"}, {\"v\": 1396}]}, {\"c\": [{\"v\": \"2016-04-12\"}, {\"v\": 1397}]}, {\"c\": [{\"v\": \"2016-04-05\"}, {\"v\": 1397}]}, {\"c\": [{\"v\": \"2015-11-03\"}, {\"v\": 1398}]}, {\"c\": [{\"v\": \"2016-04-02\"}, {\"v\": 1399}]}, {\"c\": [{\"v\": \"2016-04-26\"}, {\"v\": 1399}]}, {\"c\": [{\"v\": \"2013-09-23\"}, {\"v\": 1400}]}, {\"c\": [{\"v\": \"2016-04-09\"}, {\"v\": 1400}]}, {\"c\": [{\"v\": \"2014-01-24\"}, {\"v\": 1401}]}, {\"c\": [{\"v\": \"2016-04-15\"}, {\"v\": 1404}]}, {\"c\": [{\"v\": \"2016-07-24\"}, {\"v\": 1404}]}, {\"c\": [{\"v\": \"2016-04-23\"}, {\"v\": 1406}]}, {\"c\": [{\"v\": \"2013-09-20\"}, {\"v\": 1408}]}, {\"c\": [{\"v\": \"2016-04-18\"}, {\"v\": 1409}]}, {\"c\": [{\"v\": \"2013-08-29\"}, {\"v\": 642}]}, {\"c\": [{\"v\": \"2013-11-01\"}, {\"v\": 1410}]}, {\"c\": [{\"v\": \"2013-09-21\"}, {\"v\": 1411}]}, {\"c\": [{\"v\": \"2015-08-13\"}, {\"v\": 1411}]}, {\"c\": [{\"v\": \"2016-04-14\"}, {\"v\": 1411}]}, {\"c\": [{\"v\": \"2016-03-31\"}, {\"v\": 1411}]}, {\"c\": [{\"v\": \"2013-10-13\"}, {\"v\": 1411}]}, {\"c\": [{\"v\": \"2016-04-17\"}, {\"v\": 1411}]}, {\"c\": [{\"v\": \"2013-09-22\"}, {\"v\": 1411}]}, {\"c\": [{\"v\": \"2013-10-01\"}, {\"v\": 1412}]}, {\"c\": [{\"v\": \"2013-10-14\"}, {\"v\": 1412}]}, {\"c\": [{\"v\": \"2016-05-15\"}, {\"v\": 1412}]}, {\"c\": [{\"v\": \"2013-09-29\"}, {\"v\": 1413}]}, {\"c\": [{\"v\": \"2016-04-06\"}, {\"v\": 1413}]}, {\"c\": [{\"v\": \"2016-04-11\"}, {\"v\": 1413}]}, {\"c\": [{\"v\": \"2016-04-07\"}, {\"v\": 1414}]}, {\"c\": [{\"v\": \"2013-09-28\"}, {\"v\": 1415}]}, {\"c\": [{\"v\": \"2013-09-30\"}, {\"v\": 1416}]}, {\"c\": [{\"v\": \"2015-04-10\"}, {\"v\": 1416}]}, {\"c\": [{\"v\": \"2013-10-02\"}, {\"v\": 1416}]}, {\"c\": [{\"v\": \"2016-02-17\"}, {\"v\": 1416}]}, {\"c\": [{\"v\": \"2016-04-03\"}, {\"v\": 1416}]}, {\"c\": [{\"v\": \"2014-09-17\"}, {\"v\": 905}]}, {\"c\": [{\"v\": \"2016-04-13\"}, {\"v\": 1417}]}, {\"c\": [{\"v\": \"2013-09-19\"}, {\"v\": 1418}]}, {\"c\": [{\"v\": \"2015-12-02\"}, {\"v\": 1418}]}, {\"c\": [{\"v\": \"2013-10-12\"}, {\"v\": 1419}]}, {\"c\": [{\"v\": \"2013-10-07\"}, {\"v\": 1420}]}, {\"c\": [{\"v\": \"2013-09-15\"}, {\"v\": 1421}]}, {\"c\": [{\"v\": \"2016-04-16\"}, {\"v\": 1421}]}, {\"c\": [{\"v\": \"2016-05-04\"}, {\"v\": 1421}]}, {\"c\": [{\"v\": \"2016-04-04\"}, {\"v\": 1422}]}, {\"c\": [{\"v\": \"2013-09-14\"}, {\"v\": 1423}]}, {\"c\": [{\"v\": \"2013-10-11\"}, {\"v\": 1424}]}, {\"c\": [{\"v\": \"2013-09-16\"}, {\"v\": 1424}]}, {\"c\": [{\"v\": \"2016-05-14\"}, {\"v\": 1424}]}, {\"c\": [{\"v\": \"2013-10-10\"}, {\"v\": 1426}]}, {\"c\": [{\"v\": \"2015-07-07\"}, {\"v\": 1426}]}, {\"c\": [{\"v\": \"2013-10-09\"}, {\"v\": 1426}]}, {\"c\": [{\"v\": \"2013-11-03\"}, {\"v\": 1427}]}, {\"c\": [{\"v\": \"2014-12-16\"}, {\"v\": 1427}]}, {\"c\": [{\"v\": \"2015-06-01\"}, {\"v\": 1427}]}, {\"c\": [{\"v\": \"2013-09-17\"}, {\"v\": 1428}]}, {\"c\": [{\"v\": \"2014-07-18\"}, {\"v\": 1429}]}, {\"c\": [{\"v\": \"2013-10-05\"}, {\"v\": 1429}]}, {\"c\": [{\"v\": \"2013-10-19\"}, {\"v\": 1429}]}, {\"c\": [{\"v\": \"2013-09-18\"}, {\"v\": 1430}]}, {\"c\": [{\"v\": \"2016-05-03\"}, {\"v\": 1430}]}, {\"c\": [{\"v\": \"2016-04-08\"}, {\"v\": 1430}]}, {\"c\": [{\"v\": \"2013-12-01\"}, {\"v\": 1432}]}, {\"c\": [{\"v\": \"2015-12-20\"}, {\"v\": 1432}]}, {\"c\": [{\"v\": \"2015-10-21\"}, {\"v\": 1433}]}, {\"c\": [{\"v\": \"2013-10-03\"}, {\"v\": 1433}]}, {\"c\": [{\"v\": \"2013-10-08\"}, {\"v\": 1433}]}, {\"c\": [{\"v\": \"2015-05-06\"}, {\"v\": 1433}]}, {\"c\": [{\"v\": \"2015-06-06\"}, {\"v\": 1178}]}, {\"c\": [{\"v\": \"2015-06-09\"}, {\"v\": 1434}]}, {\"c\": [{\"v\": \"2013-10-04\"}, {\"v\": 1434}]}, {\"c\": [{\"v\": \"2014-04-11\"}, {\"v\": 1435}]}, {\"c\": [{\"v\": \"2014-11-15\"}, {\"v\": 1435}]}, {\"c\": [{\"v\": \"2015-08-21\"}, {\"v\": 1436}]}, {\"c\": [{\"v\": \"2014-03-01\"}, {\"v\": 1436}]}, {\"c\": [{\"v\": \"2015-04-07\"}, {\"v\": 1436}]}, {\"c\": [{\"v\": \"2015-07-06\"}, {\"v\": 1436}]}, {\"c\": [{\"v\": \"2013-10-06\"}, {\"v\": 1436}]}, {\"c\": [{\"v\": \"2016-08-28\"}, {\"v\": 1181}]}, {\"c\": [{\"v\": \"2016-08-27\"}, {\"v\": 1181}]}, {\"c\": [{\"v\": \"2014-02-01\"}, {\"v\": 1437}]}, {\"c\": [{\"v\": \"2013-10-24\"}, {\"v\": 1437}]}, {\"c\": [{\"v\": \"2014-01-01\"}, {\"v\": 1437}]}, {\"c\": [{\"v\": \"2014-10-02\"}, {\"v\": 1437}]}, {\"c\": [{\"v\": \"2013-10-27\"}, {\"v\": 1438}]}, {\"c\": [{\"v\": \"2015-02-12\"}, {\"v\": 1438}]}, {\"c\": [{\"v\": \"2015-07-03\"}, {\"v\": 1438}]}, {\"c\": [{\"v\": \"2014-01-31\"}, {\"v\": 1438}]}, {\"c\": [{\"v\": \"2015-11-09\"}, {\"v\": 1438}]}, {\"c\": [{\"v\": \"2014-10-27\"}, {\"v\": 1438}]}, {\"c\": [{\"v\": \"2015-05-05\"}, {\"v\": 1438}]}, {\"c\": [{\"v\": \"2015-01-07\"}, {\"v\": 1438}]}, {\"c\": [{\"v\": \"2015-05-31\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-05-30\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-17\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-26\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2014-11-03\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2014-04-28\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2013-10-22\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-07-27\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-03-17\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2013-10-23\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-27\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-07-28\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-18\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2013-10-21\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2013-10-18\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2014-05-02\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-25\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-05\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-24\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-19\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-20\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-01\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-03-30\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2014-04-23\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2014-01-16\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2014-03-06\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2013-12-12\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-06\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-19\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-06-07\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-29\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-20\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2013-11-18\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-11-08\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-04\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2013-11-30\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-27\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-16\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-24\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-30\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2013-10-20\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-29\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-06\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-21\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-23\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-01-31\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2014-10-25\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2013-10-30\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-03\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-22\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-12\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-04-02\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-07\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-28\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2014-10-01\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-18\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-09\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-02-16\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-28\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-11\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2016-03-10\"}, {\"v\": 1439}]}, {\"c\": [{\"v\": \"2015-05-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-10-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-10-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-10-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-03-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-03-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-03-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-03-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-03-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-03-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-03-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-10-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-04-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-10-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-01-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-06-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-11-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-09-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-05-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-05-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2013-12-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-07-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-03-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-31\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-25\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-11-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-02-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-19\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-08-29\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-03-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-07-26\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-04-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-27\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-20\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-18\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-01\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-24\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-22\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-13\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-28\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-14\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-09-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-07\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-06-30\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-01-02\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-21\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-02-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-10\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-03-23\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-02-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-08\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-16\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-03\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-08-09\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-06\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2015-12-05\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-01-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-15\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-11-04\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-10-17\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-11\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2014-12-12\"}, {\"v\": 1440}]}, {\"c\": [{\"v\": \"2016-08-30\"}, {\"v\": 1192}]}, {\"c\": [{\"v\": \"2016-08-29\"}, {\"v\": 1193}]}, {\"c\": [{\"v\": \"2016-08-26\"}, {\"v\": 1193}]}, {\"c\": [{\"v\": \"2016-08-25\"}, {\"v\": 1194}]}, {\"c\": [{\"v\": \"2016-08-24\"}, {\"v\": 1209}]}, {\"c\": [{\"v\": \"2016-08-20\"}, {\"v\": 1210}]}, {\"c\": [{\"v\": \"2016-08-31\"}, {\"v\": 1212}]}, {\"c\": [{\"v\": \"2016-08-18\"}, {\"v\": 1212}]}, {\"c\": [{\"v\": \"2014-03-30\"}, {\"v\": 447}]}, {\"c\": [{\"v\": \"2016-08-16\"}, {\"v\": 1218}]}, {\"c\": [{\"v\": \"2016-08-17\"}, {\"v\": 1220}]}, {\"c\": [{\"v\": \"2016-08-19\"}, {\"v\": 1244}]}, {\"c\": [{\"v\": \"2013-09-08\"}, {\"v\": 1253}]}, {\"c\": [{\"v\": \"2013-09-09\"}, {\"v\": 1255}]}, {\"c\": [{\"v\": \"2016-08-23\"}, {\"v\": 1261}]}, {\"c\": [{\"v\": \"2014-03-29\"}, {\"v\": 752}]}, {\"c\": [{\"v\": \"2016-08-15\"}, {\"v\": 1265}]}, {\"c\": [{\"v\": \"2016-08-22\"}, {\"v\": 1267}]}, {\"c\": [{\"v\": \"2016-07-27\"}, {\"v\": 1268}]}, {\"c\": [{\"v\": \"2014-09-16\"}, {\"v\": 757}]}, {\"c\": [{\"v\": \"2013-09-06\"}, {\"v\": 1272}]}, {\"c\": [{\"v\": \"2013-09-02\"}, {\"v\": 1272}]}, {\"c\": [{\"v\": \"2013-09-03\"}, {\"v\": 1273}]}, {\"c\": [{\"v\": \"2014-03-28\"}, {\"v\": 1274}]}, {\"c\": [{\"v\": \"2016-08-09\"}, {\"v\": 1275}]}, {\"c\": [{\"v\": \"2016-08-21\"}, {\"v\": 1276}]}, {\"c\": [{\"v\": \"2013-09-07\"}, {\"v\": 1277}]}], \"cols\": [{\"type\": \"date\", \"id\": \"date\", \"label\": \"date\"}, {\"type\": \"number\", \"id\": \"cs\", \"label\": \"cs\"}]},\n",
       "              {},\n",
       "              {\"fields\": \"date,cs\", \"source_index\": 2, \"name\": 2},\n",
       "              0,\n",
       "              1099);\n",
       "          }\n",
       "        );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "source_id": "4_152349376853"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%chart columns --data count_6_by_day --fields date,cs\n",
    "{\"legend\":{\"position\":\"none\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Yep - the station we looked at doesn't have data for the whole time series. That could be a bit of a problem, depending on how widespread the issue is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query  --name date_distributions\n",
    "select date_count\n",
    "from (\n",
    "select station_id, count(distinct extract(date from time)) as date_count\n",
    "from  `bigquery-public-data.san_francisco.bikeshare_status`\n",
    "group by station_id ) a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqgc-container\">\n",
       "      \n",
       "      <div class=\"bqgc \" id=\"9_152349426418\">\n",
       "      </div>\n",
       "    </div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting',\n",
       "               'datalab/element!9_152349426418',\n",
       "               'base/js/events',\n",
       "               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n",
       "              ],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render(\n",
       "              'gcharts',\n",
       "              dom,\n",
       "              events,\n",
       "              'histogram',\n",
       "              [],\n",
       "              {\"rows\": [{\"c\": [{\"v\": 1}]}, {\"c\": [{\"v\": 8}]}, {\"c\": [{\"v\": 8}]}, {\"c\": [{\"v\": 8}]}, {\"c\": [{\"v\": 8}]}, {\"c\": [{\"v\": 1037}]}, {\"c\": [{\"v\": 1038}]}, {\"c\": [{\"v\": 1059}]}, {\"c\": [{\"v\": 1059}]}, {\"c\": [{\"v\": 1087}]}, {\"c\": [{\"v\": 1089}]}, {\"c\": [{\"v\": 1096}]}, {\"c\": [{\"v\": 1098}]}, {\"c\": [{\"v\": 1098}]}, {\"c\": [{\"v\": 1098}]}, {\"c\": [{\"v\": 1098}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 1099}]}, {\"c\": [{\"v\": 863}]}, {\"c\": [{\"v\": 876}]}, {\"c\": [{\"v\": 953}]}, {\"c\": [{\"v\": 975}]}, {\"c\": [{\"v\": 975}]}, {\"c\": [{\"v\": 975}]}], \"cols\": [{\"type\": \"number\", \"id\": \"date_count\", \"label\": \"date_count\"}]},\n",
       "              {},\n",
       "              {\"fields\": \"date_count\", \"source_index\": 3, \"name\": 3},\n",
       "              0,\n",
       "              75);\n",
       "          }\n",
       "        );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "source_id": "9_152349426418"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%chart histogram --data date_distributions --fields date_count\n",
    "{\"legend\":{\"position\":\"none\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Yes - station 91 is an outlier. Most of the stations in out data have a large number of distinct dates.\n",
    "\n",
    "Let's have a look at the day of week seasonality in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name available-by-dow\n",
    "SELECT \n",
    "  extract(dayofweek from time) as dow, \n",
    "  sum(if(bikes_available < 2, 1, 0)) as not_available\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_status` status\n",
    "group by dow\n",
    "order by dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqgc-container\">\n",
       "      \n",
       "      <div class=\"bqgc \" id=\"10_152349515289\">\n",
       "      </div>\n",
       "    </div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting',\n",
       "               'datalab/element!10_152349515289',\n",
       "               'base/js/events',\n",
       "               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n",
       "              ],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render(\n",
       "              'gcharts',\n",
       "              dom,\n",
       "              events,\n",
       "              'columns',\n",
       "              [],\n",
       "              {\"rows\": [{\"c\": [{\"v\": 1}, {\"v\": 170583}]}, {\"c\": [{\"v\": 2}, {\"v\": 355263}]}, {\"c\": [{\"v\": 3}, {\"v\": 442632}]}, {\"c\": [{\"v\": 4}, {\"v\": 449672}]}, {\"c\": [{\"v\": 5}, {\"v\": 460371}]}, {\"c\": [{\"v\": 6}, {\"v\": 416080}]}, {\"c\": [{\"v\": 7}, {\"v\": 231342}]}], \"cols\": [{\"type\": \"number\", \"id\": \"dow\", \"label\": \"dow\"}, {\"type\": \"number\", \"id\": \"not_available\", \"label\": \"not_available\"}]},\n",
       "              {},\n",
       "              {\"fields\": \"dow,not_available\", \"source_index\": 4, \"name\": 4},\n",
       "              0,\n",
       "              7);\n",
       "          }\n",
       "        );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "source_id": "10_152349515289"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%chart columns --data available-by-dow --fields dow,not_available\n",
    "{\"legend\":{\"position\":\"none\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As expected, it's often harder to find a bike during the week, with higher availability during the weekend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's have a look at the hourly data. First off, we should define a binary indicator to make things a little easier for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name station-91-binary\n",
    "\n",
    "SELECT \n",
    "   status.time\n",
    "  , if(bikes_available > 2, 1, 0) as bikes_available\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_status` status \n",
    "where status.station_id = 91\n",
    "order by status.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%chart line --data station-91-binary --fields time,bikes_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqtv\" id=\"1_152333641531\"><table><tr><th>hour</th><th>f0_</th></tr><tr><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td></tr><tr><td>2</td><td>1</td></tr><tr><td>3</td><td>1</td></tr><tr><td>4</td><td>1</td></tr><tr><td>5</td><td>1</td></tr><tr><td>6</td><td>1</td></tr><tr><td>7</td><td>1</td></tr><tr><td>8</td><td>1</td></tr><tr><td>9</td><td>0</td></tr><tr><td>10</td><td>0</td></tr><tr><td>11</td><td>0</td></tr><tr><td>12</td><td>0</td></tr><tr><td>13</td><td>0</td></tr><tr><td>14</td><td>0</td></tr><tr><td>15</td><td>0</td></tr><tr><td>16</td><td>0</td></tr><tr><td>17</td><td>0</td></tr><tr><td>18</td><td>0</td></tr><tr><td>19</td><td>0</td></tr><tr><td>20</td><td>0</td></tr><tr><td>21</td><td>0</td></tr><tr><td>22</td><td>0</td></tr><tr><td>23</td><td>1</td></tr></table></div>\n",
       "    <br />(rows: 24, time: 0.2s, cached, job: job_rrW-GV6VgGfJcKZQFpSsmcxyM7ND)<br />\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting', 'datalab/element!1_152333641531', 'base/js/events',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/charting.css'],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render('gcharts', dom, events, 'table', [], {\"rows\": [{\"c\": [{\"v\": 0}, {\"v\": 1}]}, {\"c\": [{\"v\": 1}, {\"v\": 1}]}, {\"c\": [{\"v\": 2}, {\"v\": 1}]}, {\"c\": [{\"v\": 3}, {\"v\": 1}]}, {\"c\": [{\"v\": 4}, {\"v\": 1}]}, {\"c\": [{\"v\": 5}, {\"v\": 1}]}, {\"c\": [{\"v\": 6}, {\"v\": 1}]}, {\"c\": [{\"v\": 7}, {\"v\": 1}]}, {\"c\": [{\"v\": 8}, {\"v\": 1}]}, {\"c\": [{\"v\": 9}, {\"v\": 0}]}, {\"c\": [{\"v\": 10}, {\"v\": 0}]}, {\"c\": [{\"v\": 11}, {\"v\": 0}]}, {\"c\": [{\"v\": 12}, {\"v\": 0}]}, {\"c\": [{\"v\": 13}, {\"v\": 0}]}, {\"c\": [{\"v\": 14}, {\"v\": 0}]}, {\"c\": [{\"v\": 15}, {\"v\": 0}]}, {\"c\": [{\"v\": 16}, {\"v\": 0}]}, {\"c\": [{\"v\": 17}, {\"v\": 0}]}, {\"c\": [{\"v\": 18}, {\"v\": 0}]}, {\"c\": [{\"v\": 19}, {\"v\": 0}]}, {\"c\": [{\"v\": 20}, {\"v\": 0}]}, {\"c\": [{\"v\": 21}, {\"v\": 0}]}, {\"c\": [{\"v\": 22}, {\"v\": 0}]}, {\"c\": [{\"v\": 23}, {\"v\": 1}]}], \"cols\": [{\"type\": \"number\", \"id\": \"hour\", \"label\": \"hour\"}, {\"type\": \"number\", \"id\": \"f0_\", \"label\": \"f0_\"}]},\n",
       "            {\n",
       "              pageSize: 25,\n",
       "              cssClassNames:  {\n",
       "                tableRow: 'gchart-table-row',\n",
       "                headerRow: 'gchart-table-headerrow',\n",
       "                oddTableRow: 'gchart-table-oddrow',\n",
       "                selectedTableRow: 'gchart-table-selectedrow',\n",
       "                hoverTableRow: 'gchart-table-hoverrow',\n",
       "                tableCell: 'gchart-table-cell',\n",
       "                headerCell: 'gchart-table-headercell',\n",
       "                rowNumberCell: 'gchart-table-rownumcell'\n",
       "              }\n",
       "            },\n",
       "            {source_index: 0, fields: 'hour,f0_'},\n",
       "            0,\n",
       "            24);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "QueryResultsTable job_rrW-GV6VgGfJcKZQFpSsmcxyM7ND"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq query\n",
    "\n",
    "SELECT extract(hour from time) as hour, min(if(bikes_available > 2, 1, 0)) FROM `bigquery-public-data.san_francisco.bikeshare_status` status\n",
    "where station_id = 91\n",
    "group by hour\n",
    "order by hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It looks like there are some hours which are more likely to have 0 availability in some areas. Let's see if that generalises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name available-by-hour\n",
    "\n",
    "SELECT \n",
    "  extract(hour from time) as hour, \n",
    "  sum(if(bikes_available < 2, 1, 0)) as not_available\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_status` status\n",
    "group by hour\n",
    "order by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqgc-container\">\n",
       "      \n",
       "      <div class=\"bqgc \" id=\"15_152349561956\">\n",
       "      </div>\n",
       "    </div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting',\n",
       "               'datalab/element!15_152349561956',\n",
       "               'base/js/events',\n",
       "               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n",
       "              ],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render(\n",
       "              'gcharts',\n",
       "              dom,\n",
       "              events,\n",
       "              'columns',\n",
       "              [],\n",
       "              {\"rows\": [{\"c\": [{\"v\": 0}, {\"v\": 90171}]}, {\"c\": [{\"v\": 1}, {\"v\": 89994}]}, {\"c\": [{\"v\": 2}, {\"v\": 90372}]}, {\"c\": [{\"v\": 3}, {\"v\": 90362}]}, {\"c\": [{\"v\": 4}, {\"v\": 89166}]}, {\"c\": [{\"v\": 5}, {\"v\": 91782}]}, {\"c\": [{\"v\": 6}, {\"v\": 110106}]}, {\"c\": [{\"v\": 7}, {\"v\": 130604}]}, {\"c\": [{\"v\": 8}, {\"v\": 126174}]}, {\"c\": [{\"v\": 9}, {\"v\": 156713}]}, {\"c\": [{\"v\": 10}, {\"v\": 122848}]}, {\"c\": [{\"v\": 11}, {\"v\": 95832}]}, {\"c\": [{\"v\": 12}, {\"v\": 77684}]}, {\"c\": [{\"v\": 13}, {\"v\": 68449}]}, {\"c\": [{\"v\": 14}, {\"v\": 79967}]}, {\"c\": [{\"v\": 15}, {\"v\": 85975}]}, {\"c\": [{\"v\": 16}, {\"v\": 97038}]}, {\"c\": [{\"v\": 17}, {\"v\": 161925}]}, {\"c\": [{\"v\": 18}, {\"v\": 202204}]}, {\"c\": [{\"v\": 19}, {\"v\": 136729}]}, {\"c\": [{\"v\": 20}, {\"v\": 91648}]}, {\"c\": [{\"v\": 21}, {\"v\": 73157}]}, {\"c\": [{\"v\": 22}, {\"v\": 79654}]}, {\"c\": [{\"v\": 23}, {\"v\": 87389}]}], \"cols\": [{\"type\": \"number\", \"id\": \"hour\", \"label\": \"hour\"}, {\"type\": \"number\", \"id\": \"not_available\", \"label\": \"not_available\"}]},\n",
       "              {\"legend\": {\"position\": \"none\"}},\n",
       "              {\"fields\": \"hour,not_available\", \"source_index\": 5, \"name\": 5},\n",
       "              0,\n",
       "              24);\n",
       "          }\n",
       "        );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "source_id": "15_152349561956"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%chart columns --data available-by-hour --fields hour,not_available  \n",
    "{\"legend\":{\"position\":\"none\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "OK, so at standard commuting times we do see an uptick in the number of docks which we couldn't visit. This seasonality could be a useful signal in our model. Now to investigate location of the docking station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bq_data = bq.Query(\"\"\"\n",
    "select\n",
    "  status.hour,\n",
    "  stations.station_id,\n",
    "  stations.latitude,\n",
    "  stations.longitude,\n",
    "  status.bikes_available\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_stations` stations \n",
    "JOIN (\n",
    "select \n",
    "   station_id,\n",
    "   extract(hour from time) as hour,\n",
    "   extract(dayofweek from time) as dow,\n",
    "   sum(if(bikes_available > 2, 1, 0)) as bikes_available\n",
    "   from `bigquery-public-data.san_francisco.bikeshare_status`\n",
    "   group by station_id, hour\n",
    "   having hour=18\n",
    ") status on stations.station_id = status.station_id\n",
    "\"\"\")\n",
    "df = bq_data.execute(output_options=bq.QueryOutput.dataframe()).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f49b00bacd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_amount = float(df['bikes_available'].max())\n",
    "hmap = folium.Map(location=[37.333988, -121.894902], zoom_start=9)\n",
    "hm_wide = HeatMap(zip(df.latitude.values, df.longitude.values, df.bikes_available.values), \n",
    "                  min_opacity=0.2,\n",
    "                  max_val = max_amount,\n",
    "                  radius=17, blur=15,\n",
    "                  max_zoom=1,\n",
    "                 )\n",
    "hmap.add_child(hm_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There seem to be some areas which have higher levels of availability than others at 18-19. Including latitude and longitude in most algorithms can lead to strange results, so we should keep an eye on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name dockcount_and_availability\n",
    "\n",
    "select\n",
    "  sum(not_available) as not_available,\n",
    "  stations.dockcount\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_stations` stations \n",
    "JOIN (\n",
    "select \n",
    "   station_id,\n",
    "  sum(if(bikes_available < 2, 1, 0)) as not_available\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_status` status\n",
    "group by station_id\n",
    ") status on stations.station_id = status.station_id\n",
    "group by stations.dockcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqgc-container\">\n",
       "      \n",
       "      <div class=\"bqgc \" id=\"3_152357639013\">\n",
       "      </div>\n",
       "    </div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting',\n",
       "               'datalab/element!3_152357639013',\n",
       "               'base/js/events',\n",
       "               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n",
       "              ],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render(\n",
       "              'gcharts',\n",
       "              dom,\n",
       "              events,\n",
       "              'columns',\n",
       "              [],\n",
       "              {\"rows\": [{\"c\": [{\"v\": 31}, {\"v\": 11}]}, {\"c\": [{\"v\": 35}, {\"v\": 356}]}, {\"c\": [{\"v\": 19}, {\"v\": 905684}]}, {\"c\": [{\"v\": 27}, {\"v\": 93116}]}, {\"c\": [{\"v\": 15}, {\"v\": 1052912}]}, {\"c\": [{\"v\": 11}, {\"v\": 204281}]}, {\"c\": [{\"v\": 25}, {\"v\": 14804}]}, {\"c\": [{\"v\": 23}, {\"v\": 254612}]}], \"cols\": [{\"type\": \"number\", \"id\": \"dockcount\", \"label\": \"dockcount\"}, {\"type\": \"number\", \"id\": \"not_available\", \"label\": \"not_available\"}]},\n",
       "              {\"vAxis\": {\"title\": \"number of low availability events\"}, \"legend\": {\"position\": \"none\"}, \"hAxis\": {\"title\": \"size of docking station\"}},\n",
       "              {\"fields\": \"dockcount,not_available\", \"source_index\": 2, \"name\": 2},\n",
       "              0,\n",
       "              8);\n",
       "          }\n",
       "        );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "source_id": "3_152357639013"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%chart columns --data dockcount_and_availability --fields dockcount,not_available\n",
    "{\n",
    "  \"legend\":{\"position\":\"none\"},\n",
    "  \"vAxis\": {\n",
    "    \"title\":\"number of low availability events\"\n",
    "  },\n",
    "  \"hAxis\": {\n",
    "    \"title\": \"size of docking station\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name count_of_docking_sizes\n",
    "\n",
    "select dockcount, count(distinct station_id) as cs\n",
    "from `bigquery-public-data.san_francisco.bikeshare_stations`\n",
    "group by dockcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqgc-container\">\n",
       "      \n",
       "      <div class=\"bqgc \" id=\"2_152357637640\">\n",
       "      </div>\n",
       "    </div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting',\n",
       "               'datalab/element!2_152357637640',\n",
       "               'base/js/events',\n",
       "               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n",
       "              ],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render(\n",
       "              'gcharts',\n",
       "              dom,\n",
       "              events,\n",
       "              'columns',\n",
       "              [],\n",
       "              {\"rows\": [{\"c\": [{\"v\": 35}, {\"v\": 1}]}, {\"c\": [{\"v\": 25}, {\"v\": 1}]}, {\"c\": [{\"v\": 31}, {\"v\": 1}]}, {\"c\": [{\"v\": 11}, {\"v\": 4}]}, {\"c\": [{\"v\": 27}, {\"v\": 4}]}, {\"c\": [{\"v\": 23}, {\"v\": 9}]}, {\"c\": [{\"v\": 19}, {\"v\": 21}]}, {\"c\": [{\"v\": 15}, {\"v\": 33}]}], \"cols\": [{\"type\": \"number\", \"id\": \"dockcount\", \"label\": \"dockcount\"}, {\"type\": \"number\", \"id\": \"cs\", \"label\": \"cs\"}]},\n",
       "              {\"vAxis\": {\"title\": \"number of stations\"}, \"legend\": {\"position\": \"none\"}, \"hAxis\": {\"title\": \"size of docking station\"}},\n",
       "              {\"fields\": \"dockcount,cs\", \"source_index\": 1, \"name\": 1},\n",
       "              0,\n",
       "              8);\n",
       "          }\n",
       "        );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "source_id": "2_152357637640"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%chart columns --data count_of_docking_sizes --fields dockcount,cs\n",
    "{\n",
    "  \"legend\":{\"position\":\"none\"},\n",
    "  \"vAxis\": {\n",
    "    \"title\":\"number of stations\"\n",
    "  },\n",
    "  \"hAxis\": {\n",
    "    \"title\": \"size of docking station\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It seems that medium size docking stations have a reasonable chance of having a lack of availability. This is because most docking stations have a capacity of 15. This may mean that the dockcount property is not a reliable marker of availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bq query --name location_and_availability\n",
    "\n",
    "select\n",
    "  sum(not_available) as not_available,\n",
    "  stations.landmark\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_stations` stations \n",
    "JOIN (\n",
    "select \n",
    "   station_id,\n",
    "  sum(if(bikes_available < 2, 1, 0)) as not_available\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_status` status\n",
    "group by station_id\n",
    ") status on stations.station_id = status.station_id\n",
    "group by stations.landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqgc-container\">\n",
       "      \n",
       "      <div class=\"bqgc \" id=\"1_152357631831\">\n",
       "      </div>\n",
       "    </div>\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting',\n",
       "               'datalab/element!1_152357631831',\n",
       "               'base/js/events',\n",
       "               'datalab/style!/nbextensions/gcpdatalab/charting.css'\n",
       "              ],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render(\n",
       "              'gcharts',\n",
       "              dom,\n",
       "              events,\n",
       "              'columns',\n",
       "              [],\n",
       "              {\"rows\": [{\"c\": [{\"v\": \"San Jose\"}, {\"v\": 293190}]}, {\"c\": [{\"v\": \"Palo Alto\"}, {\"v\": 78023}]}, {\"c\": [{\"v\": \"Mountain View\"}, {\"v\": 179662}]}, {\"c\": [{\"v\": \"Redwood City\"}, {\"v\": 57657}]}, {\"c\": [{\"v\": \"San Francisco\"}, {\"v\": 1917244}]}], \"cols\": [{\"type\": \"string\", \"id\": \"landmark\", \"label\": \"landmark\"}, {\"type\": \"number\", \"id\": \"not_available\", \"label\": \"not_available\"}]},\n",
       "              {\"vAxis\": {\"title\": \"number of low availability events\"}, \"legend\": {\"position\": \"none\"}, \"hAxis\": {\"title\": \"location of docking station\"}},\n",
       "              {\"fields\": \"landmark,not_available\", \"source_index\": 0, \"name\": 0},\n",
       "              0,\n",
       "              5);\n",
       "          }\n",
       "        );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "source_id": "1_152357631831"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%chart columns --data location_and_availability --fields landmark,not_available\n",
    "{\n",
    "  \"legend\":{\"position\":\"none\"},\n",
    "  \"vAxis\": {\n",
    "    \"title\":\"number of low availability events\"\n",
    "  },\n",
    "  \"hAxis\": {\n",
    "    \"title\": \"location of docking station\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Almost all of the lack of availability comes from docking stations in San Francisco, good news if you are in Mountain View and need a city hire bike.\n",
    "Onto building a model. We will include hour, latitude and longitude, dockcount, region name, and a binary indicator of availability. \n",
    "How much data should we include?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqtv\" id=\"2_152333798417\"><table><tr><th>f0_</th></tr><tr><td>107501619</td></tr></table></div>\n",
       "    <br />(rows: 1, time: 1.4s,     0B processed, job: job_aYF5ZbLDHJ-TD9nI9uFXhae0hMLk)<br />\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting', 'datalab/element!2_152333798417', 'base/js/events',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/charting.css'],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render('gcharts', dom, events, 'table', [], {\"rows\": [{\"c\": [{\"v\": 107501619}]}], \"cols\": [{\"type\": \"number\", \"id\": \"f0_\", \"label\": \"f0_\"}]},\n",
       "            {\n",
       "              pageSize: 25,\n",
       "              cssClassNames:  {\n",
       "                tableRow: 'gchart-table-row',\n",
       "                headerRow: 'gchart-table-headerrow',\n",
       "                oddTableRow: 'gchart-table-oddrow',\n",
       "                selectedTableRow: 'gchart-table-selectedrow',\n",
       "                hoverTableRow: 'gchart-table-hoverrow',\n",
       "                tableCell: 'gchart-table-cell',\n",
       "                headerCell: 'gchart-table-headercell',\n",
       "                rowNumberCell: 'gchart-table-rownumcell'\n",
       "              }\n",
       "            },\n",
       "            {source_index: 1, fields: 'f0_'},\n",
       "            0,\n",
       "            1);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "QueryResultsTable job_aYF5ZbLDHJ-TD9nI9uFXhae0hMLk"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq query\n",
    "\n",
    "SELECT \n",
    "  count(*) as count_records\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_status` status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Yowza, 100 million records. We almost definitely don't need all of those, and we'd need a larger machine than this notebook is running on to comfortably handle them. Let's check the balance of this dataset to set out a sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqtv\" id=\"1_152334342100\"><table><tr><th>low_availability</th><th>f0_</th></tr><tr><td>0</td><td>104975676</td></tr><tr><td>1</td><td>2525943</td></tr></table></div>\n",
       "    <br />(rows: 2, time: 0.4s, cached, job: job_gpkJRZ288zc3n6lzRVe3oF4buUYD)<br />\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting', 'datalab/element!1_152334342100', 'base/js/events',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/charting.css'],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render('gcharts', dom, events, 'table', [], {\"rows\": [{\"c\": [{\"v\": 0}, {\"v\": 104975676}]}, {\"c\": [{\"v\": 1}, {\"v\": 2525943}]}], \"cols\": [{\"type\": \"number\", \"id\": \"low_availability\", \"label\": \"low_availability\"}, {\"type\": \"number\", \"id\": \"f0_\", \"label\": \"f0_\"}]},\n",
       "            {\n",
       "              pageSize: 25,\n",
       "              cssClassNames:  {\n",
       "                tableRow: 'gchart-table-row',\n",
       "                headerRow: 'gchart-table-headerrow',\n",
       "                oddTableRow: 'gchart-table-oddrow',\n",
       "                selectedTableRow: 'gchart-table-selectedrow',\n",
       "                hoverTableRow: 'gchart-table-hoverrow',\n",
       "                tableCell: 'gchart-table-cell',\n",
       "                headerCell: 'gchart-table-headercell',\n",
       "                rowNumberCell: 'gchart-table-rownumcell'\n",
       "              }\n",
       "            },\n",
       "            {source_index: 0, fields: 'low_availability,f0_'},\n",
       "            0,\n",
       "            2);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "QueryResultsTable job_gpkJRZ288zc3n6lzRVe3oF4buUYD"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq query\n",
    "select if(status.bikes_available < 2, 1, 0) as low_availability, count(*) as count_records\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_status` status\n",
    "group by low_availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "OK, so around 2% of our data is \"low availability\". We don't want to lose our signal. Let's rebalance these data.\n",
    "\n",
    "We'll use a good trick on BigQuery, RAND() < sample fraction, to randomly sample the dataset before pulling data out of BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>dockcount</th>\n",
       "      <th>in_sf</th>\n",
       "      <th>in_sj</th>\n",
       "      <th>in_rc</th>\n",
       "      <th>in_mv</th>\n",
       "      <th>in_pa</th>\n",
       "      <th>low_availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37.333988</td>\n",
       "      <td>-121.894902</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>37.333988</td>\n",
       "      <td>-121.894902</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>37.333988</td>\n",
       "      <td>-121.894902</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>37.333988</td>\n",
       "      <td>-121.894902</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>37.333988</td>\n",
       "      <td>-121.894902</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour   latitude   longitude  dockcount  in_sf  in_sj  in_rc  in_mv  in_pa  \\\n",
       "0     0  37.333988 -121.894902         11      0      1      0      0      0   \n",
       "1     0  37.333988 -121.894902         11      0      1      0      0      0   \n",
       "2     0  37.333988 -121.894902         11      0      1      0      0      0   \n",
       "3     0  37.333988 -121.894902         11      0      1      0      0      0   \n",
       "4     0  37.333988 -121.894902         11      0      1      0      0      0   \n",
       "\n",
       "   low_availability  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_class_count = 100000\n",
    "high_class_count = 100000\n",
    "q = bq.Query(\"\"\"\n",
    "SELECT \n",
    "  hour,\n",
    "  latitude,\n",
    "  longitude,\n",
    "  dockcount,\n",
    "  in_sf,\n",
    "  in_sj,\n",
    "  in_rc,\n",
    "  in_mv,\n",
    "  in_pa,\n",
    "  low_availability\n",
    "FROM (\n",
    "SELECT \n",
    "    extract(hour from time) as hour\n",
    "  , extract(dayofweek from time) as dow\n",
    "  , stations.latitude\n",
    "  , stations.longitude\n",
    "  , stations.dockcount\n",
    "  , if(stations.landmark = 'San Francisco', 1, 0) as in_sf\n",
    "  , if(stations.landmark = 'San Jose', 1, 0) as in_sj\n",
    "  , if(stations.landmark = 'Redwood City', 1, 0) as in_rc\n",
    "  , if(stations.landmark = 'Mountain View', 1, 0) as in_mv\n",
    "  , if(stations.landmark = 'Palo Alto', 1, 0) as in_pa\n",
    "  , if(status.bikes_available < 2, 1, 0) as low_availability\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_stations` stations \n",
    "JOIN `bigquery-public-data.san_francisco.bikeshare_status` status on stations.station_id = status.station_id\n",
    ") a\n",
    "WHERE (low_availability = 1 AND RAND() < {}/104975676) OR (low_availability = 0 AND RAND() < {}/104975676)\n",
    "\"\"\".format(low_class_count, high_class_count))\n",
    "df = q.execute(output_options=bq.QueryOutput.dataframe()).result()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have selected our data, let's train a model using the TensorFlow estimator API. This makes building a simple model more straightforward and adds out of the box functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6189587810>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/sfc_model', '_save_summary_steps': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6189587810>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/sfc_model', '_save_summary_steps': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/sfc_model/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/sfc_model/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 88.72288, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 88.72288, step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 196.274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 196.274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 24.610237, step = 101 (0.515 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 24.610237, step = 101 (0.515 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 251.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 251.638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 10.851547, step = 201 (0.395 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 10.851547, step = 201 (0.395 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 246.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 246.225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 117.18285, step = 301 (0.408 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 117.18285, step = 301 (0.408 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 263.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 263.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.543789, step = 401 (0.380 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.543789, step = 401 (0.380 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 252.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 252.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.1689285, step = 501 (0.395 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.1689285, step = 501 (0.395 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 230.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 230.139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 25.521585, step = 601 (0.435 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 25.521585, step = 601 (0.435 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 204.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 204.308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 5.380833, step = 701 (0.490 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 5.380833, step = 701 (0.490 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 234.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 234.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.3597503, step = 801 (0.429 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.3597503, step = 801 (0.429 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 217.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 217.651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 10.658804, step = 901 (0.457 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 10.658804, step = 901 (0.457 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 212.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 212.756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.2263172, step = 1001 (0.469 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.2263172, step = 1001 (0.469 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 5.544381, step = 1101 (0.477 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 5.544381, step = 1101 (0.477 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 245.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 245.548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.263328, step = 1201 (0.406 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.263328, step = 1201 (0.406 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.022989, step = 1301 (0.474 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.022989, step = 1301 (0.474 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 205.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 205.294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 16.952559, step = 1401 (0.488 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 16.952559, step = 1401 (0.488 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.437456, step = 1501 (0.476 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.437456, step = 1501 (0.476 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 15.517637, step = 1601 (0.485 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 15.517637, step = 1601 (0.485 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.506597, step = 1701 (0.483 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.506597, step = 1701 (0.483 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 221.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 221.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 14.112251, step = 1801 (0.451 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 14.112251, step = 1801 (0.451 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 208.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 208.053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 19.72948, step = 1901 (0.482 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 19.72948, step = 1901 (0.482 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 21.48827, step = 2001 (0.482 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 21.48827, step = 2001 (0.482 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 207.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 207.792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 5.0304427, step = 2101 (0.481 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 5.0304427, step = 2101 (0.481 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 18.868639, step = 2201 (0.485 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 18.868639, step = 2201 (0.485 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 201.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 201.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 13.169241, step = 2301 (0.498 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 13.169241, step = 2301 (0.498 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 221.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 221.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 10.361349, step = 2401 (0.456 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 10.361349, step = 2401 (0.456 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 201.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 201.461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.446033, step = 2501 (0.493 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.446033, step = 2501 (0.493 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.078503, step = 2601 (0.475 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.078503, step = 2601 (0.475 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 239.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 239.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.494724, step = 2701 (0.417 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.494724, step = 2701 (0.417 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 236.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 236.709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 11.731966, step = 2801 (0.422 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 11.731966, step = 2801 (0.422 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 218.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 218.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 18.521553, step = 2901 (0.462 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 18.521553, step = 2901 (0.462 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 34.190365, step = 3001 (0.475 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 34.190365, step = 3001 (0.475 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 254.148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 254.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 5.42079, step = 3101 (0.397 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 5.42079, step = 3101 (0.397 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 212.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 212.113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.2789297, step = 3201 (0.471 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.2789297, step = 3201 (0.471 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 233.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 233.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 15.392616, step = 3301 (0.424 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 15.392616, step = 3301 (0.424 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 214.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 214.441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.755715, step = 3401 (0.467 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.755715, step = 3401 (0.467 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 204.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 204.511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 24.246456, step = 3501 (0.491 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 24.246456, step = 3501 (0.491 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 211.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 211.311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.645162, step = 3601 (0.473 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.645162, step = 3601 (0.473 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 231.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 231.067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.5278306, step = 3701 (0.433 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.5278306, step = 3701 (0.433 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 215.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 215.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 15.401213, step = 3801 (0.464 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 15.401213, step = 3801 (0.464 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.11576, step = 3901 (0.484 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.11576, step = 3901 (0.484 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 208.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 208.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 23.46291, step = 4001 (0.477 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 23.46291, step = 4001 (0.477 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 205.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 205.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 19.463306, step = 4101 (0.488 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 19.463306, step = 4101 (0.488 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 224.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 224.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.722035, step = 4201 (0.452 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.722035, step = 4201 (0.452 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.394278, step = 4301 (0.474 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.394278, step = 4301 (0.474 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 214.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 214.151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 12.760031, step = 4401 (0.466 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 12.760031, step = 4401 (0.466 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 201.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 201.488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 4.8472004, step = 4501 (0.496 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 4.8472004, step = 4501 (0.496 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 237.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 237.179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 14.879469, step = 4601 (0.425 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 14.879469, step = 4601 (0.425 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 209.728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.778162, step = 4701 (0.475 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.778162, step = 4701 (0.475 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 208.832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 208.832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 26.01438, step = 4801 (0.478 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 26.01438, step = 4801 (0.478 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 13.092712, step = 4901 (0.476 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 13.092712, step = 4901 (0.476 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 236.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 236.953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.099085, step = 5001 (0.422 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.099085, step = 5001 (0.422 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 223.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 223.663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.5179, step = 5101 (0.448 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 17.5179, step = 5101 (0.448 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 208.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 208.076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.6398144, step = 5201 (0.479 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.6398144, step = 5201 (0.479 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.3102264, step = 5301 (0.479 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.3102264, step = 5301 (0.479 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 218.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 218.744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.8652706, step = 5401 (0.456 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.8652706, step = 5401 (0.456 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 234.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 234.493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 10.574279, step = 5501 (0.427 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 10.574279, step = 5501 (0.427 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 210.802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.3891673, step = 5601 (0.474 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.3891673, step = 5601 (0.474 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 214.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 214.135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 12.511558, step = 5701 (0.467 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 12.511558, step = 5701 (0.467 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 238.886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 238.886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.918871, step = 5801 (0.417 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.918871, step = 5801 (0.417 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 258.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 258.044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 7.3953824, step = 5901 (0.391 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 7.3953824, step = 5901 (0.391 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 211.244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 211.244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.273862, step = 6001 (0.471 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 6.273862, step = 6001 (0.471 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 202.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 202.795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 18.113989, step = 6101 (0.496 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 18.113989, step = 6101 (0.496 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 219.244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 219.244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 11.3785925, step = 6201 (0.452 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 11.3785925, step = 6201 (0.452 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 215.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 215.463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.855383, step = 6301 (0.464 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 9.855383, step = 6301 (0.464 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 31.567205, step = 6401 (0.484 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 31.567205, step = 6401 (0.484 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 6432 into /tmp/sfc_model/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 6432 into /tmp/sfc_model/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 4.309165.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 4.309165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-12-01:03:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-12-01:03:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/sfc_model/model.ckpt-6432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/sfc_model/model.ckpt-6432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-04-12-01:03:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-04-12-01:03:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 6432: accuracy = 0.9767758, accuracy_baseline = 0.9767758, auc = 0.6933974, auc_precision_recall = 0.063388176, average_loss = 0.1045431, global_step = 6432, label/mean = 0.023224177, loss = 13.381256, prediction/mean = 0.021453679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 6432: accuracy = 0.9767758, accuracy_baseline = 0.9767758, auc = 0.6933974, auc_precision_recall = 0.063388176, average_loss = 0.1045431, global_step = 6432, label/mean = 0.023224177, loss = 13.381256, prediction/mean = 0.021453679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9767758\n",
      "accuracy_baseline: 0.9767758\n",
      "auc: 0.6933974\n",
      "auc_precision_recall: 0.063388176\n",
      "average_loss: 0.1045431\n",
      "global_step: 6432\n",
      "label/mean: 0.023224177\n",
      "loss: 13.381256\n",
      "prediction/mean: 0.021453679\n"
     ]
    }
   ],
   "source": [
    "# split the data into test and train\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]\n",
    "\n",
    "target_column = 'low_availability'\n",
    "y_test = df_test[target_column]\n",
    "y_train = df_train[target_column]\n",
    "df_train = df_train.drop(columns=[target_column])\n",
    "df_test = df_test.drop(columns=[target_column])\n",
    "\n",
    "# model parameters\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "shuffle=True\n",
    "\n",
    "# input functions\n",
    "input_fn_train = tf.estimator.inputs.pandas_input_fn(\n",
    "  df_train, y=y_train,\n",
    "  batch_size=batch_size, num_epochs=num_epochs, shuffle=shuffle, target_column=target_column\n",
    ")\n",
    "\n",
    "input_fn_test = tf.estimator.inputs.pandas_input_fn(\n",
    "  df_test, y=y_test,\n",
    "  batch_size=batch_size, num_epochs=num_epochs, shuffle=shuffle, target_column=target_column\n",
    ")\n",
    "\n",
    "# feature columns to use as input\n",
    "lat = tf.feature_column.numeric_column('latitude')\n",
    "lon = tf.feature_column.numeric_column('longitude')\n",
    "dc = tf.feature_column.numeric_column('dockcount')\n",
    "dockc = tf.feature_column.bucketized_column(dc, boundaries=[11, 15, 19, 23, 25])\n",
    "sf = tf.feature_column.categorical_column_with_vocabulary_list('in_sf', [0,1])\n",
    "sj = tf.feature_column.categorical_column_with_vocabulary_list('in_sj', [0,1])\n",
    "rc = tf.feature_column.categorical_column_with_vocabulary_list('in_rc', [0,1])\n",
    "mv = tf.feature_column.categorical_column_with_vocabulary_list('in_mv', [0,1])\n",
    "pa = tf.feature_column.categorical_column_with_vocabulary_list('in_pa', [0,1])\n",
    "hour = tf.feature_column.categorical_column_with_vocabulary_list('hour', [0,1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
    "dow = tf.feature_column.categorical_column_with_vocabulary_list('dow', [1, 2, 3, 4, 5, 6, 7])\n",
    "base_columns = [lat, lon, dockc, sf, sj, rc, mv, pa, hour]\n",
    "\n",
    "# model training and evaluation\n",
    "model = tf.estimator.LinearClassifier(model_dir='/tmp/sfc_model', feature_columns=base_columns)\n",
    "model.train(input_fn=input_fn_train)\n",
    "results = model.evaluate(input_fn=input_fn_test)\n",
    "for key in sorted(results):\n",
    "  print('%s: %s' % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This model is ok, AUC varying between 0.6 and 0.8 depending on the sample. High accuracy. \n",
    "\n",
    "I am interesting in expanding this to be able to predict if a specific station is going to have capacity in n time steps. This could power a service where I can be told what the chances are that the bike I'm hoping to get will be gone by the time I get there - this would have been incredibly useful on my last visit to NYC.\n",
    "\n",
    "To start with, let's check the frequency of status updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqtv\" id=\"1_152340919426\"><table><tr><th>f0_</th><th>f1_</th><th>f2_</th><th>f3_</th></tr><tr><td>51</td><td>60.9163333133</td><td>2985300</td><td>107501619</td></tr></table></div>\n",
       "    <br />(rows: 1, time: 22.4s,     1GB processed, job: job_RH6Q5m8bknERcaXcnrRtxP8hQFfp)<br />\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting', 'datalab/element!1_152340919426', 'base/js/events',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/charting.css'],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render('gcharts', dom, events, 'table', [], {\"rows\": [{\"c\": [{\"v\": 51}, {\"v\": 60.91633331331513}, {\"v\": 2985300}, {\"v\": 107501619}]}], \"cols\": [{\"type\": \"number\", \"id\": \"f0_\", \"label\": \"f0_\"}, {\"type\": \"number\", \"id\": \"f1_\", \"label\": \"f1_\"}, {\"type\": \"number\", \"id\": \"f2_\", \"label\": \"f2_\"}, {\"type\": \"number\", \"id\": \"f3_\", \"label\": \"f3_\"}]},\n",
       "            {\n",
       "              pageSize: 25,\n",
       "              cssClassNames:  {\n",
       "                tableRow: 'gchart-table-row',\n",
       "                headerRow: 'gchart-table-headerrow',\n",
       "                oddTableRow: 'gchart-table-oddrow',\n",
       "                selectedTableRow: 'gchart-table-selectedrow',\n",
       "                hoverTableRow: 'gchart-table-hoverrow',\n",
       "                tableCell: 'gchart-table-cell',\n",
       "                headerCell: 'gchart-table-headercell',\n",
       "                rowNumberCell: 'gchart-table-rownumcell'\n",
       "              }\n",
       "            },\n",
       "            {source_index: 0, fields: 'f0_,f1_,f2_,f3_'},\n",
       "            0,\n",
       "            1);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "QueryResultsTable job_RH6Q5m8bknERcaXcnrRtxP8hQFfp"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq query\n",
    "\n",
    "select min(sd) as min_td, avg(sd) as mean_td, max(sd) as max_td, count(*) as num_records from (\n",
    "SELECT  \n",
    "timestamp_diff( lead(time) \n",
    "over(partition by station_id \n",
    "order by time asc\n",
    "), time, SECOND) as sd\n",
    "FROM `bigquery-public-data.san_francisco.bikeshare_status` \n",
    ") a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Right so oftentimes, the station sends a status every minute. Let's pull data for a specific station and get started with predicting availability. I'm going to use LEAD(), an analytical function from BigQuery, to pull out a future record for the query station. I'm also preparing two columns for labels, which is how TensorFlow likes to consume target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the station_id is a global variable. If it is changed, the data extraction and model training should be rerun.\n",
    "station_id = 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "q = bq.Query(\n",
    "\"\"\"\n",
    "SELECT\n",
    "  bikes_available,\n",
    "  IF(next_ba <= 2,\n",
    "    1,\n",
    "    0) AS low_availability,\n",
    "  IF(next_ba > 2,\n",
    "    1,\n",
    "    0) AS high_availability  \n",
    "FROM (\n",
    "  SELECT\n",
    "    time,\n",
    "    bikes_available,\n",
    "    LEAD(bikes_available, 30) OVER(PARTITION BY station_id ORDER BY time ASC ) AS next_ba\n",
    "  FROM\n",
    "    `bigquery-public-data.san_francisco.bikeshare_status`\n",
    "  WHERE\n",
    "    station_id = {}\n",
    "  ORDER BY\n",
    "    time ASC ) a\n",
    "WHERE\n",
    "  next_ba IS NOT NULL\n",
    "\"\"\".format(station_id)\n",
    ")\n",
    "df = q.execute(output_options=bq.QueryOutput.dataframe()).result()\n",
    "\n",
    "# we need to massage the data to pass it to a model as a time series\n",
    "as_strided = np.lib.stride_tricks.as_strided  \n",
    "win = 15 # collect data for next 15 steps\n",
    "v = as_strided(df.bikes_available, (len(df) - (win - 1), win), (df.bikes_available.values.strides * 2))\n",
    "df['strides'] = pd.Series(v.tolist())\n",
    "df = df[df.strides.notnull()]  # If nulls are not filtered out, TensorFlow throws really strange exceptions\n",
    "\n",
    "# test/train split\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "test = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need an iterator for the DataFrame to allow random batches out of the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reference: https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\n",
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n-1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.iloc[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        x = np.expand_dims(np.array(res['strides'].tolist()), axis=1)\n",
    "        y = res[['low_availability', 'high_availability']].as_matrix()\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's define a TensorFlow graph, that part which holds the directions for data and weights moving around the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 2000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 15\n",
    "timesteps = 1\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 2\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float32\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float32\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "  \n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "  \n",
    "logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have prepared data and a defined graph, we work with a Session object to learn weights for the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate iterators - in this block to avoid issues with iterator reuse      \n",
    "train_itr = SimpleDataIterator(train)\n",
    "test_itr = SimpleDataIterator(test)\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = train_itr.next_batch(batch_size)\n",
    "        \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            print(\"****** in loss op *****\")\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    test_x, test_y = test_itr.next_batch(len(test))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_x, Y: test_y}))\n",
    "    \n",
    "    save_path = saver.save(sess, \"/tmp/model_{}.ckpt\".format(station_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We have a fantastic time series model for one station. We should generalise this for multiple stations. Logically that isn't hard, however TensorFlow doesn't allow us to have multiple models in one session. In pseudocode, what we'd do is this:\n",
    "```\n",
    "accuracy = {}\n",
    "models = {}\n",
    "\n",
    "for station_id in station_ids:\n",
    "  train, test = get_data(station_id)\n",
    "  model = train_model(train)\n",
    "  acc = test_model(model)\n",
    "  accuracy[station_id] = acc\n",
    "  models[station_id] = model\n",
    "  \n",
    "def query(station_id):\n",
    "  model = models[station_id]\n",
    "  recent_data = get_recent_data(station_id)\n",
    "  pred = model.predict(recent_data)\n",
    "  return pred\n",
    "```  \n",
    "With other libraries, this would be a little easier to quickly code up a demo of. We start by parameterising the data pulling and the session training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pull_data(s_id):\n",
    "  q = bq.Query(\n",
    "  \"\"\"\n",
    "  SELECT\n",
    "    bikes_available,\n",
    "    IF(next_ba <= 2,\n",
    "      1,\n",
    "      0) AS low_availability,\n",
    "    IF(next_ba > 2,\n",
    "      1,\n",
    "      0) AS high_availability  \n",
    "  FROM (\n",
    "    SELECT\n",
    "      time,\n",
    "      bikes_available,\n",
    "      LEAD(bikes_available, 30) OVER(PARTITION BY station_id ORDER BY time ASC ) AS next_ba\n",
    "    FROM\n",
    "      `bigquery-public-data.san_francisco.bikeshare_status`\n",
    "    WHERE\n",
    "      station_id = {}\n",
    "    ORDER BY\n",
    "      time ASC ) a\n",
    "  WHERE\n",
    "    next_ba IS NOT NULL\n",
    "  \"\"\".format(s_id)\n",
    "  )\n",
    "  df = q.execute(output_options=bq.QueryOutput.dataframe()).result()\n",
    "\n",
    "  # we need to massage the data to pass it to a model as a time series\n",
    "  as_strided = np.lib.stride_tricks.as_strided  \n",
    "  win = 15 # collect data for next 15 steps\n",
    "  v = as_strided(df.bikes_available, (len(df) - (win - 1), win), (df.bikes_available.values.strides * 2))\n",
    "  df['strides'] = pd.Series(v.tolist())\n",
    "  df = df[df.strides.notnull()]\n",
    "  msk = np.random.rand(len(df)) < 0.8\n",
    "  train = df[msk]\n",
    "  test = df[~msk]\n",
    "  return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We next parameterise the session learning, allowing us to save the accuracy for each station's model as well as writing out each set of weights to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "def train_session(s_id):\n",
    "  print(\"*****\")\n",
    "  print(\"Training for station {}.\".format(s_id))\n",
    "  print(\"*****\")\n",
    "  print(\"pulling data...\")\n",
    "  train, test = pull_data(s_id)\n",
    "  print(\"data fetched.\")\n",
    "  train_itr = SimpleDataIterator(train)\n",
    "  test_itr = SimpleDataIterator(test)\n",
    "  \n",
    "  # Start training\n",
    "  with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    print(\"Training model for station {}.\".format(s_id))\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = train_itr.next_batch(batch_size)\n",
    "        \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            print(\"****** in loss op *****\")\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    test_x, test_y = test_itr.next_batch(len(test))\n",
    "    session_accuracy = sess.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "    print(\"Testing Accuracy:\", session_accuracy)\n",
    "    \n",
    "    save_path = saver.save(sess, \"/tmp/model_{}.ckpt\".format(s_id))\n",
    "    \n",
    "    return session_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we loop over the station IDs and learn weights for the graph for this station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stations_train():\n",
    "  client = bigquery.Client()\n",
    "  query_job = client.query(\n",
    "     \"\"\"\n",
    "     SELECT DISTINCT station_id\n",
    "     FROM `bigquery-public-data.san_francisco.bikeshare_stations`\n",
    "     \"\"\"\n",
    "  )\n",
    "  results = query_job.result()\n",
    "  accuracy_store = {}\n",
    "  for row in results:\n",
    "    s_id = row.station_id\n",
    "    accuracy_store[s_id] = train_session(s_id)\n",
    "    \n",
    "  return accuracy_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, training a bunch of deep neural networks is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "Training for 4.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.2845, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0988, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1263, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1175, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0834, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0485, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0909, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0542, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1742, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0520, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0669, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0715, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0982, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0355, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0325, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.1085, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0553, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0564, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0630, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0315, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0271, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0671, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0629, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0528, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0744, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0402, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0416, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0326, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0904, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0355, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0268, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0541, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0440, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0538, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0169, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0267, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0332, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0202, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0545, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0301, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0975, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0905, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0476, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0463, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0389, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0777, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0762, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.1231, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0743, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0417, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1433, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.988667)\n",
      "*****\n",
      "Training for 37.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1850, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1198, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0512, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0718, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0926, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1124, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0565, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0464, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0623, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0442, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0636, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0639, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0353, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0884, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0829, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0440, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0251, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0395, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0546, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0206, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0443, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0471, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0246, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0521, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0311, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0273, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0482, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0311, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0350, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0735, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0788, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0203, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0178, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0368, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0468, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0181, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0408, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0343, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0717, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1146, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0670, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0295, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0287, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0547, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0309, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0233, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0147, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0230, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0303, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0294, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0800, Training Accuracy= 0.969\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9903387)\n",
      "*****\n",
      "Training for 35.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0876, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0328, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0470, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0708, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0739, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0984, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0560, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1318, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0277, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0491, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0488, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0475, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0230, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1073, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0271, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0411, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0838, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0219, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0172, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0743, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0502, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0187, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0172, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0322, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1071, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0512, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0241, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0221, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0548, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0178, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0215, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0294, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0264, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0177, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0490, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0211, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0132, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0346, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0435, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0224, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0055, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0303, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0148, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0232, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0132, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0711, Training Accuracy= 0.977\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9958026)\n",
      "*****\n",
      "Training for 32.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.6303, Training Accuracy= 0.805\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.2684, Training Accuracy= 0.812\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1798, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1379, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1163, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0980, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0889, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1025, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1030, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.1794, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1004, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1363, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0517, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0949, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1342, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0812, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0361, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0751, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0938, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0581, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0239, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0647, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0606, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1101, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.1354, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0511, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0476, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0561, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0596, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0451, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0434, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0792, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0884, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0560, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0471, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.1518, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0965, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0457, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0387, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0997, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0446, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1328, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0657, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0346, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0451, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1159, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0291, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0291, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0278, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0868, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1336, Training Accuracy= 0.969\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.98869735)\n",
      "*****\n",
      "Training for 84.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0825, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0230, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0347, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0700, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0554, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0196, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0446, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0242, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0236, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0333, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0686, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0225, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0168, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0224, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0193, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0205, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0404, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0235, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0785, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0165, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0185, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0379, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0134, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0225, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0508, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0161, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0302, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0119, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0307, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0213, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0181, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0248, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0213, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0579, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0262, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0963, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0132, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0257, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9955929)\n",
      "*****\n",
      "Training for 8.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0468, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0035, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0195, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0112, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0136, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0151, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0235, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0172, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0165, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0159, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0465, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0133, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0089, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0100, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0101, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0383, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0080, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0081, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0153, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0189, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0068, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0062, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0623, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0179, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0228, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0095, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0168, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0094, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0056, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0315, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0060, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0033, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0048, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0125, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9983047)\n",
      "*****\n",
      "Training for 9.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 2.0054, Training Accuracy= 0.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0286, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0536, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0256, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0256, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0352, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0114, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0209, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0235, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0216, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0359, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0110, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0202, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0132, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0205, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0324, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0072, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0081, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0210, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0178, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0291, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0067, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0151, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0125, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0135, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0281, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0214, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0062, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0073, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0154, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0254, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0246, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0042, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0028, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0108, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0064, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9965376)\n",
      "*****\n",
      "Training for 3.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0974, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0197, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0280, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0166, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0199, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0116, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0380, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0047, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0238, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0121, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0232, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0124, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0168, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0033, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0030, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0106, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0793, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0069, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0132, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0073, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0094, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0095, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0192, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0033, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0042, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0241, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0152, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0117, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0182, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0060, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0172, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0072, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0069, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0394, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9974841)\n",
      "*****\n",
      "Training for 13.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0828, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0634, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0274, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0549, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0311, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0337, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0340, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0416, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0142, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0349, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0402, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0307, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0080, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0214, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0180, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0172, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1023, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0230, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0240, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0137, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0245, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0155, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0172, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0086, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0156, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0109, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0183, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0292, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0281, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0165, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0361, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0723, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0300, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0153, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0068, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0463, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0104, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0225, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0196, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0203, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0527, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0354, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9957474)\n",
      "*****\n",
      "Training for 10.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1742, Training Accuracy= 0.906\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0979, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1325, Training Accuracy= 0.898\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0767, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0503, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0844, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0511, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0463, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0839, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0715, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0724, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0532, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0499, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0294, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0552, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0896, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0297, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0287, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0395, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0826, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0269, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0510, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0419, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0230, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0235, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0417, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0188, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0144, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0454, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0273, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0123, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0281, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0443, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0241, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0525, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0205, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0265, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0215, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0330, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0190, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0254, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0293, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0495, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0180, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0414, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0215, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0474, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0848, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0238, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0247, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9935712)\n",
      "*****\n",
      "Training for 16.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1413, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0352, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0145, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0728, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0217, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0201, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0151, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0119, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0261, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0306, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0147, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0406, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0154, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0274, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0532, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0339, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0204, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0155, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0208, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0073, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0245, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0121, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0171, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0363, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0381, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0142, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0194, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0188, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0210, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0275, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0105, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0437, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0154, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0289, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0301, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0324, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99652594)\n",
      "*****\n",
      "Training for 7.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0290, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0134, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0293, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0189, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0348, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0131, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0230, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0371, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0174, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0190, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0183, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0186, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0171, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0078, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0184, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0174, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0200, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0047, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0307, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0084, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.1152, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0095, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0191, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0055, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0046, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0113, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0028, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0254, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0318, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0053, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.997814)\n",
      "*****\n",
      "Training for 6.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 4.9157, Training Accuracy= 0.055\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0394, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0675, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0173, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0155, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0230, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0237, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0192, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0690, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0200, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0353, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0477, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0309, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0254, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0355, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0176, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0147, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0220, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0182, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0417, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0445, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0500, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0141, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0315, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0404, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0176, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0321, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0251, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0192, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0613, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0104, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0115, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0284, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0084, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0304, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0309, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0283, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0139, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0298, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9959631)\n",
      "*****\n",
      "Training for 25.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1081, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0498, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0328, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0536, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0447, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0861, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0405, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0264, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0502, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0481, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0490, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0524, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0434, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0435, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0449, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0344, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0182, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0248, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0197, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0349, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0249, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0257, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0174, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0403, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0185, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0223, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0234, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0271, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0347, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0316, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0205, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0292, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0239, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0170, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0196, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0410, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0127, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0391, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0287, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0197, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0154, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0140, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0284, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0215, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0228, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0206, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0201, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.993595)\n",
      "*****\n",
      "Training for 21.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0008, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0103, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0218, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0160, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0210, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0064, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0260, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0199, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0039, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0186, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0050, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0048, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0124, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0143, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0319, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0128, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0198, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0203, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0118, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0426, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0252, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0126, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0063, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0176, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0137, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0150, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0248, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0103, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0153, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0085, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0201, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0253, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0029, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0033, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0222, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9974547)\n",
      "*****\n",
      "Training for 24.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0762, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0491, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0163, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0184, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0189, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0234, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0173, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0218, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0239, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0165, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0160, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0210, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0157, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0169, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0262, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0143, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0141, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0040, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0163, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0246, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0217, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0424, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.1613, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0060, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0065, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0535, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0455, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0123, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0072, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0184, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0086, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0035, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0080, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9978761)\n",
      "*****\n",
      "Training for 26.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0924, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0608, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0532, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0272, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0208, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0283, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0158, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0388, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0251, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0265, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0653, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0188, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0198, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0119, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0190, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0299, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0210, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0403, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0235, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0139, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0246, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0200, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0217, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0175, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0094, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0178, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0222, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0043, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0104, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0171, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0183, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0164, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0220, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0199, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0184, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0173, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0237, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0204, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0061, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0160, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0348, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0173, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99664277)\n",
      "*****\n",
      "Training for 38.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0716, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0172, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0232, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0123, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0178, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0040, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0175, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0122, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0106, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0151, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0252, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0227, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0099, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0163, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0657, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0153, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0514, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0103, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0159, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0088, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0012, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0108, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0152, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0092, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0078, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0035, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0034, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0141, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0063, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0061, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0060, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0030, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0044, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99945474)\n",
      "*****\n",
      "Training for 36.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0281, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0077, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0242, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0236, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0110, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0068, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0211, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0008, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0089, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0489, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0102, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0284, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0035, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0113, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0005, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0045, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0072, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0171, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0106, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0118, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0086, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0165, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0048, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0085, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0220, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0031, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0048, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0185, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0064, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0186, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0084, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0095, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0011, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0184, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99899656)\n",
      "*****\n",
      "Training for 23.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0176, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0228, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0029, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0366, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0035, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0186, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0182, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0178, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0430, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0315, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0313, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0160, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0507, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0160, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0035, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0025, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0284, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0008, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0150, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0233, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0276, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0149, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0249, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0130, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0165, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0026, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0151, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0138, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0137, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0148, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0130, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0199, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0029, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0214, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0112, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0177, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0024, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0026, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99507993)\n",
      "*****\n",
      "Training for 27.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0953, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0438, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0551, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0326, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0384, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0391, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0381, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0345, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0357, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0424, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0536, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0314, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0303, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0347, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0248, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0252, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0290, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0678, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0212, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0303, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0277, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0475, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0308, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0179, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0084, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0218, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0193, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0297, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0139, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0181, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0065, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0274, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0312, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0652, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0374, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0141, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0286, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0424, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0304, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0135, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0293, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0305, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0351, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0338, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0330, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0288, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0191, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99546134)\n",
      "*****\n",
      "Training for 41.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.2722, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1731, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1737, Training Accuracy= 0.891\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1308, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1631, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.2969, Training Accuracy= 0.867\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1045, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1451, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1258, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0972, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.2103, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0827, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1310, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1010, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1885, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.1352, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1594, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1794, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.1518, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0697, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0775, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1359, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1845, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1053, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.1302, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0655, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0984, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1486, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0966, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0959, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1638, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1426, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.1027, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0912, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0813, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.2226, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0688, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0947, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0823, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1292, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1369, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0764, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0912, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1461, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.1738, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0628, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0751, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.2022, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.1140, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0906, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1051, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9764981)\n",
      "*****\n",
      "Training for 48.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1394, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1089, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1254, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.2134, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1436, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1459, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1468, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1775, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.2027, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0929, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.2079, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1405, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1410, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.2506, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1571, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0943, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1295, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0840, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0713, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0767, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1387, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1558, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0757, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0785, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.1241, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1777, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0612, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1097, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.1111, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1263, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1052, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0738, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0829, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0907, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.1193, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0778, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.1164, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1557, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1324, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0862, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1269, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1552, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.1392, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1749, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0865, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1046, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0482, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0932, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.1336, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0709, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1082, Training Accuracy= 0.969\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9737507)\n",
      "*****\n",
      "Training for 46.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.6282, Training Accuracy= 0.586\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0826, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0320, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0634, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0257, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0910, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1383, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0388, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0318, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0750, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0357, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1311, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0354, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0768, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0578, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0898, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0243, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0316, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0130, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0281, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0438, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0273, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0369, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1183, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0266, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0407, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0890, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0177, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0224, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0251, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0336, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0573, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0273, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0468, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0552, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0199, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0569, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0375, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0628, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0316, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0427, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0247, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0193, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0257, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0275, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0270, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0337, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0185, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9909234)\n",
      "*****\n",
      "Training for 42.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.3163, Training Accuracy= 0.906\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0680, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0341, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0915, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0851, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0528, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1160, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0810, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1217, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0541, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0487, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0975, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0730, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0603, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0673, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0305, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0698, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0921, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0642, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0950, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0729, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0289, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0205, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0642, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.1023, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0809, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0771, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0590, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0903, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0425, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1146, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0828, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0575, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0779, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0418, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0881, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0703, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0673, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1288, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0634, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0525, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0303, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0387, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1048, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0296, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1281, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0737, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0645, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0662, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0834, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1168, Training Accuracy= 0.977\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9800558)\n",
      "*****\n",
      "Training for 45.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.2717, Training Accuracy= 0.859\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.2369, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.3066, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1540, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1345, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1416, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1307, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1566, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1788, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0928, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1086, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1584, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1002, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.2138, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1570, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0672, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1793, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1372, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.1443, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1378, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1484, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0769, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1001, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0941, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0898, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1195, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1561, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1500, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.2424, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1346, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0947, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1313, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.1291, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1152, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.1790, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0951, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.1272, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1128, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1544, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.2048, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1237, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1201, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.1175, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0749, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0910, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0577, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1917, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.1105, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.1159, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.2176, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1375, Training Accuracy= 0.977\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9704444)\n",
      "*****\n",
      "Training for 54.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.3514, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1065, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1042, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0621, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0779, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0348, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0537, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0926, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0983, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0895, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0939, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0616, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1276, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0759, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0605, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.1459, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0676, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0603, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0691, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1009, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0438, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1129, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0750, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0346, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0872, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0367, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0552, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0394, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0787, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0737, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1178, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0948, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0403, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1344, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.1098, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0675, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0914, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1199, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1009, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1342, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1274, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0941, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0359, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0792, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0855, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1372, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0522, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0560, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0546, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0954, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0678, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9735756)\n",
      "*****\n",
      "Training for 60.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.5466, Training Accuracy= 0.797\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.2229, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1925, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1630, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1374, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1987, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.2098, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1088, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1322, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.1236, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1347, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1717, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1479, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0844, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1839, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0925, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0886, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1195, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.1326, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0825, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1428, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1511, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1429, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1533, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0586, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1339, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1840, Training Accuracy= 0.914\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1141, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.1422, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1324, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0675, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1802, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0733, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1297, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.1293, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.2017, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.1334, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1341, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1236, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1533, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1280, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1630, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.1232, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1289, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.2096, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1447, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1114, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.1610, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.2071, Training Accuracy= 0.914\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.1199, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1756, Training Accuracy= 0.945\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9626053)\n",
      "*****\n",
      "Training for 57.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1431, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0780, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0471, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1980, Training Accuracy= 0.875\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0973, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0959, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0977, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0812, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0989, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0918, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0508, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0934, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1116, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0738, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0741, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.1529, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0899, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0394, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0739, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0844, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0980, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0606, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0460, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0748, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0755, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0632, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0945, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0851, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0511, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0441, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1154, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1129, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0429, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0896, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0591, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0558, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0436, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0650, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0825, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0355, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0573, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1471, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0590, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0717, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0419, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1122, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0356, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0356, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0507, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0684, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0757, Training Accuracy= 0.969\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.97941613)\n",
      "*****\n",
      "Training for 33.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 2.9476, Training Accuracy= 0.023\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0420, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0358, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0383, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0209, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0135, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0297, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0148, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0524, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0199, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0233, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0343, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0258, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0275, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0139, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0180, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0223, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0235, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0142, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0978, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0473, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0330, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0137, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0206, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0120, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0157, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0140, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0056, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0303, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0134, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0095, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0051, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0127, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0416, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99814063)\n",
      "*****\n",
      "Training for 30.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0191, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0303, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0202, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0284, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0265, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0296, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0137, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0230, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0033, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0030, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0429, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0267, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0118, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0293, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0063, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0069, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0061, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0112, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0339, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0198, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0434, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0180, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0236, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0136, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0376, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0238, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0157, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0080, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0039, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0451, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0088, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0379, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0172, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0044, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0232, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0013, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0063, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0057, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0143, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0282, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0259, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9963663)\n",
      "*****\n",
      "Training for 65.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1351, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.2013, Training Accuracy= 0.914\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1648, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0879, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0888, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0860, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1834, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1150, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0804, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0918, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1733, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0865, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1115, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1311, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0992, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0948, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0768, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1343, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0967, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1579, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1252, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1119, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1652, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0759, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.1701, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1927, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1943, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0964, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.1065, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1814, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1091, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0740, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.1814, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1964, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.1429, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.1500, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0655, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1295, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1030, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1088, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1338, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0851, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.1445, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0256, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.1206, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0656, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1463, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0754, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.1367, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.1067, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1992, Training Accuracy= 0.938\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9649402)\n",
      "*****\n",
      "Training for 64.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.4449, Training Accuracy= 0.898\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.2381, Training Accuracy= 0.859\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0996, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0404, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1113, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1073, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0883, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1039, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1194, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.1224, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1387, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1254, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0698, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1323, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1207, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.1377, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1253, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1274, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0654, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1170, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1355, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1820, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1173, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1323, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0726, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1438, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0518, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1705, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0764, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1826, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1187, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1226, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0455, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1373, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0623, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0767, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.1342, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0355, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1230, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0974, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0402, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1548, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0757, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1119, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0453, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1677, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0726, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0830, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0446, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0802, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1933, Training Accuracy= 0.945\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.97056824)\n",
      "*****\n",
      "Training for 80.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1329, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0755, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0612, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0901, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0672, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0689, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0685, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0373, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0278, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0382, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0265, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0567, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0158, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0259, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0350, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0274, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0304, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0262, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0302, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0263, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0251, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0455, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0216, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0503, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0270, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0240, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0194, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0127, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0363, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0186, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0244, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0201, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0273, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0053, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0305, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9954721)\n",
      "*****\n",
      "Training for 31.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 4.0147, Training Accuracy= 0.023\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0456, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0468, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0177, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0156, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0065, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0060, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0061, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0227, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0016, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0212, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0276, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0133, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0081, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0062, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0221, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0040, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0063, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0282, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0062, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0047, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0064, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0053, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0207, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0042, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0392, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0048, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0014, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0051, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0056, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0069, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0215, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0038, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.998176)\n",
      "*****\n",
      "Training for 82.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.2876, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1815, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1332, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1042, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.2962, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1596, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1205, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1472, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1541, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.1152, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1347, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1017, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1089, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1806, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0958, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0990, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1153, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0822, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0990, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1455, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1164, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1032, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0863, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1065, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.1424, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0940, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1080, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0499, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.1183, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0414, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0487, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1897, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.1720, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1702, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0745, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.1168, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0520, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0616, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0601, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1266, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0805, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1701, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.1139, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1607, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0417, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0562, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1281, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0957, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0261, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0792, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1484, Training Accuracy= 0.961\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9699849)\n",
      "*****\n",
      "Training for 83.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.4760, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0189, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0328, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0371, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0522, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0313, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0187, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0148, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0212, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0199, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0157, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0267, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0215, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0263, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0393, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0508, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0267, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0170, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0544, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0133, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0178, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0868, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0229, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0153, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0327, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0142, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0298, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0048, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0061, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0094, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0168, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9981649)\n",
      "*****\n",
      "Training for 73.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1225, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1019, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0689, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0960, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1186, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1541, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0972, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0774, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1708, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0754, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1777, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0593, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1209, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1333, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1052, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0751, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1474, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0663, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0718, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0903, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1403, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0502, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0974, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1117, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0845, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1289, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0831, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0504, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0941, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1054, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1193, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1193, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0658, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0909, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.1290, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0751, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0558, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0824, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0708, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0514, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0755, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1007, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0302, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0829, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0881, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0736, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1208, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.1127, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0511, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.1034, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1090, Training Accuracy= 0.953\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9738528)\n",
      "*****\n",
      "Training for 14.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0708, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0562, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0032, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0293, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0139, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0205, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0459, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0265, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0347, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0193, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0262, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0107, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0102, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0123, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0163, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0288, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0393, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0143, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0268, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0094, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0163, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0078, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0084, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0064, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0123, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0028, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0212, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0061, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0109, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0060, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0086, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0214, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0015, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0072, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0188, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0043, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0051, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0177, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0012, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.998495)\n",
      "*****\n",
      "Training for 5.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0702, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0728, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0677, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0484, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0859, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0208, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0081, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0296, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0211, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0213, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0134, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0197, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0334, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0175, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0105, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0132, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0172, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0042, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0229, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0134, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0362, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0057, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0150, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0262, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0271, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0287, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0144, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0396, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0408, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0175, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0078, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0165, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0055, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0186, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0213, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0080, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0303, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0181, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0269, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0313, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0134, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9983183)\n",
      "*****\n",
      "Training for 11.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0168, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0198, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0276, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0160, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0164, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0329, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0340, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0237, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0096, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.1062, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0310, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0331, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0188, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0132, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0141, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0070, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0154, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0171, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0034, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0068, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0168, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0055, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0156, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0247, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0275, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0184, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0158, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0189, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0078, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0047, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0200, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0238, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0229, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9976785)\n",
      "*****\n",
      "Training for 12.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1318, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0355, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0113, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0148, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0046, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0285, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0023, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0111, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0122, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0104, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0326, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0027, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0070, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0082, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0046, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0068, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0141, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0284, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0272, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0039, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0008, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0086, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0006, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0369, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0086, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0190, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0104, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0032, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0079, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0517, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0033, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0031, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0047, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0133, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0055, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0254, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0056, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0055, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0113, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0148, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0056, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9981922)\n",
      "*****\n",
      "Training for 89.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0745, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0009, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0006, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0005, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 1.0)\n",
      "*****\n",
      "Training for 88.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0010, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0006, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0003, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0001, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0000, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 1.0)\n",
      "*****\n",
      "Training for 47.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1307, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1307, Training Accuracy= 0.906\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1464, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0828, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0738, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1063, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0729, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0807, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1052, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0646, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0415, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0809, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1684, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0352, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1284, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0939, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0641, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0537, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0286, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0622, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0375, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0422, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0473, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0409, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0614, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0602, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0268, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1220, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0633, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1042, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0891, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0953, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0885, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0620, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0686, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0939, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0425, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0256, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0892, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0420, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0573, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0336, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0407, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0349, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0327, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0763, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0761, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0993, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0588, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0384, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1223, Training Accuracy= 0.977\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9851206)\n",
      "*****\n",
      "Training for 56.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1787, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1337, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1211, Training Accuracy= 0.914\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1393, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1174, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1451, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0583, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1211, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0484, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0770, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0638, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1480, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1569, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1194, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1393, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.1294, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0822, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0694, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0693, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0520, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0664, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0501, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0720, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0857, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0710, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0697, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1017, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0716, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.1101, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1245, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1127, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1387, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.1467, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0568, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0542, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0247, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.1063, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1098, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0988, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0341, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0692, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0803, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0657, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1107, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0571, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1208, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1720, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0546, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0770, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0462, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1141, Training Accuracy= 0.961\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9711052)\n",
      "*****\n",
      "Training for 51.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.4206, Training Accuracy= 0.875\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1929, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1021, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1483, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0744, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1075, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0667, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1397, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.2322, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.1479, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1587, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0699, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1208, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0774, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1172, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0597, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0839, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0919, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0913, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1329, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1266, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1538, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0988, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1775, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0761, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0651, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0814, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1186, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0748, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1631, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0863, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1228, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0804, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1076, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0786, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0689, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0609, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0383, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0576, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0679, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1285, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0493, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0641, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1065, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0934, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1068, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1039, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0218, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0855, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0658, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0465, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9730525)\n",
      "*****\n",
      "Training for 49.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1673, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0463, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1079, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0498, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0606, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1117, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0709, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0553, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0635, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0420, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0365, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0591, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0284, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0306, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0359, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0469, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0636, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0481, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0301, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0466, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0990, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0723, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0604, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0224, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0350, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0335, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0277, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0457, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0425, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0222, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0172, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0402, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0855, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0356, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0287, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0448, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0518, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0674, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0772, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0418, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0235, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0491, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0315, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0503, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0272, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0478, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0487, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0481, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0243, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0250, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0476, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.98471636)\n",
      "*****\n",
      "Training for 58.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0532, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0099, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0485, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0243, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0506, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0345, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0587, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0172, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0328, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0135, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0280, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0218, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0223, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0593, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0396, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0482, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0081, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0321, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0313, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0262, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0298, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0266, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0730, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0221, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0288, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0255, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0265, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0459, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0449, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0214, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0335, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0173, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0549, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0601, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0148, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0220, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0140, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0337, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0213, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0233, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0174, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0266, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0169, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0217, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0295, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0955, Training Accuracy= 0.977\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99342984)\n",
      "*****\n",
      "Training for 62.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.4672, Training Accuracy= 0.852\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1249, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1592, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1507, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1318, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1150, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1386, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.2068, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1092, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.1195, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.2040, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1104, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0655, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0983, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1618, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.1859, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1004, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1288, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.1250, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0693, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1699, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1201, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.2789, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1246, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.1450, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1420, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1360, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.2010, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0942, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1307, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1203, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1064, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.1336, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1807, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0903, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.1383, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0801, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0957, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0981, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1580, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0768, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1140, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.1175, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1187, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.1162, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1405, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0705, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0464, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.2202, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.1319, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1293, Training Accuracy= 0.961\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9661889)\n",
      "*****\n",
      "Training for 63.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.2671, Training Accuracy= 0.891\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0977, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0660, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0973, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1055, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0798, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0914, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0853, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1540, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0850, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0483, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1517, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0810, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0490, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0812, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0765, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0671, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0519, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.2001, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1318, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0437, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1248, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1135, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0796, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0704, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0369, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0910, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0866, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0729, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0346, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.1017, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0838, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0778, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0404, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0759, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.1417, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.1359, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1059, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0585, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0467, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1609, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0478, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0931, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1096, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0447, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1124, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0331, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0963, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0967, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0960, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0950, Training Accuracy= 0.977\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9816505)\n",
      "*****\n",
      "Training for 68.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.4462, Training Accuracy= 0.891\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0395, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0396, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0480, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0790, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0407, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0459, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1388, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0143, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0194, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0492, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0237, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0999, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0321, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0336, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0375, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0252, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0833, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0495, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0681, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0724, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0466, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0476, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0381, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1040, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0280, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0327, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0480, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0348, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0246, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0552, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0617, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0293, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0243, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0199, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1226, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0127, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0749, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0252, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0271, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0606, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0465, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0509, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0343, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0241, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0502, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0264, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0756, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0335, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9896508)\n",
      "*****\n",
      "Training for 66.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1890, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.2060, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1069, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0885, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1113, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0941, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1639, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1142, Training Accuracy= 0.914\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0835, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0780, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1286, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0785, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0707, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0760, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1057, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0688, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0800, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0792, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.1116, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0519, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0738, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0536, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0456, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0337, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0376, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0720, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0984, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0708, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0260, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0818, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0894, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0908, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0594, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0299, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0734, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.2157, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0615, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1328, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0354, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0460, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0899, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1070, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0384, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0670, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0909, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0261, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1138, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.1140, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0449, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0922, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0738, Training Accuracy= 0.969\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.97737926)\n",
      "*****\n",
      "Training for 70.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1666, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1793, Training Accuracy= 0.906\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1628, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0956, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1292, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0850, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1111, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0964, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1349, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.1331, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1099, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0715, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0772, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1013, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0798, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.2449, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1249, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1106, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.1249, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0846, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.1697, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1337, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1715, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1103, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.1270, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1553, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1832, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1452, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.1063, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1396, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0890, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1113, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.1053, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1379, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.1269, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.1113, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.1275, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1376, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1307, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1119, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1286, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1007, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0972, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1008, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.1141, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.1543, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1411, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.1230, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0814, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.1305, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1390, Training Accuracy= 0.953\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.94956577)\n",
      "*****\n",
      "Training for 71.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1545, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1068, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1091, Training Accuracy= 0.906\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1144, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0909, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0643, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0359, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0959, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0807, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0990, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0386, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1204, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0524, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0326, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0603, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0429, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0638, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1120, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0713, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0960, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0870, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0692, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0636, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0492, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0966, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0765, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0914, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0522, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0634, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0521, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0661, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0797, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0426, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0581, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0465, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0328, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0535, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0519, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0918, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0554, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0977, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.2105, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0777, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0763, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0798, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0430, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0274, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0341, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0156, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0998, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0466, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9806384)\n",
      "*****\n",
      "Training for 75.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1771, Training Accuracy= 0.906\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1138, Training Accuracy= 0.914\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1359, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0898, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1284, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0910, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0847, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.1290, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0924, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0645, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1180, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1673, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1534, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0342, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0861, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0627, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0805, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0725, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0964, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0448, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0615, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0579, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1715, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0699, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0544, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0504, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1398, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1136, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0919, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0845, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0613, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0400, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0463, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0652, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0515, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0272, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0742, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0327, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0751, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0984, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0463, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0468, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.1043, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0452, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0454, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0961, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0732, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0404, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0513, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0238, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0630, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9832963)\n",
      "*****\n",
      "Training for 76.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.2504, Training Accuracy= 0.883\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1153, Training Accuracy= 0.906\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1104, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1327, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1069, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.1061, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1221, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0736, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0635, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0401, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0827, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0911, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0959, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1148, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1501, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.1509, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1474, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0763, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0765, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0943, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0651, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1546, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0719, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0416, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.1008, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0783, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0723, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0783, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0533, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0889, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0538, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.1285, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0632, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0694, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0905, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0600, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0551, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0987, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1251, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0535, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0705, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0616, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.1073, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0628, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0413, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0526, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0379, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0462, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.1238, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0395, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.1055, Training Accuracy= 0.953\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9797959)\n",
      "*****\n",
      "Training for 39.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1601, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0881, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0548, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1117, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0643, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0904, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0582, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0596, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0408, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.1222, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0639, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0778, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0382, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0136, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0274, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0705, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0319, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0173, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0782, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1135, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0594, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0251, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0556, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0635, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0592, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0416, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0450, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0474, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0475, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0794, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0319, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0467, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0458, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0494, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0707, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0400, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0299, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0595, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0173, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0245, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0765, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0497, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0477, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0292, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0233, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0540, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0426, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0637, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0212, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0260, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0483, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9824659)\n",
      "*****\n",
      "Training for 34.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0594, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0277, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0162, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0148, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0262, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0061, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0060, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0583, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0020, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0123, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0212, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0203, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0996, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0253, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0137, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0168, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0104, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0078, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0057, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0007, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0133, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0044, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0109, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0246, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0554, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0165, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0042, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0068, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0043, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0188, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0056, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0063, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0025, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0012, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99794316)\n",
      "*****\n",
      "Training for 29.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0213, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0375, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0034, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0050, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0351, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0175, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0438, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0101, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0133, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0059, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0287, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0387, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0199, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0221, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0253, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0067, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0045, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0230, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0111, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0167, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0086, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0282, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0245, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0078, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0156, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0155, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0286, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0154, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0229, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0284, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0017, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0156, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0312, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0119, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0053, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0144, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0033, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0035, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0158, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0062, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0067, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99707884)\n",
      "*****\n",
      "Training for 28.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0693, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0039, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0104, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0179, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0164, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0053, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0174, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0276, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0080, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0044, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0230, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0141, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0070, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0062, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0244, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0223, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0047, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0112, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0927, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0259, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0325, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0233, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0244, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0044, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0069, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0037, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0051, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0051, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.99680054)\n",
      "*****\n",
      "Training for 50.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0835, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0584, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0606, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0305, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0506, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0458, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0265, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0748, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0476, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0538, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0496, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0712, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0371, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0812, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0536, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0564, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0374, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.1028, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0645, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0780, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0747, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0623, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0753, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0488, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0775, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0541, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0476, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0833, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1279, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0539, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0911, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0562, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.1865, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0821, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0505, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0476, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0543, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0594, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1254, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0301, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0478, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0879, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1093, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0754, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0343, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.1147, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0323, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0666, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.1406, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0626, Training Accuracy= 0.961\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.97229874)\n",
      "*****\n",
      "Training for 55.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0853, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0678, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1071, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0810, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0678, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0942, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0385, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0506, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0466, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0751, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0497, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1239, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0678, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0797, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1001, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.1054, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0800, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0747, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0592, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1004, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0536, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0470, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1172, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.1080, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0632, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0357, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1021, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1142, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0967, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0763, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0523, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0719, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.1024, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0445, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0794, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0542, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0254, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.1460, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0726, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0579, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0640, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0375, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0518, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1063, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0663, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0578, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0557, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0578, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.1675, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0815, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0804, Training Accuracy= 0.953\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.97244686)\n",
      "*****\n",
      "Training for 59.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0769, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0383, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0436, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0409, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0590, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0356, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0451, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0454, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0569, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0664, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0339, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0370, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0571, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0288, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0305, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0427, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0520, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0171, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0366, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0348, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0163, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0149, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0269, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0380, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0354, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0451, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0672, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0557, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0285, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0621, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0271, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0189, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0234, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0653, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0253, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0275, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0123, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0180, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0182, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0213, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0282, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0210, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0337, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0151, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0584, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0447, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0259, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.98968756)\n",
      "*****\n",
      "Training for 72.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1201, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0495, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0680, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0626, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0513, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0374, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0300, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0338, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0440, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0333, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0344, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1111, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0481, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0266, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0456, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0360, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0823, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0294, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0306, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0584, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0294, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0232, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0677, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0362, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0315, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0444, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0411, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0217, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0384, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0299, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0221, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0208, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0657, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0214, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0442, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0405, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0326, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0292, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0331, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0308, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0304, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0450, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0556, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0818, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0677, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0580, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0595, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0733, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0513, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0519, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0333, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.98617804)\n",
      "*****\n",
      "Training for 69.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0934, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.1013, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1216, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.1173, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.1017, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0941, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.1649, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0594, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.1136, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0800, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.1296, Training Accuracy= 0.914\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.1426, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.1003, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.1029, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1068, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0742, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.1486, Training Accuracy= 0.898\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1360, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0996, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1070, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0749, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.1690, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.1219, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0849, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0748, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1174, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.1705, Training Accuracy= 0.906\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.1770, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.1137, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.1077, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0586, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0982, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.1242, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0932, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.1028, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0758, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.1256, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0762, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0787, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.1087, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.1441, Training Accuracy= 0.930\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0556, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0936, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.1024, Training Accuracy= 0.938\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0889, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0376, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0956, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0875, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0770, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0779, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0797, Training Accuracy= 0.969\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.96203434)\n",
      "*****\n",
      "Training for 74.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0705, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0448, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0597, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0527, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0392, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0489, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0670, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0981, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0328, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0632, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0290, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0509, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0440, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0477, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.1404, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0462, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0332, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0302, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0382, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0813, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0601, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0789, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0986, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0596, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0507, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0399, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0792, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0343, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0406, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0501, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0448, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0526, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0770, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0291, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0497, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.1265, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0578, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0919, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.1712, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0256, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0264, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0653, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0605, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0687, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.1099, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0607, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0982, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.1258, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0633, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0507, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0787, Training Accuracy= 0.977\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.98079693)\n",
      "*****\n",
      "Training for 22.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.4255, Training Accuracy= 0.844\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0570, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0231, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0337, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0101, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0259, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0260, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0239, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0185, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0238, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0210, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0093, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0034, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0309, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0089, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0105, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0186, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0170, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0081, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0195, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0243, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0202, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0082, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0171, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0141, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0206, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0142, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0045, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0143, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0064, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0065, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0043, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0170, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0054, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0004, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0022, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0036, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0127, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0080, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.998728)\n",
      "*****\n",
      "Training for 2.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0633, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0057, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0307, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0179, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0204, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0133, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0121, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0152, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0295, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0180, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0251, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0002, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0291, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0018, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0201, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0086, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0325, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0195, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0784, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0094, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0049, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0575, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0549, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0638, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0185, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0067, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0249, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0056, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0019, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0067, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0048, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0245, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0500, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.997294)\n",
      "*****\n",
      "Training for 61.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 2.6474, Training Accuracy= 0.031\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0284, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0692, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0376, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0490, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0282, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0399, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0359, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0163, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0357, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0393, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0316, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0320, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0304, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0778, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0380, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0937, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.1248, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0320, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.1397, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0383, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0502, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0611, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0739, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0448, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0379, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0225, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0388, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0364, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0146, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0205, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0394, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0720, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0400, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0442, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0735, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0368, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0385, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0492, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0157, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0379, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0640, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0735, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0406, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0523, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0293, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0410, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0234, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0482, Training Accuracy= 0.984\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9851615)\n",
      "*****\n",
      "Training for 67.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.1442, Training Accuracy= 0.922\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0535, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.1841, Training Accuracy= 0.945\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0771, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0617, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0546, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0491, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0995, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0670, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0538, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0438, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0880, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0510, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0848, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0601, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0365, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0644, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0724, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0470, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0638, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0963, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0470, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0912, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0633, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0910, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.1486, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0702, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0736, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0503, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0455, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0419, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0241, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0545, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0493, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0226, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0579, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0772, Training Accuracy= 0.953\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0648, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0562, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0610, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0648, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.1113, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0837, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0569, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0871, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0688, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0627, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0597, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0935, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0506, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0273, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9794122)\n",
      "*****\n",
      "Training for 77.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0042, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0220, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0206, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0161, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0021, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0755, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0044, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0507, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0203, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0302, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0156, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0125, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.2892, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0342, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0102, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0280, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0219, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0067, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0028, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0115, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0063, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0160, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0228, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0313, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0171, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0850, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0051, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0172, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0065, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0172, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0041, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0321, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0153, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0251, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0099, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0265, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0173, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0135, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0034, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0136, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0055, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0174, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0121, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0666, Training Accuracy= 0.992\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9936227)\n",
      "*****\n",
      "Training for 90.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 200, Minibatch Loss= 0.0268, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 400, Minibatch Loss= 0.0358, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 600, Minibatch Loss= 0.0262, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 800, Minibatch Loss= 0.0304, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 1000, Minibatch Loss= 0.0339, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1200, Minibatch Loss= 0.0417, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 1400, Minibatch Loss= 0.0149, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1600, Minibatch Loss= 0.0243, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 1800, Minibatch Loss= 0.0259, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2000, Minibatch Loss= 0.0211, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 2200, Minibatch Loss= 0.0315, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2400, Minibatch Loss= 0.0370, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 2600, Minibatch Loss= 0.0542, Training Accuracy= 0.961\n",
      "****** in loss op *****\n",
      "Step 2800, Minibatch Loss= 0.0378, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3000, Minibatch Loss= 0.0303, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3200, Minibatch Loss= 0.0145, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 3400, Minibatch Loss= 0.0408, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 3600, Minibatch Loss= 0.0291, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 3800, Minibatch Loss= 0.0122, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4000, Minibatch Loss= 0.0182, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 4200, Minibatch Loss= 0.0392, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4400, Minibatch Loss= 0.0267, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4600, Minibatch Loss= 0.0227, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 4800, Minibatch Loss= 0.0227, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5000, Minibatch Loss= 0.0268, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5200, Minibatch Loss= 0.0182, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 5400, Minibatch Loss= 0.0250, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5600, Minibatch Loss= 0.0234, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 5800, Minibatch Loss= 0.0204, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6000, Minibatch Loss= 0.0602, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 6200, Minibatch Loss= 0.0325, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 6400, Minibatch Loss= 0.0277, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6600, Minibatch Loss= 0.0248, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 6800, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 7000, Minibatch Loss= 0.0611, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 7200, Minibatch Loss= 0.0317, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7400, Minibatch Loss= 0.0258, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 7600, Minibatch Loss= 0.0233, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 7800, Minibatch Loss= 0.0233, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8000, Minibatch Loss= 0.0336, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8200, Minibatch Loss= 0.0166, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8400, Minibatch Loss= 0.0367, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 8600, Minibatch Loss= 0.0288, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 8800, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9000, Minibatch Loss= 0.0052, Training Accuracy= 1.000\n",
      "****** in loss op *****\n",
      "Step 9200, Minibatch Loss= 0.0474, Training Accuracy= 0.969\n",
      "****** in loss op *****\n",
      "Step 9400, Minibatch Loss= 0.0240, Training Accuracy= 0.984\n",
      "****** in loss op *****\n",
      "Step 9600, Minibatch Loss= 0.0479, Training Accuracy= 0.977\n",
      "****** in loss op *****\n",
      "Step 9800, Minibatch Loss= 0.0244, Training Accuracy= 0.992\n",
      "****** in loss op *****\n",
      "Step 10000, Minibatch Loss= 0.0320, Training Accuracy= 0.977\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.9878683)\n",
      "*****\n",
      "Training for 91.\n",
      "*****\n",
      "****** in loss op *****\n",
      "Step 1, Minibatch Loss= 0.4985, Training Accuracy= 0.828\n"
     ]
    }
   ],
   "source": [
    "accuracy_store = stations_train()  # this will take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for k, v in accuracy_store.iteritems():\n",
    "  print(\"Accuracy for station {}: {}.\".format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To simulate an online lookup, we need to be able to fetch 15 consecutive samples for a station ID. In the real world, we'd simply pull the most recent status updates. In this code block, we're simulating that by setting a random time window and reshaping that data such at TensorFlow will work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "def station_time_range(s_id):\n",
    "  client = bigquery.Client()\n",
    "  query_job = client.query(\n",
    "     \"\"\"\n",
    "     SELECT min(time) as start_ts, max(time) as end_ts\n",
    "     FROM `bigquery-public-data.san_francisco.bikeshare_status`\n",
    "     WHERE\n",
    "       station_id = {}\n",
    "     \"\"\".format(s_id)\n",
    "  )\n",
    "  results = query_job.result()\n",
    "  for row in results:\n",
    "    return row.start_ts, row.end_ts\n",
    "\n",
    "def date_range_pair(start_date, end_date):\n",
    "  delta = end_date - start_date\n",
    "  int_delta = (delta.days * 24 * 60 * 60) + delta.seconds\n",
    "  random_second = randrange(int_delta)\n",
    "  start_dt = start_date + timedelta(seconds=random_second)\n",
    "  end_dt = start_dt + timedelta(minutes=20)\n",
    "  return start_dt, end_dt\n",
    "\n",
    "def pull_data(s_id):\n",
    "  min_date, max_date = station_time_range(s_id)\n",
    "  start_dt, end_dt = date_range_pair(min_date, max_date - timedelta(minutes=2))\n",
    "\n",
    "  client = bigquery.Client()\n",
    "  query_job = client.query(\n",
    "  \"\"\"\n",
    "  SELECT\n",
    "    bikes_available\n",
    "  FROM\n",
    "    `bigquery-public-data.san_francisco.bikeshare_status`\n",
    "  WHERE\n",
    "    station_id = {}\n",
    "    AND time between '{}' and '{}'\n",
    "  ORDER BY\n",
    "    time ASC \n",
    "  LIMIT 15\n",
    "  \"\"\".format(s_id, datetime.strftime(start_dt, date_format), datetime.strftime(end_dt, date_format))\n",
    "  )\n",
    "  results = query_job.result()\n",
    "  xs = []\n",
    "  for row in results:\n",
    "    xs.append(row.bikes_available)\n",
    "    \n",
    "  return np.expand_dims(np.expand_dims(np.array(xs), axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now have a way to simulate getting 15 recent status updates for the station we're working on. To make predictions online with the trained model, we need to restore the model from disk and query the model. With extra effort, the sessions for each trained model could be stored in a map to avoid initialising a model for each call. This would be useful if we had one backend server in a REST API which would be queried with the station ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.3232475e-12 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "def do_prediction(s_id):\n",
    "  with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/model_{}.ckpt\".format(s_id))\n",
    "    preds = sess.run(prediction, {X: pull_data(s_id)})\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "do_prediction(91)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
